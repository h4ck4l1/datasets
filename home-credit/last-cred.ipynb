{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile data_file.py\nimport os,sys,warnings,time,re,math,gc\nwarnings.filterwarnings(\"ignore\")\nfrom glob import glob\nfrom pathlib import Path\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport polars.selectors as cs\nfrom sklearn.metrics import roc_auc_score,auc\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cgb\n\nprint(\"scrpit start\")\n\npath_to_train = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train\"\npath_to_test = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test\"\n\n\nall_train_files = glob(path_to_train+\"/*.parquet\")\nall_test_files = glob(path_to_test+\"/*.parquet\")\ntrain_files_df = pl.DataFrame({\"index\":range(len(all_train_files)),\"path\":all_train_files})\ntest_files_df = pl.DataFrame({\"index\":range(len(all_test_files)),\"path\":all_test_files})\ntrain_files_df = (\n    train_files_df\n    .with_columns(\n        (pl.col(\"path\").str.split(\"/\").list.get(-1)).alias(\"filename\")\n    )\n    .sort(by=\"filename\")\n)\ntest_files_df = (\n    test_files_df\n    .with_columns(\n        (pl.col(\"path\").str.split(\"/\").list.get(-1)).alias(\"filename\")\n    )\n    .sort(by=\"filename\")\n)\n\n\n\ntrain_base = (\n    pl.read_parquet(path_to_train+\"/train_base.parquet\")\n    .select(\n        pl.col(\"case_id\").cast(pl.UInt64),\n        cs.contains(\"date\").str.to_date().alias(\"Date\"),\n        pl.col(\"WEEK_NUM\").cast(pl.UInt8).alias(\"week_num\"),\n        pl.col(\"target\").cast(pl.UInt8)\n    )\n    .with_columns(\n        pl.col(\"Date\").dt.month().alias(\"month\"),\n        pl.col(\"Date\").dt.weekday().alias(\"weekday\"),\n        pl.col(\"Date\").dt.week().alias(\"week\"),\n        (pl.col(\"Date\").dt.year() - 2018).cast(pl.UInt8).alias(\"year\")\n    )\n    .select(~cs.contains(\"target\"),cs.contains(\"target\"))\n)\n\n\ntest_base = (\n    pl.read_parquet(path_to_test+\"/test_base.parquet\")\n    .select(\n        pl.col(\"case_id\").cast(pl.UInt64),\n        cs.contains(\"date\").str.to_date().alias(\"Date\"),\n        pl.col(\"WEEK_NUM\").cast(pl.UInt8).alias(\"week_num\")\n    )\n    .with_columns(\n        pl.col(\"Date\").dt.month().alias(\"month\"),\n        pl.col(\"Date\").dt.weekday().alias(\"weekday\"),\n        pl.col(\"Date\").dt.week().alias(\"week\"),\n        (pl.col(\"Date\").dt.year() - 2018).cast(pl.UInt8).alias(\"year\"),\n        pl.lit(0).cast(pl.UInt8).alias(\"target\")\n    )\n    .select(~cs.contains(\"target\"),cs.contains(\"target\"))\n)\n\n\ndef rename_cols(df:pl.DataFrame,name:str):\n    if name == \"intshallow\":\n        return df.rename(\n            {\n                \"dpdmaxdatemonth_442T\":\"dpdmaxdatemonth_442D\",\n                \"dpdmaxdatemonth_89T\":\"dpdmaxdatemonth_89D\",\n                \"dpdmaxdateyear_596T\":\"dpdmaxdateyear_596D\",\n                \"dpdmaxdateyear_896T\":\"dpdmaxdateyear_896D\",\n                \"overdueamountmaxdatemonth_284T\":\"overdueamountmaxdatemonth_284D\",\n                \"overdueamountmaxdatemonth_365T\":\"overdueamountmaxdatemonth_365D\",\n                \"overdueamountmaxdateyear_2T\":\"overdueamountmaxdateyear_2D\",\n                \"overdueamountmaxdateyear_994T\":\"overdueamountmaxdateyear_994D\",\n            }\n        )\n    elif name == \"intdepth\":\n        return df.rename(\n            {\n                \"pmts_month_158T\":\"pmts_month_158D\",\n                \"pmts_month_706T\":\"pmts_month_706D\",\n                \"pmts_year_1139T\":\"pmts_year_1139D\",\n                \"pmts_year_507T\":\"pmts_year_507D\",\n            }\n        )\n    elif name == \"extshallow\":\n        return df.rename(\n            {\n                \"dpdmaxdatemonth_804T\":\"dpdmaxdatemonth_804D\",\n                \"dpdmaxdateyear_742T\":\"dpdmaxdateyear_742D\",\n                \"overdueamountmaxdatemonth_494T\":\"overdueamountmaxdatemonth_494D\",\n                \"overdueamountmaxdateyear_432T\":\"overdueamountmaxdateyear_432D\",\n            }\n        )\n    else:\n        return df\n    \n    \n    \n\ndef convert_dtype(df:pl.DataFrame):\n    return (\n        df\n        .select(\n            cs.by_name(\"case_id\").cast(pl.UInt64),\n            cs.contains(\"num_group\").cast(pl.UInt16),\n            cs.ends_with(\"D\").cast(pl.Date),\n            (cs.ends_with(\"T\",\"M\") | (cs.ends_with(\"L\") & cs.string())).cast(pl.String),\n            (cs.ends_with(\"L\") & cs.integer()).cast(pl.Int32),\n            (cs.ends_with(\"L\") & cs.float()).cast(pl.Float32),\n            (cs.ends_with(\"P\",\"A\") & cs.unsigned_integer()).cast(pl.UInt32),\n            (cs.ends_with(\"P\",\"A\") & cs.signed_integer()).cast(pl.Int32),\n            (cs.ends_with(\"P\",\"A\") & cs.float()).cast(pl.Float32),\n            pl.col(pl.Boolean)\n        )\n    )\n\n\ndef grouping(df:pl.DataFrame):\n    return (\n        df\n        .group_by(\"case_id\")\n        .agg(\n            cs.numeric().max().prefix(\"max_\"),\n            cs.numeric().mean().prefix(\"mean_\"),\n            cs.numeric().first().prefix(\"first_\"),\n            cs.numeric().last().prefix(\"last_\"),\n            cs.date().first().prefix(\"first_\"),\n            cs.date().last().prefix(\"last_\"),\n            (cs.string() | cs.boolean()).drop_nulls().mode().first().prefix(\"mode_\"),\n            (cs.string() | cs.boolean()).last().prefix(\"last_\"),\n            (cs.string() | cs.boolean()).first().prefix(\"first_\")\n        )\n    )\n\n\ndef preprocess(\n        string_name:str,\n        prefix_string:str,\n        train_base_df:pl.DataFrame,\n        test_base_df:pl.DataFrame,\n        cat_cols:list,\n        num_cols:list\n        ):\n    train_files_list = train_files_df.filter(pl.col(\"filename\").str.contains(string_name))[\"path\"].to_list()\n    test_files_list = test_files_df.filter(pl.col(\"filename\").str.contains(string_name))[\"path\"].to_list()\n    with pl.StringCache():\n        train_df = (\n            pl.concat(\n                [\n                    pl.read_parquet(train_file)\n                    .pipe(rename_cols,prefix_string)\n                    .pipe(convert_dtype)\n                    .pipe(grouping)\n                    for train_file in train_files_list\n                ],\n                parallel=False,\n                rechunk=False\n            )\n        )\n        test_schema = {\"_\".join(col_name.split(\"_\")[1:]) if col_name != \"case_id\" else col_name:dtype for col_name,dtype in train_df.schema.items()}\n        test_columns = {col_name for col_name in test_schema.keys()}\n        if prefix_string == \"staticexternal\":\n            train_df = (\n                train_df\n                .with_columns(\n                    cs.contains(\"302T\").str.split(\"%\").list.gather([0,1])\n                    .map_elements(\n                        lambda x: (int(x[0]) + int(x[1].strip().split(\" \")[1]))/200,return_dtype=pl.Float32\n                    )\n                )\n            )\n        \n        cat_cols += train_df.select((~(cs.numeric() | cs.date())).prefix(f\"{prefix_string}_\")).columns\n        num_cols += train_df.select((cs.numeric() | cs.date()).exclude(\"case_id\").prefix(f\"{prefix_string}_\")).columns\n        train_base_df = train_base_df.join(\n            train_df\n            .with_columns(pl.col(pl.String).cast(pl.Categorical).rank(\"dense\"))\n            .select(pl.col(\"case_id\"),pl.all().exclude(\"case_id\").shrink_dtype().prefix(f\"{prefix_string}_\")),\n            on=\"case_id\",\n            how=\"left\"\n        )\n        test_df = (\n            pl.concat(\n                [\n                    pl.read_parquet(test_file)\n                    .pipe(rename_cols,prefix_string)\n                    .select(test_columns)\n                    .cast(test_schema)\n                    .pipe(grouping)\n                    for test_file in test_files_list\n                ],\n                parallel=False,\n                rechunk=False\n            )\n        )\n        if prefix_string == \"staticexternal\":\n            test_df = (\n                test_df\n                .with_columns(\n                    cs.contains(\"302T\").str.split(\"%\").list.gather([0,1])\n                    .map_elements(\n                        lambda x: (int(x[0]) + int(x[1].strip().split(\" \")[1]))/200,return_dtype=pl.Float32\n                    )\n                )\n            )\n        test_base_df = test_base_df.join(\n            test_df\n            .with_columns(pl.col(pl.String).cast(pl.Categorical).rank(\"dense\"))\n            .select(pl.col(\"case_id\"),pl.all().exclude(\"case_id\").shrink_dtype().prefix(f\"{prefix_string}_\")),\n            on=\"case_id\",\n            how=\"left\"\n        )\n    return train_base_df,test_base_df,cat_cols,num_cols\n\n\nstring_list = [\"applprev_1\",\"applprev_2\",\"static_0\",\"static_cb\",\"person_1\",\"person_2\",\"other_1\",\"deposit_1\",\"debitcard\",\"bureau_a_1\",\"bureau_a_2\",\"bureau_b_1\",\"bureau_b_2\",\"registry_a\",\"registry_b\",\"registry_b\"]\nprefix_string_list = [\"pastshallow\",\"pastdepth\",\"staticbase\",\"staticexternal\",\"personshallow\",\"persondepth\",\"othershallow\",\"depositshallow\",\"cardshallow\",\"intshallow\",\"intdepth\",\"extshallow\",\"extdepth\",\"rega\",\"regb\",\"regc\"]\n\n\ncat_cols = []\nnum_cols = []\ncount = 0\nfor string_name,prefix_name in zip(string_list,prefix_string_list):\n    train_base,test_base,cat_cols,num_cols = preprocess(string_name,prefix_name,train_base,test_base,cat_cols,num_cols)\n    print(f\"done: {count+1} for {prefix_name}\")\n    count += 1\n    gc.collect()\n\n(\n    train_base\n    .with_columns(\n        (pl.col(pl.Date) - pl.col(\"Date\")).dt.total_days(),\n        pl.col(pl.Date).exclude(\"Date\").dt.weekday().prefix(\"weekdaydate_\"),\n        pl.col(pl.Date).exclude(\"Date\").dt.month().prefix(\"monthdate_\"),\n        pl.col(pl.Date).exclude(\"Date\").dt.week().prefix(\"weekdate_\")\n    )\n    .drop(\"Date\",\"case_id\")\n    .with_columns(pl.col(pl.Boolean).cast(pl.UInt8))\n    .write_parquet(\"/kaggle/working/train_df.parquet\")\n)\n\nall_date_cols = train_base.select(cs.contains([\"weekdaydate\",\"monthdate\",\"weekdate\"])).columns\nbase_cols = [\"week\",\"year\",\"month\",\"weekday\"]\ncat_cols += base_cols\ncat_cols += all_date_cols\n\n(\n    test_base\n    .with_columns(\n        (pl.col(pl.Date) - pl.col(\"Date\")).dt.total_days(),\n        pl.col(pl.Date).exclude(\"Date\").dt.weekday().prefix(\"weekdaydate_\"),\n        pl.col(pl.Date).exclude(\"Date\").dt.month().prefix(\"monthdate_\"),\n        pl.col(pl.Date).exclude(\"Date\").dt.week().prefix(\"weekdate_\")\n    )\n    .drop(\"Date\")\n    .with_columns(pl.col(pl.Boolean).cast(pl.UInt8))\n    .write_parquet(\"/kaggle/working/test_df.parquet\")\n)\n\njoblib.dump((cat_cols,num_cols),\"cols.pkl\")\nprint(\"first scrip done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T10:49:35.424332Z","iopub.execute_input":"2024-05-29T10:49:35.424678Z","iopub.status.idle":"2024-05-29T10:49:35.443789Z","shell.execute_reply.started":"2024-05-29T10:49:35.424650Z","shell.execute_reply":"2024-05-29T10:49:35.442803Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing data_file.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python data_file.py","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:49:35.542733Z","iopub.execute_input":"2024-05-29T10:49:35.543008Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"scrpit start\ndone: 1 for pastshallow\ndone: 2 for pastdepth\ndone: 3 for staticbase\ndone: 4 for staticexternal\ndone: 5 for personshallow\ndone: 6 for persondepth\ndone: 7 for othershallow\ndone: 8 for depositshallow\ndone: 9 for cardshallow\ndone: 10 for intshallow\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile preprocess.py\n\nimport gc\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport polars.selectors as cs\nimport joblib\n\nprint(\"scrpit start\")\n\ncat_cols,num_cols = joblib.load(\"/kaggle/working/cols.pkl\")\n\nprint(\"Pre-processed num cols: \",len(num_cols))\nprint(\"Total cat cols: \",len(cat_cols))\n\ntrain_df = (\n    pl.scan_parquet(\"/kaggle/working/train_df.parquet\")\n    .select(pl.all().shrink_dtype())\n)\n\nnum_cols +=  train_df.select(cs.contains([\"weekdaydate\",\"weekdate\",\"monthdate\"])).columns\n\ntrain_df = (\n    pl.read_parquet(\"/kaggle/working/train_df.parquet\")\n    .select(pl.all().shrink_dtype())\n    .select(num_cols)\n    .to_pandas()\n)\ngc.collect()\n\nprint(\"Total columns in train:\",len(train_df.columns))\n\nprint(f\"Total categorical columns: {len(cat_cols)} and Total numerical columns {len(num_cols)}\")\n\nnans_df = train_df.isna()\nnans_groups={}\nfor col in num_cols:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ngc.collect()\n\ndef reduce_grps(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = train_df[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n        use.append(vx)\n    return use\n\ndef group_col_by_corr(matrix,thresh=0.8):\n    correlation_matrix = matrix.corr()\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= thresh:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            grps= group_col_by_corr(train_df[Vs], thresh=0.8)\n            use=reduce_grps(grps)\n            uses=uses+use\n    else:\n        uses=uses+v\n        \nprint(\"Post-processed num cols: \",len(uses))\nprint(\"Total cols\",len(cat_cols+uses))\n\njoblib.dump((cat_cols,uses),\"all_cols.pkl\")\nprint(\"second script done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python preprocess.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile training.py\n\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nimport joblib,gc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nclass_params = {\n    \"objective\":\"binary\",\n    \"boosting_type\": \"gbdt\",\n    \"metric\":\"auc\",\n    \"max_depth\": 128,\n    \"learning_rate\": 0.01,\n    \"n_estimators\": 5000,\n    \"colsample_bynode\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"random_state\": 420,\n    \"reg_alpha\": 0.15,\n    \"reg_lambda\": 15,\n    \"extra_trees\": True,\n    \"num_leaves\": 256,\n    \"device\": \"gpu\",\n    \"importance_type\": \"gain\",\n    \"verbose\": -1\n}\n\n cat_cols, num_cols = joblib.load(\"/kaggle/working/all_cols.pkl\")\n\ntotal_df = (\n    pl.read_parquet(\n        \"/kaggle/working/train_df.parquet\",\n        low_memory=True,\n        columns=cat_cols + num_cols +[\"target\",\"week_num\"]\n    )\n    .select(pl.all().shrink_dtype())\n)\n\nweek_num = total_df[\"week_num\"]\n\ntotal_df = total_df.drop(\"week_num\")\n\ngc.collect()\n\ndef filter_ind(df:pl.DataFrame,indexes:np.array):\n    return (\n        df\n        .with_row_index()\n        .filter(\n            pl.col(\"index\").is_in(indexes)\n        )\n        .drop(\"index\")\n    )\n\n\ncv = StratifiedGroupKFold(n_splits=5,shuffle=True)\n\nfor i,(train_ind,valid_ind) in enumerate(\n    cv.split(total_df,total_df.select(\"target\"),\n    groups=week_num)):\n    print(f\"Training start for LGBClassifier: {i+1}\")\n    lgb_model = lgb.LGBMClassifier(**class_params)\n    lgb_model.fit(\n        (\n            total_df\n            .drop(\"target\")\n            .pipe(filter_ind,train_ind)\n        ),\n        (\n            total_df\n            .select('target')\n            .pipe(filter_ind,train_ind)\n            .to_series()\n            .to_numpy()\n            .ravel()\n        ),\n        eval_set=[(\n            (\n                total_df\n                .drop(\"target\")\n                .pipe(filter_ind,valid_ind)\n            ),\n            (\n                total_df\n                .select(\"target\")\n                .pipe(filter_ind,valid_ind)\n                .to_series()\n                .to_numpy()\n                .ravel()\n            )\n        )],\n        callbacks=[lgb.log_evaluation(500),lgb.early_stopping(800)]\n    )\n    joblib.dump(lgb_model,f\"/kaggle/working/lgb_model_{i+1}.pkl\")\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python training.py","metadata":{},"execution_count":null,"outputs":[]}]}