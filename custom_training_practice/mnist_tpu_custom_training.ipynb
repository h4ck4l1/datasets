{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bismillah Hirrahamaa Nirraheem\n"
     ]
    }
   ],
   "source": [
    "print(\"Bismillah Hirrahamaa Nirraheem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys,warnings,time,re,math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import clear_output\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from typing import Literal\n",
    "from multiprocessing import Process\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor,wait\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import TFRecordWriter,TFRecordOptions,FixedLenFeature,VarLenFeature,serialize_tensor,parse_tensor,parse_single_example\n",
    "from tensorflow.data import TFRecordDataset,Dataset\n",
    "from tensorflow.train import Feature,Features,BytesList,FloatList,Int64List,Example\n",
    "from sklearn.datasets import fetch_openml\n",
    "from google.cloud import storage\n",
    "client = storage.Client(\"kaggle-406814\")\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "from contextlib2 import ExitStack\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"gs://stanfordrna/mnist/*.tfrecord\"\n",
    "all_files = tf.io.gfile.glob(PATH)\n",
    "train_files = all_files[:15]\n",
    "valid_files = all_files[15:18]\n",
    "test_files = all_files[18:]\n",
    "train_raw = TFRecordDataset(train_files,compression_type=\"GZIP\")\n",
    "test_raw = TFRecordDataset(test_files,compression_type=\"GZIP\")\n",
    "valid_raw = TFRecordDataset(valid_files,compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_feature = dict(\n",
    "    image=VarLenFeature(tf.string),\n",
    "    label = FixedLenFeature([],tf.float32)\n",
    ")\n",
    "\n",
    "def mnist_example(example):\n",
    "    example = parse_single_example(example,mnist_feature)\n",
    "    example[\"image\"] = parse_tensor(tf.sparse.to_dense(example[\"image\"])[0],out_type=tf.float32)\n",
    "    return example\n",
    "\n",
    "def create_ds(ds:tf.data.Dataset,batch_size:int,is_train:bool=False,shuffle_size:int=5000):\n",
    "    ds = ds.map(mnist_example,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda example: (example[\"image\"],example[\"label\"]),num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_train:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size,drop_remainder=True)\n",
    "    if not is_train:\n",
    "        ds = ds.cache()\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_ds = create_ds(train_raw,batch_size=BATCH_SIZE,is_train=True)\n",
    "valid_ds = create_ds(valid_raw,batch_size=BATCH_SIZE,is_train=False)\n",
    "test_ds = create_ds(test_raw,batch_size=BATCH_SIZE,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataLossError",
     "evalue": "{{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} corrupted record at 1299252 [Op:DatasetToSingleElement] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X,y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2924\u001b[0m, in \u001b[0;36mDatasetV2.get_single_element\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[1;32m   2921\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m _validate_and_encode(name)\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m structure\u001b[38;5;241m.\u001b[39mfrom_compatible_tensor_list(\n\u001b[1;32m   2923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melement_spec,\n\u001b[0;32m-> 2924\u001b[0m     \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_to_single_element\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_structure\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:1424\u001b[0m, in \u001b[0;36mdataset_to_single_element\u001b[0;34m(dataset, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1424\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   1426\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mDataLossError\u001b[0m: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} corrupted record at 1299252 [Op:DatasetToSingleElement] name: "
     ]
    }
   ],
   "source": [
    "X,y = train_ds.take(1).get_single_element()\n",
    "print(\"X shape: \",X.shape)\n",
    "print(\"y shape: \",y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
