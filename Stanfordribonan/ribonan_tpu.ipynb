{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bismillah Hirrahamaa Nirraheem\n"
     ]
    }
   ],
   "source": [
    "print(\"Bismillah Hirrahamaa Nirraheem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,warnings,time,re,math\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor,wait\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "from einops import rearrange\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "%matplotlib inline\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu_cluster = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_cluster)\n",
    "    tf.config.experimental_connect_to_cluster(tpu_cluster)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_cluster)\n",
    "    details = None\n",
    "    device = \"TPU\"\n",
    "if any(device.device_type == \"GPU\" for device in tf.config.list_physical_devices()):\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"GPU\")\n",
    "    device = \"GPU\"\n",
    "    details = tf.config.experimental.get_device_details(tf.config.list_physical_devices(\"GPU\")[0])\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"CPU\")\n",
    "    device = \"CPU\"\n",
    "    details = tf.config.experimental.get_device_details(tf.config.list_physical_devices(\"CPU\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"gs://stanfordrna/ribo/train_*.tfrecord\"\n",
    "total_files = tf.io.gfile.glob(PATH)\n",
    "train_files = total_files[:150]\n",
    "val_files = total_files[150:160]\n",
    "test_files = total_files[160:]\n",
    "train_raw_ds = tf.data.TFRecordDataset(train_files,compression_type=\"GZIP\")\n",
    "val_raw_ds = tf.data.TFRecordDataset(val_files,compression_type=\"GZIP\")\n",
    "test_raw_ds = tf.data.TFRecordDataset(test_files,compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_feature = dict(\n",
    "    seq_id = tf.io.FixedLenFeature([],tf.string),\n",
    "    seq = tf.io.VarLenFeature(tf.string),\n",
    "    dataset_name_2a3 = tf.io.FixedLenFeature([],tf.string),\n",
    "    dataset_name_dms = tf.io.FixedLenFeature([],tf.string),\n",
    "    reads_2a3 = tf.io.FixedLenFeature([],tf.string),\n",
    "    reads_dms = tf.io.FixedLenFeature([],tf.string),\n",
    "    signal_to_noise_2a3 = tf.io.FixedLenFeature([],tf.string),\n",
    "    signal_to_noise_dms = tf.io.FixedLenFeature([],tf.string),\n",
    "    reactivity_2a3 = tf.io.VarLenFeature(tf.string),\n",
    "    reactivity_dms = tf.io.VarLenFeature(tf.string),\n",
    "    reactivity_error_2a3 = tf.io.VarLenFeature(tf.string),\n",
    "    reactivity_error_dms = tf.io.VarLenFeature(tf.string),\n",
    "    sn_filter_2a3 = tf.io.FixedLenFeature([],tf.string),\n",
    "    sn_filter_dms = tf.io.FixedLenFeature([],tf.string),\n",
    "    length = tf.io.FixedLenFeature([],tf.string),\n",
    "    bpp_matrix = tf.io.VarLenFeature(tf.string),\n",
    "    bracket_seq = tf.io.VarLenFeature(tf.string)\n",
    "    )\n",
    "\n",
    "def rna_example(example):\n",
    "    example = tf.io.parse_single_example(example, rna_feature)\n",
    "\n",
    "    ### Dense Features\n",
    "    example[\"seq_id\"] = tf.io.parse_tensor(example[\"seq_id\"], out_type=tf.string)\n",
    "    example[\"reads_2a3\"] = tf.io.parse_tensor(example[\"reads_2a3\"], out_type=tf.float32)\n",
    "    example[\"reads_dms\"] = tf.io.parse_tensor(example[\"reads_dms\"], out_type=tf.float32)\n",
    "    example[\"sn_filter_2a3\"] = tf.io.parse_tensor(example[\"sn_filter_2a3\"], out_type=tf.float32)\n",
    "    example[\"sn_filter_dms\"] = tf.io.parse_tensor(example[\"sn_filter_dms\"], out_type=tf.float32)\n",
    "    example[\"dataset_name_2a3\"] = tf.io.parse_tensor(example[\"dataset_name_2a3\"], out_type=tf.string)\n",
    "    example[\"dataset_name_dms\"] = tf.io.parse_tensor(example[\"dataset_name_dms\"], out_type=tf.string)\n",
    "    example[\"signal_to_noise_2a3\"] = tf.io.parse_tensor(example[\"signal_to_noise_2a3\"], out_type=tf.float32)\n",
    "    example[\"signal_to_noise_dms\"] = tf.io.parse_tensor(example[\"signal_to_noise_dms\"], out_type=tf.float32)\n",
    "    example[\"length\"] = tf.io.parse_tensor(example[\"length\"], out_type=tf.float32)\n",
    "\n",
    "    ### Sparse Features\n",
    "    example[\"seq\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"seq\"])[0], out_type=tf.float32)\n",
    "    example[\"reactivity_2a3\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"reactivity_2a3\"])[0], out_type=tf.float32)\n",
    "    example[\"reactivity_dms\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"reactivity_dms\"])[0], out_type=tf.float32)\n",
    "    example[\"reactivity_error_2a3\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"reactivity_error_2a3\"])[0], out_type=tf.float32)\n",
    "    example[\"reactivity_error_dms\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"reactivity_error_dms\"])[0], out_type=tf.float32)\n",
    "    example[\"bpp_matrix\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"bpp_matrix\"])[0], out_type=tf.float32)\n",
    "    example[\"bracket_seq\"] = tf.io.parse_tensor(tf.sparse.to_dense(example[\"bracket_seq\"])[0], out_type=tf.float32)\n",
    "\n",
    "    return example\n",
    "\n",
    "train_modified_ds = train_raw_ds.map(rna_example, tf.data.AUTOTUNE)\n",
    "val_modified_ds = val_raw_ds.map(rna_example, tf.data.AUTOTUNE)\n",
    "test_modified_ds = test_raw_ds.map(rna_example, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  :  b'8cdfeef009ea'\n",
      "\n",
      "\n",
      "\n",
      "seq  :  [3. 3. 3. 0. 0. 2. 3. 0. 2. 4. 2. 3. 0. 3. 4. 0. 3. 0. 3. 4.]\n",
      "seq shape : (170,)\n",
      "\n",
      "\n",
      "\n",
      "dataset_name_2a3  :  b'15k_2A3'\n",
      "\n",
      "\n",
      "\n",
      "dataset_name_dms  :  b'15k_DMS'\n",
      "\n",
      "\n",
      "\n",
      "reads_2a3 :  2343.0\n",
      "\n",
      "\n",
      "\n",
      "reads_dms :  1668.0\n",
      "\n",
      "\n",
      "\n",
      "signal_to_noise_2a3 :  0.944\n",
      "\n",
      "\n",
      "\n",
      "signal_to_noise_dms :  0.972\n",
      "\n",
      "\n",
      "\n",
      "reactivity_2a3  :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "shape of reactivity_2a3 : (206,)\n",
      "\n",
      "\n",
      "\n",
      "reactivity_dms  :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "shape of reactivity_dms : (206,)\n",
      "\n",
      "\n",
      "\n",
      "reactivity_error_2a3 :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "shape of reactivity_error_2a3 (206,)\n",
      "\n",
      "\n",
      "\n",
      "reactivity_error_dms :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "shape of reactivity_error_dms (206,)\n",
      "\n",
      "\n",
      "\n",
      "sn_filter_2a3 :  0.0\n",
      "\n",
      "\n",
      "\n",
      "sn_filter_dms :  0.0\n",
      "\n",
      "\n",
      "\n",
      "Lenght : 170.0\n",
      "\n",
      "\n",
      "\n",
      "bpp_matrix : [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "bpp_matrix shape : (170, 170)\n",
      "\n",
      "\n",
      "\n",
      "bracket_sequence : [8. 8. 8. 8. 8. 0. 0. 0. 0. 0. 0. 8. 8. 8. 8. 8. 1. 1. 1. 1.]\n",
      "bracket_sequence shape : (170,)\n"
     ]
    }
   ],
   "source": [
    "single_example = train_modified_ds.take(1).get_single_element()\n",
    "print(\"id\",\" : \",single_example[\"seq_id\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"seq\",\" : \",single_example[\"seq\"].numpy()[:20])\n",
    "print(\"seq shape :\",single_example[\"seq\"].numpy().shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"dataset_name_2a3\",\" : \",single_example[\"dataset_name_2a3\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"dataset_name_dms\",\" : \",single_example[\"dataset_name_dms\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"reads_2a3\",\": \",single_example[\"reads_2a3\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"reads_dms\",\": \",single_example[\"reads_dms\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"signal_to_noise_2a3\",\": \",single_example[\"signal_to_noise_2a3\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"signal_to_noise_dms\",\": \",single_example[\"signal_to_noise_dms\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"reactivity_2a3\",\" : \",single_example[\"reactivity_2a3\"].numpy()[:20])\n",
    "print(\"shape of reactivity_2a3 :\",single_example[\"reactivity_2a3\"].shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"reactivity_dms\",\" : \",single_example[\"reactivity_dms\"].numpy()[:20])\n",
    "print(\"shape of reactivity_dms :\",single_example[\"reactivity_dms\"].shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"reactivity_error_2a3\",\": \",single_example[\"reactivity_error_2a3\"].numpy()[:20])\n",
    "print(\"shape of reactivity_error_2a3\",single_example[\"reactivity_error_2a3\"].shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"reactivity_error_dms\",\": \",single_example[\"reactivity_error_dms\"].numpy()[:20])\n",
    "print(\"shape of reactivity_error_dms\",single_example[\"reactivity_error_dms\"].shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"sn_filter_2a3\",\": \",single_example[\"sn_filter_2a3\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"sn_filter_dms\",\": \",single_example[\"sn_filter_dms\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"Lenght :\",single_example[\"length\"].numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"bpp_matrix :\",single_example[\"bpp_matrix\"].numpy()[:5,:5])\n",
    "print(\"bpp_matrix shape :\",single_example[\"bpp_matrix\"].numpy().shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"bracket_sequence :\",single_example[\"bracket_seq\"].numpy()[:20])\n",
    "print('bracket_sequence shape :',single_example[\"bracket_seq\"].numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_map = {\"A\":1,\"C\":2,\"G\":3,\"U\":4,\"START\":4,\"END\":5,\"EMPTY\":0}\n",
    "bracket_map = {\"(\":1,\")\":2,\"[\":3,\"]\":4,\"{\":5,\"}\":6,\"<\":7,\">\":8,\".\":9,\"START\":10,\"END\":11,\"EMPTY\":0}\n",
    "\n",
    "def convert_and_pad(ex,Lmax:int=206,shift=True,sn_filter:bool=True):\n",
    "\n",
    "    l = tf.shape(ex[\"seq\"],tf.int32)[0]\n",
    "\n",
    "    if not shift:\n",
    "        shift = 0\n",
    "    else:\n",
    "        shift = tf.random.uniform(shape=[1],minval=0,maxval=Lmax-l+1,dtype=tf.int32)[0]\n",
    "\n",
    "    # Sequence Processing and Mask Processing\n",
    "    seq = ex[\"seq\"] + 1\n",
    "    mask = tf.math.greater(tf.pad(seq,[[shift+1,Lmax-l-shift+1]]),0)\n",
    "    seq = tf.pad(seq,[[1,0]],constant_values=seq_map[\"START\"])                               # seq_map[\"START\"]\n",
    "    seq = tf.pad(seq,[[0,1]],constant_values=seq_map[\"END\"])                                 # seq_map[\"END\"]\n",
    "    seq = tf.pad(seq,[[shift,Lmax-l-shift]])                                                 # seq_map[\"EMPTY\"]\n",
    "\n",
    "\n",
    "    # Bracket Processing\n",
    "    brac = ex[\"bracket_seq\"] + 1\n",
    "    brac = tf.pad(brac,[[1,0]],constant_values=bracket_map[\"START\"])                            # bracket_map[\"START\"]\n",
    "    brac = tf.pad(brac,[[0,1]],constant_values=bracket_map[\"END\"])                              # bracket_map[\"END\"]\n",
    "    brac = tf.pad(brac,[[shift,Lmax-l-shift]],constant_values=bracket_map[\"EMPTY\"])             # bracket_map[\"EMPTY\"]\n",
    "\n",
    "    # Reactivity Processing\n",
    "    reac = tf.stack([ex[\"reactivity_2a3\"][:l],ex[\"reactivity_dms\"][:l]],axis=-1)\n",
    "    reac = tf.pad(reac,[[shift+1,Lmax+1-l-shift],[0,0]],constant_values=np.nan)\n",
    "\n",
    "    # SN_filter\n",
    "    sn = (ex[\"sn_filter_2a3\"] > 0) & (ex[\"sn_filter_dms\"] > 0)\n",
    "\n",
    "    # BPPMatrix\n",
    "    bppm = ex[\"bpp_matrix\"][:l,:l]\n",
    "    bppm = tf.pad(bppm,[[shift+1,Lmax+1-l-shift],[shift+1,Lmax+1-l-shift]])\n",
    "\n",
    "    if sn_filter:\n",
    "        return (seq,brac,mask,sn,bppm),(reac,mask)\n",
    "    return (seq,brac,mask,bppm),(reac,mask)\n",
    "\n",
    "\n",
    "BATCH_SIZE = strategy.num_replicas_in_sync * 16 if (device == \"TPU\") else 32\n",
    "\n",
    "\n",
    "def create_train_ds(dataset,batch_size:int=BATCH_SIZE):\n",
    "    dataset = dataset.map(convert_and_pad,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(20000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def create_val_ds(dataset,batch_size:int=BATCH_SIZE):\n",
    "    dataset = dataset.map(convert_and_pad,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.cache()\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def create_test_ds(dataset,batch_size:int=BATCH_SIZE):\n",
    "    dataset = dataset.map(convert_and_pad,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = create_train_ds(train_modified_ds,batch_size=BATCH_SIZE)\n",
    "val_ds = create_val_ds(val_modified_ds,batch_size=BATCH_SIZE)\n",
    "test_ds = create_test_ds(test_modified_ds,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train_ds.take(1).get_single_element()\n",
    "seq_input = X[0]\n",
    "bracket_input = X[1]\n",
    "mask = X[2]\n",
    "sn_filter = X[3]\n",
    "bpp_matrix = X[4]\n",
    "reactivity = y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetric2A3(keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self,**kwargs):\n",
    "        super(CustomMetric2A3,self).__init__(**kwargs)\n",
    "        self.mae = self.add_weight(name=\"mae_2a3_metric\",initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self,y_true,y_pred, **kwargs):\n",
    "        y_mask_metric = y_true[1]\n",
    "        y_true_metric = y_true[0][...,0]\n",
    "        y_pred_metric = y_pred[...,0]\n",
    "        y_pred_metric = tf.clip_by_value(y_pred_metric[y_mask_metric],clip_value_min=0,clip_value_max=1)\n",
    "        y_true_metric = tf.clip_by_value(y_true_metric[y_mask_metric],clip_value_min=0,clip_value_max=1)\n",
    "        mae = tf.abs(tf.subtract(y_true_metric,y_pred_metric))\n",
    "        mae = tf.reduce_mean(mae[~tf.math.is_nan(mae)])\n",
    "        self.mae.assign_add(mae)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mae\n",
    "    \n",
    "    def reset_state(self):\n",
    "        return self.mae.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricDMS(keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self,**kwargs):\n",
    "        super(CustomMetricDMS,self).__init__(**kwargs)\n",
    "        self.mae = self.add_weight(name=\"mae_dms_metric\",initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self,y_true,y_pred, **kwargs):\n",
    "        y_mask_metric = y_true[1]\n",
    "        y_true_metric = y_true[0][...,1]\n",
    "        y_pred_metric = y_pred[...,1]\n",
    "        y_pred_metric = tf.clip_by_value(y_pred_metric[y_mask_metric],clip_value_min=0,clip_value_max=1)\n",
    "        y_true_metric = tf.clip_by_value(y_true_metric[y_mask_metric],clip_value_min=0,clip_value_max=1)\n",
    "        mae = tf.abs(tf.subtract(y_true_metric,y_pred_metric))\n",
    "        mae = tf.reduce_mean(mae[~tf.math.is_nan(mae)])\n",
    "        self.mae.assign_add(mae)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mae\n",
    "    \n",
    "    def reset_state(self):\n",
    "        return self.mae.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4863567\n",
      "0.4832784\n"
     ]
    }
   ],
   "source": [
    "cust_metric_2a3 = CustomMetric2A3()\n",
    "cust_metric_dms = CustomMetricDMS()\n",
    "y_pred = tf.random.uniform(shape=y[0].shape,minval=-1,maxval=2)\n",
    "cust_metric_2a3.update_state(y,y_pred)\n",
    "cust_metric_dms.update_state(y,y_pred)\n",
    "print(cust_metric_2a3.result().numpy())\n",
    "print(cust_metric_dms.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(keras.losses.Loss):\n",
    "\n",
    "    def __init__(self,**kwargs):\n",
    "        super(CustomLoss,self).__init__(**kwargs)\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        y_mask = tf.cast(y_true[1],tf.bool)\n",
    "        y_true = y_true[0]\n",
    "        y_pred = tf.clip_by_value(y_pred[y_mask],clip_value_min=0,clip_value_max=1)\n",
    "        y_true = tf.clip_by_value(y_true[y_mask],clip_value_max=1,clip_value_min=0)\n",
    "        mae = tf.reduce_mean(tf.math.abs(tf.math.subtract(y_true,y_pred)),axis=-1)\n",
    "        return tf.reduce_mean(mae[~tf.math.is_nan(mae)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48453128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_loss = CustomLoss()\n",
    "cust_loss(y,y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticPosEncoding(keras.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "            vocab_size:int,\n",
    "            d_model,\n",
    "            length,\n",
    "            casting,\n",
    "            **kwargs):\n",
    "\n",
    "        super(StaticPosEncoding,self).__init__(**kwargs)\n",
    "\n",
    "        assert d_model%2 == 0,\"Depth of model(d_model) should be even\"\n",
    "\n",
    "        d_model = d_model//2\n",
    "        positions = np.arange(length)[:,np.newaxis]\n",
    "        angles = np.arange(d_model)[np.newaxis,:]/d_model\n",
    "        angles = 1/(10000**angles)\n",
    "        angle_rads = positions * angles\n",
    "        if casting == \"concat\":\n",
    "            encode = tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
    "        else:\n",
    "            encode = np.zeros(shape=[length,d_model])\n",
    "            encode[:,::2] = tf.sin(angle_rads)\n",
    "            encode[:,1::2] = tf.cos(angle_rads)\n",
    "\n",
    "        self.encode = tf.cast(encode,tf.float32)\n",
    "        self.factor = tf.sqrt(tf.cast(d_model,tf.float32))\n",
    "        self.embedding_layer = keras.layers.Embedding(vocab_size,d_model*2,mask_zero=True)\n",
    "\n",
    "    def compute_mask(self, *args,**kwargs):\n",
    "        return self.embedding_layer.compute_mask(*args,**kwargs)\n",
    "\n",
    "    def call(self,x):\n",
    "        seq_l = tf.shape(x)[1]\n",
    "        x = self.embedding_layer(x)\n",
    "        x *= self.factor\n",
    "        return x + self.encode[tf.newaxis,:seq_l,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 208, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encode = StaticPosEncoding(vocab_size=len(seq_map),d_model=512,length=2048,casting=\"concat\")\n",
    "pos_encode_output = pos_encode(X[0])\n",
    "pos_encode_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(keras.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 n_filters,\n",
    "                 ksize,\n",
    "                 padding,\n",
    "                 bias,\n",
    "                 drop_rate,\n",
    "                 norm,\n",
    "                 **kwargs):\n",
    "        super(ConvolutionLayer,self).__init__(**kwargs)\n",
    "        self.conv_layers = [keras.layers.Conv2D(filters=n_filters,kernel_size=ksize,padding=padding,use_bias=bias) for _ in range(n_layers)]\n",
    "        self.perm = keras.layers.Permute(dims=[3,1,2])\n",
    "        self.norm = keras.layers.LayerNormalization() if norm == \"layer_norm\" else keras.layers.BatchNormalization()\n",
    "        self.drop = keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self,x):\n",
    "        x = tf.expand_dims(x,axis=-1)\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        x = self.perm(x)\n",
    "        return self.norm(self.drop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 6, 208, 208])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = ConvolutionLayer(n_layers=3,n_filters=6,ksize=3,padding=\"same\",bias=False,drop_rate=0.1,norm=\"layer_norm\")\n",
    "bpp_conv_ouput = conv(bpp_matrix)\n",
    "bpp_conv_ouput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.random.uniform(shape=[32,208,208])\n",
    "# first_a = a[:,tf.newaxis,:,:]\n",
    "# last_a = a[...,tf.newaxis]\n",
    "# conv1 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",data_format=\"channels_first\",use_bias=False)\n",
    "# conv2 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",data_format=\"channels_first\",use_bias=False)\n",
    "# conv3 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",data_format=\"channels_first\",use_bias=False)\n",
    "# first_a = conv1(first_a)\n",
    "# first_a = conv2(first_a)\n",
    "# first_a = conv3(first_a)\n",
    "# weights1 = conv1.get_weights()\n",
    "# weights2 = conv2.get_weights()\n",
    "# weights3 = conv3.get_weights()\n",
    "# con1 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",use_bias=False)\n",
    "# con2 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",use_bias=False)\n",
    "# con3 = keras.layers.Conv2D(filters=6,kernel_size=3,padding=\"same\",use_bias=False)\n",
    "# transopose_layer = keras.layers.Permute(dims=[3,1,2])\n",
    "# _ = con1(tf.random.uniform(shape=[32,208,208,1]))\n",
    "# _ = con2(tf.random.uniform(shape=[32,208,208,6]))\n",
    "# _ = con3(tf.random.uniform(shape=[32,208,208,6]))\n",
    "# con1.set_weights(weights1)\n",
    "# con2.set_weights(weights2)\n",
    "# con3.set_weights(weights3)\n",
    "# last_a = con1(last_a)\n",
    "# last_a = con2(last_a)\n",
    "# last_a = con3(last_a)\n",
    "# last_a = transopose_layer(last_a)\n",
    "# tf.reduce_mean(tf.cast(tf.equal(first_a,last_a),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutedAttention(keras.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "            num_heads,\n",
    "            key_dim,\n",
    "            **kwargs):\n",
    "\n",
    "        super(ConvolutedAttention,self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.wq = keras.layers.Dense(num_heads*key_dim)\n",
    "        self.wv = keras.layers.Dense(num_heads*key_dim)\n",
    "        self.wk = keras.layers.Dense(num_heads*key_dim)\n",
    "        self.dense = keras.layers.Dense(key_dim)\n",
    "        self.first_drop = keras.layers.Dropout(0.1)\n",
    "        self.last_drop = keras.layers.Dropout(0.1)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.factor = tf.math.rsqrt(tf.constant(key_dim,tf.float32))\n",
    "        self.softmax = keras.layers.Softmax()\n",
    "        self.add = keras.layers.Add()\n",
    "\n",
    "\n",
    "    def call(self,query,key,value,convoluted=None,return_attention_scores=False,mask=None):\n",
    "\n",
    "\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        seq_len = tf.shape(query)[1]\n",
    "        q = self.wq(query)\n",
    "        k = self.wk(key)\n",
    "        v = self.wv(value)\n",
    "\n",
    "        q = tf.transpose(tf.reshape(q,shape=[batch_size,-1,self.num_heads,self.key_dim]),perm=[0,2,1,3])\n",
    "        k = tf.transpose(tf.reshape(k,shape=[batch_size,-1,self.num_heads,self.key_dim]),perm=[0,2,1,3])\n",
    "        v = tf.transpose(tf.reshape(v,shape=[batch_size,-1,self.num_heads,self.key_dim]),perm=[0,2,1,3])\n",
    "\n",
    "        attention_score = self.factor * tf.matmul(q,k,transpose_b=True)\n",
    "        attention_score = self.first_drop(self.softmax(attention_score + convoluted))\n",
    "\n",
    "        attention_out = tf.reshape(tf.transpose(tf.matmul(attention_score,v),perm=[0,2,1,3]),shape=[batch_size,seq_len,-1])\n",
    "        attention_out = self.last_drop(self.dense(attention_out))\n",
    "        attention_out = self.layer_norm(self.add([attention_out,query]))\n",
    "\n",
    "        if return_attention_scores:\n",
    "            return attention_out,attention_score\n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 208, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_attn = ConvolutedAttention(num_heads=6,key_dim=512)\n",
    "conv_attn_out = conv_attn(query=pos_encode_output,key=pos_encode_output,value=pos_encode_output,convoluted=bpp_conv_ouput)\n",
    "conv_attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(keras.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "            feed_out_unit,\n",
    "            feed_forward,\n",
    "            feed_forward_drop,\n",
    "            **kwargs):\n",
    "\n",
    "        super(FeedForward,self).__init__(**kwargs)\n",
    "        self.dense_out = keras.layers.Dense(feed_out_unit)\n",
    "        self.feed_forward = keras.layers.Dense(feed_forward)\n",
    "        self.feed_forward_drop = keras.layers.Dropout(feed_forward_drop)\n",
    "        self.norm = keras.layers.LayerNormalization()\n",
    "        self.add = keras.layers.Add()\n",
    "\n",
    "    def call(self,x):\n",
    "\n",
    "        x_copy = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dense_out(x)\n",
    "        x = self.feed_forward_drop(x)\n",
    "        x = self.feed_forward_drop(x)\n",
    "        return self.norm(self.add([x_copy,x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 208, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForward(feed_out_unit=512,feed_forward=2048,feed_forward_drop=0.2,norm=\"layer_norm\")\n",
    "feed_out = feed_forward(conv_attn_out)\n",
    "feed_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoderUint(keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "            encoder_convolution_layers:int,\n",
    "            encoder_convolution_ksize:int,\n",
    "            encoder_convolution_padding:str,\n",
    "            encoder_convolution_bias:bool,\n",
    "            encoder_convolution_drop_rate:float,\n",
    "            encoder_convolution_norm:str,\n",
    "            encoder_convolution_num_heads:int,\n",
    "            encoder_cross_attention_num_heads:int,\n",
    "            encoder_key_dim:int,\n",
    "            encoder_feed_out_units:int,\n",
    "            encoder_feed_forward_units:int,\n",
    "            encoder_feed_forward_drop_rate:float,\n",
    "            **kwargs):\n",
    "\n",
    "        super(SequenceEncoderUint,self).__init__(**kwargs)\n",
    "\n",
    "        self.convolution_layer = ConvolutionLayer(\n",
    "            encoder_convolution_layers,\n",
    "            encoder_convolution_num_heads,\n",
    "            encoder_convolution_ksize,\n",
    "            encoder_convolution_padding,\n",
    "            encoder_convolution_bias,\n",
    "            encoder_convolution_drop_rate,\n",
    "            encoder_convolution_norm\n",
    "            )\n",
    "        \n",
    "        self.convoluted_attention = ConvolutedAttention(\n",
    "            encoder_convolution_num_heads,\n",
    "            encoder_key_dim\n",
    "            )\n",
    "        \n",
    "        self.encoder_cross_attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=encoder_cross_attention_num_heads,\n",
    "            key_dim=encoder_key_dim\n",
    "            )\n",
    "        \n",
    "        self.feed_forward = FeedForward(\n",
    "            encoder_feed_out_units,\n",
    "            encoder_feed_forward_units,\n",
    "            encoder_feed_forward_drop_rate\n",
    "            )\n",
    "        \n",
    "    def call(self,sequence_encode_out,bpp_matrix,bracket_encode_out):\n",
    "\n",
    "        convolution_out = self.convolution_layer(bpp_matrix)\n",
    "        convolutedattention_out = self.convoluted_attention(\n",
    "            query=sequence_encode_out,\n",
    "            key=sequence_encode_out,\n",
    "            value=sequence_encode_out,\n",
    "            convoluted=convolution_out\n",
    "            )\n",
    "        crossattention_out = self.encoder_cross_attention(\n",
    "            query=convolutedattention_out,\n",
    "            key=bracket_encode_out,\n",
    "            value=bracket_encode_out\n",
    "            )\n",
    "        feed_out = self.feed_forward(crossattention_out)\n",
    "\n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 208, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_encoder = SequenceEncoderUint(\n",
    "    encoder_convolution_layers=3,\n",
    "    encoder_convolution_ksize=3,\n",
    "    encoder_convolution_padding='same',\n",
    "    encoder_convolution_bias=False,\n",
    "    encoder_convolution_drop_rate=0.1,\n",
    "    encoder_convolution_num_heads=6,\n",
    "    encoder_cross_attention_num_heads=6,\n",
    "    encoder_convolution_norm=\"layer_norm\",\n",
    "    encoder_key_dim=512,\n",
    "    encoder_feed_forward_drop_rate=0.1,\n",
    "    encoder_feed_forward_units=2048,\n",
    "    encoder_feed_out_units=512\n",
    "    )\n",
    "\n",
    "seq_encode_out = tf.random.uniform(shape=[32,208,512])\n",
    "bpp_mat =  tf.random.uniform(shape=[32,208,208])\n",
    "bracket_sequence_out = tf.random.uniform(shape=[32,208,512])\n",
    "seq_encoder_out = seq_encoder(seq_encode_out,bpp_mat,bracket_sequence_out)\n",
    "seq_encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BracketEncoderUnit(keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "            encoder_convolution_layers:int,\n",
    "            encoder_convolution_ksize:int,\n",
    "            encoder_convolution_padding:int,\n",
    "            encoder_convolution_bias:int,\n",
    "            encoder_convolution_drop_rate:float,\n",
    "            encoder_convolution_norm:int,\n",
    "            encoder_convolution_num_heads:int,\n",
    "            encoder_key_dim:int,\n",
    "            encoder_feed_out_units:int,\n",
    "            encoder_feed_forward_units:int,\n",
    "            encoder_feed_forward_drop_rate:float,\n",
    "            **kwargs):\n",
    "        \n",
    "\n",
    "        super(BracketEncoderUnit,self).__init__(**kwargs)\n",
    "\n",
    "        self.convolution_layer = ConvolutionLayer(\n",
    "            encoder_convolution_layers,\n",
    "            encoder_convolution_num_heads,\n",
    "            encoder_convolution_ksize,\n",
    "            encoder_convolution_padding,\n",
    "            encoder_convolution_bias,\n",
    "            encoder_convolution_drop_rate,\n",
    "            encoder_convolution_norm\n",
    "        )\n",
    "\n",
    "        self.convoluted_attention = ConvolutedAttention(\n",
    "            encoder_convolution_num_heads,\n",
    "            encoder_key_dim\n",
    "        )\n",
    "\n",
    "        self.feed_forward = FeedForward(\n",
    "            encoder_feed_out_units,\n",
    "            encoder_feed_forward_units,\n",
    "            encoder_feed_forward_drop_rate\n",
    "        )\n",
    "\n",
    "    def call(self,bracket_encode_out,bpp_matrix):\n",
    "\n",
    "        convolution_out = self.convolution_layer(bpp_matrix)\n",
    "        convoluted_attention_out = self.convoluted_attention(\n",
    "            query = bracket_encode_out,\n",
    "            key = bracket_encode_out,\n",
    "            value = bracket_encode_out,\n",
    "            convoluted = convolution_out\n",
    "        )\n",
    "        feed_out = self.feed_forward(convoluted_attention_out)\n",
    "        \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 208, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bracket_seq_encoder = BracketEncoderUnit(\n",
    "    encoder_convolution_layers=3,\n",
    "    encoder_convolution_ksize=3,\n",
    "    encoder_convolution_padding=\"same\",\n",
    "    encoder_convolution_bias=False,\n",
    "    encoder_convolution_drop_rate=0.1,\n",
    "    encoder_convolution_num_heads=6,\n",
    "    encoder_convolution_norm=\"layer_norm\",\n",
    "    encoder_key_dim=512,\n",
    "    encoder_feed_forward_drop_rate=0.1,\n",
    "    encoder_feed_forward_units=2048,\n",
    "    encoder_feed_out_units=512\n",
    ")\n",
    "\n",
    "bracket_encode_out = tf.random.uniform(shape=[32,208,512])\n",
    "bpp_mat = tf.random.uniform(shape=[32,208,208])\n",
    "bracket_seq_encoder_out = bracket_seq_encoder(bracket_encode_out,bpp_mat)\n",
    "bracket_seq_encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "            \n",
    "            **kwargs):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
