{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkbvpMvqpxQ+UIVxWA3rbD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/Spa_to_En_NeuralTranslationNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HdZaEJRSQzRY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "import os,warnings\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "os.environ['TF_MIN_LOG_LEVEL'] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)\n",
        "strategy = tf.distribute.experimental.CentralStorageStrategy()"
      ],
      "metadata": {
        "id": "NLUsujJYTDD0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/job:localhost\"):\n",
        "    file_path = keras.utils.get_file(fname=\"/content/spa_to_en.zip\",origin=\"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\")\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"/content/en_to_spa\")\n",
        "    with open(\"/content/en_to_spa/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "    new_text = text.replace(\"¿\",\"\").replace(\"¡\",\"\")\n",
        "    full_text = [line.split(\"\\t\") for line in new_text.splitlines()]\n",
        "    en_text,es_text = zip(*full_text)\n",
        "    total_size = len(en_text) # 189117"
      ],
      "metadata": {
        "id": "HJx1Fj6lXXoe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layers(vocab_size=1000,sequence_length=50):\n",
        "    en_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    es_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    en_vec_layer.adapt(en_text)\n",
        "    es_vec_layer.adapt([f\"sos {sentence} eos\" for sentence in es_text])\n",
        "    return en_vec_layer,es_vec_layer"
      ],
      "metadata": {
        "id": "D1vyDpKehOnn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(en_text,es_text,es_vec_layer,train_size):\n",
        "    X_train = tf.constant(en_text[:train_size])\n",
        "    X_valid = tf.constant(en_text[train_size:])\n",
        "    X_dec_train = tf.constant([f\"sos {sentence}\" for sentence in es_text[:train_size]])\n",
        "    X_dec_valid = tf.constant([f\"sos {sentence}\" for sentence in es_text[train_size:]])\n",
        "    y_train = es_vec_layer([f\"{sentence} eos\" for sentence in es_text[:train_size]])\n",
        "    y_valid = es_vec_layer([f\"{sentence} eos\" for sentence in es_text[train_size:]])\n",
        "    return (X_train,X_dec_train),y_train,(X_valid,X_dec_valid),y_valid"
      ],
      "metadata": {
        "id": "O7CnNqokk-n6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLP(keras.Model):\n",
        "\n",
        "    def __init__(self,en_vec_layer,es_vec_layer,vocab_size=1000,embed_size=128,**kwargs):\n",
        "\n",
        "        super(NLP,self).__init__(**kwargs)\n",
        "        self.en_vec_layer = en_vec_layer\n",
        "        self.es_vec_layer = es_vec_layer\n",
        "        self.en_embed = keras.layers.Embedding(vocab_size,embed_size)\n",
        "        self.es_embed = keras.layers.Embedding(vocab_size,embed_size)\n",
        "        self.en_encoder = keras.layers.LSTM(512,return_state=True)\n",
        "        self.es_decoder = keras.layers.LSTM(512,return_sequences=True)\n",
        "        self.out = keras.layers.Dense(vocab_size,\"softmax\")\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        en_input = inputs[0]\n",
        "        es_input = inputs[1]\n",
        "        en_encoded_out = self.en_vec_layer(en_input)\n",
        "        es_encoded_out = self.es_vec_layer(es_input)\n",
        "        en_embed_out = self.en_embed(en_encoded_out)\n",
        "        es_embed_out = self.es_embed(es_encoded_out)\n",
        "        encoder_out,*en_state = self.en_encoder(en_embed_out)\n",
        "        decoder_out = self.es_decoder(es_embed_out,initial_state=en_state)\n",
        "        dense_out = self.out(decoder_out)\n",
        "        return dense_out"
      ],
      "metadata": {
        "id": "5N3TGteHmN3i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    train_size = 100_000\n",
        "    valid_size = total_size-train_size\n",
        "    BATCH_SIZE = 50*8\n",
        "    en_vec_layer,es_vec_layer = get_layers()\n",
        "    X_train,y_train,X_valid,y_valid = get_dataset(en_text,es_text,es_vec_layer,train_size=train_size)\n",
        "    nlp_model = NLP(en_vec_layer,es_vec_layer)\n",
        "    nlp_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"],\n",
        "        steps_per_execution=10\n",
        "    )\n",
        "    train_steps = train_size//BATCH_SIZE\n",
        "    valid_steps = valid_size//BATCH_SIZE"
      ],
      "metadata": {
        "id": "KXkXJyr3sTwM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = nlp_model.fit(X_train,y_train,epochs=10,batch_size=BATCH_SIZE,validation_data=(X_valid,y_valid),steps_per_epoch=train_steps,validation_steps=valid_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq-Pn7f3zlVl",
        "outputId": "cdd7f5c4-bd51-44f1-b09d-e5cc5f1d812a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 232s 927ms/step - loss: 0.7402 - accuracy: 0.8878 - val_loss: 1.0953 - val_accuracy: 0.8193\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 215s 860ms/step - loss: 0.5003 - accuracy: 0.9134 - val_loss: 0.9851 - val_accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 216s 864ms/step - loss: 0.4652 - accuracy: 0.9145 - val_loss: 0.9214 - val_accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 217s 867ms/step - loss: 0.4386 - accuracy: 0.9157 - val_loss: 0.8802 - val_accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 215s 861ms/step - loss: 0.4202 - accuracy: 0.9168 - val_loss: 0.8477 - val_accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 212s 849ms/step - loss: 0.4010 - accuracy: 0.9194 - val_loss: 0.7987 - val_accuracy: 0.8453\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 214s 857ms/step - loss: 0.3764 - accuracy: 0.9234 - val_loss: 0.7556 - val_accuracy: 0.8507\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 211s 844ms/step - loss: 0.3532 - accuracy: 0.9267 - val_loss: 0.7320 - val_accuracy: 0.8546\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 210s 842ms/step - loss: 0.3290 - accuracy: 0.9305 - val_loss: 0.6979 - val_accuracy: 0.8580\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 209s 836ms/step - loss: 0.3031 - accuracy: 0.9342 - val_loss: 0.6727 - val_accuracy: 0.8610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adYAiCKiVt6q",
        "outputId": "afe61e97-bc0c-4461-8e14-d89524d73694"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model.save(\"/content/drive/MyDrive/Colab Notebooks/NLP_practice/nlp_model\",save_format=\"tf\",overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSOklFPvVzrj",
        "outputId": "b14a441f-294d-4e7b-912c-7d9dacfc6a3a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/NLP_practice/nlp_model_weights\",save_format=\"tf\",overwrite=True)"
      ],
      "metadata": {
        "id": "I9MSHpzrWNWf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "0XC6YvDsWgpr"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}