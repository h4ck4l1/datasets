{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/transformers_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHXtkRG7FqeM"
      },
      "outputs": [],
      "source": [
        "import os,sys,warnings\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install \"tensorflow-text==2.13.0\"\n",
        "    %pip install einops\n",
        "    %pip install -U kaleido\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from zipfile import ZipFile\n",
        "from typing import Literal\n",
        "import shutil\n",
        "import numpy as np\n",
        "import einops as ep\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as tftext\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    tf.config.set_logical_device_configuration(gpus[0],[tf.config.LogicalDeviceConfiguration(memory_limit=9000)])\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saHbNz_-JTTh"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "else:\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "for en,es in zip(en_text[:10],es_text[:10]):\n",
        "    print(f\"{en} ----> {es}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiULl73CNCCH"
      },
      "outputs": [],
      "source": [
        "def standardize(sentence:str):\n",
        "    sentence = tftext.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z?!,.多]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[?!.,多]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3c7atVjZV8P"
      },
      "outputs": [],
      "source": [
        "spa_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(np.array(es_text))\n",
        "    .map(standardize)\n",
        ")\n",
        "en_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(np.array(en_text))\n",
        "    .map(standardize)\n",
        ")\n",
        "\n",
        "spa_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    spa_ds,\n",
        "    vocab_size=5000,\n",
        "    reserved_tokens=[\"[PAD]\",\"[UNK]\",\"[START]\",\"[END]\"],\n",
        "    bert_tokenizer_params=dict(normalization_form=\"NFKD\"),\n",
        "    learn_params={}\n",
        ")\n",
        "\n",
        "\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    en_ds,\n",
        "    vocab_size=5000,\n",
        "    reserved_tokens=[\"[PAD]\",\"[UNK]\",\"[START]\",\"[END]\"],\n",
        "    bert_tokenizer_params=dict(normalization_form=\"NFKD\"),\n",
        "    learn_params={}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMLlRojaci1u"
      },
      "outputs": [],
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QfAx-nHm0S_"
      },
      "outputs": [],
      "source": [
        "with open(\"en_vocab.txt\",\"w\") as f:\n",
        "    for token in en_vocab:\n",
        "        print(token,file=f)\n",
        "\n",
        "\n",
        "with open(\"spa_vocab.txt\",\"w\") as f:\n",
        "    for token in spa_vocab:\n",
        "        print(token,file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYCHVIwPphRZ"
      },
      "outputs": [],
      "source": [
        "print(len(spa_vocab))\n",
        "print(len(en_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAleMXl8oHSk"
      },
      "outputs": [],
      "source": [
        "spa_tokenizer = tftext.BertTokenizer(\"spa_vocab.txt\",normalization_form=\"NFKD\")\n",
        "en_tokenizer = tftext.BertTokenizer(\"en_vocab.txt\",normalization_form=\"NFKD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwqPBXFioYvV"
      },
      "outputs": [],
      "source": [
        "spa_ex = spa_tokenizer.tokenize(standardize(es_text[1000:1010])).merge_dims(-2,-1)\n",
        "spa_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygr8yphgo6zI"
      },
      "outputs": [],
      "source": [
        "for i,j in zip(tf.strings.reduce_join(spa_tokenizer.detokenize(spa_ex),separator=\" \",axis=-1),es_text[1000:1010]):\n",
        "    print(i.numpy(),\"<----->\",j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcFPXyadvOv9"
      },
      "outputs": [],
      "source": [
        "START = tf.argmax(tf.equal(spa_vocab,\"[START]\"))\n",
        "END = tf.argmax(tf.equal(spa_vocab,\"[END]\"))\n",
        "print(START.numpy(),END.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3lo7NXDtyl3"
      },
      "outputs": [],
      "source": [
        "def up_stream_processing(sentence:str,lang:Literal[\"es\",\"en\"]):\n",
        "    assert lang in [\"es\",\"en\"], f\"{lang} is not in the opitons ['es','en']\"\n",
        "    sentence = tftext.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z?!,.多]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[?!.,多]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    if lang == \"en\":\n",
        "        tokens = en_tokenizer.tokenize(sentence).merge_dims(-2,-1)\n",
        "    else:\n",
        "        tokens = spa_tokenizer.tokenize(sentence).merge_dims(-2,-1)\n",
        "    batch_shape = tf.shape(tokens)[0]\n",
        "    start_tokens = tf.fill(dims=[batch_shape,1],value=START)\n",
        "    end_tokens = tf.fill(dims=[batch_shape,1],value=END)\n",
        "    return tf.concat([start_tokens,tokens,end_tokens],axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0Bw7xlLw34X"
      },
      "outputs": [],
      "source": [
        "def down_stream_processing(tokens,lang:Literal[\"en\",\"es\"]):\n",
        "    if lang == \"en\":\n",
        "        words = en_tokenizer.detokenize(tokens)\n",
        "    else:\n",
        "        words = spa_tokenizer.detokenize(tokens)\n",
        "    bad_tokens = \"|\".join([re.escape(tok) for tok in [\"[PAD]\",\"[START]\",\"[END]\"]])\n",
        "    mask = tf.strings.regex_full_match(words,bad_tokens)\n",
        "    full_sentence = tf.ragged.boolean_mask(words,~mask)\n",
        "    return tf.strings.reduce_join(full_sentence,axis=-1,separator=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2N0IP4kzHWN"
      },
      "outputs": [],
      "source": [
        "ex_tokens = up_stream_processing(en_text[1000:1010],\"en\")\n",
        "ex_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQCnecc_HHQ"
      },
      "outputs": [],
      "source": [
        "round_trip_tokens = down_stream_processing(ex_tokens,\"en\")\n",
        "round_trip_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BBoyy8m_OWq"
      },
      "outputs": [],
      "source": [
        "ex_tokens.row_lengths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmv8QC0cDCRZ"
      },
      "outputs": [],
      "source": [
        "en_tokens = up_stream_processing(en_text[1000:1010],\"en\")\n",
        "es_tokens = up_stream_processing(es_text[1000:1010],\"es\")\n",
        "print(en_tokens)\n",
        "print(es_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAjTP0xFTq4j"
      },
      "outputs": [],
      "source": [
        "en_tokens_len = up_stream_processing(en_text,\"en\").row_lengths().numpy()\n",
        "es_tokens_len = up_stream_processing(es_text,\"es\").row_lengths().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG2OaD7xVvjP"
      },
      "outputs": [],
      "source": [
        "all_tokens = en_tokens_len.tolist()+es_tokens_len.tolist()\n",
        "counts,bins = np.histogram(all_tokens,bins=np.linspace(0,500,501))\n",
        "fig = px.bar(y=[0]+counts.tolist(),x=bins.tolist())\n",
        "fig.update_xaxes(range=[0,200])\n",
        "fig.show(\"png\",width=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0wl8AW4dQrj"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 128\n",
        "def prepare_ds(en,es):\n",
        "    en = up_stream_processing(en,\"en\")\n",
        "    es = up_stream_processing(es,\"es\")\n",
        "    en = en[:,:MAX_TOKENS]\n",
        "    es = es[:,:MAX_TOKENS]\n",
        "    return (en.to_tensor(),es[:,:-1].to_tensor()),es[:,1:].to_tensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BA3Gvnwj20y"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "en_array = np.array(en_text)\n",
        "es_array = np.array(es_text)\n",
        "all_indices = np.random.uniform(size=len(en_text))\n",
        "train_mask = all_indices <= 0.8\n",
        "valid_mask = (all_indices > 0.8) & (all_indices <= 0.95)\n",
        "test_mask = all_indices > 0.95\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_array[train_mask],es_array[train_mask]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(prepare_ds)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "valid_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_array[valid_mask],es_array[valid_mask]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(prepare_ds)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "test_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_array[test_mask],es_array[test_mask]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(prepare_ds)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCgBlq7Fnqkd"
      },
      "outputs": [],
      "source": [
        "for (en_in,es_in),tar_in in train_ds.take(1):\n",
        "    print(en_in.shape,es_in.shape,tar_in.shape)\n",
        "    print(es_in[0,:10])\n",
        "    print(tar_in[0,:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z73J08zYnwpN"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JGIXe--B7w6"
      },
      "outputs": [],
      "source": [
        "class Shapecheck(tf.Module):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "\n",
        "        super(Shapecheck,self).__init__(**kwargs)\n",
        "        self.shapes = {}\n",
        "\n",
        "    def __call__(self,tensor,names:str):\n",
        "\n",
        "        if not tf.executing_eagerly():\n",
        "            return\n",
        "        all_dimension_names = names.split()\n",
        "        shape_of_tensor = tf.shape(tensor).numpy().tolist()\n",
        "        if len(shape_of_tensor) != len(all_dimension_names):\n",
        "            if len(shape_of_tensor) > len(all_dimension_names):\n",
        "                raise ValueError(f\"The shapes and dimensions don't match and tensor with shape {shape_of_tensor} has more dimensions than provided: {len(all_dimension_names)}\")\n",
        "            else:\n",
        "                raise ValueError(f\"The shapes and dimensions don't match and tensor with shape {shape_of_tensor} has less dimensions than provided: {len(all_dimension_names)}\")\n",
        "\n",
        "        else:\n",
        "            for _shape,_name in zip(shape_of_tensor,all_dimension_names):\n",
        "                if _name not in self.shapes:\n",
        "                    self.shapes[_name] = _shape\n",
        "                else:\n",
        "                    if self.shapes[_name] == _shape:\n",
        "                        continue\n",
        "                    else:\n",
        "                        raise ValueError(f\"The given tensor with shape{shape_of_tensor} has a mismatch with dimension {_shape} with old dimension present {self.shapes[_name]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6xNQaqJB7w7"
      },
      "outputs": [],
      "source": [
        "shape_check = Shapecheck()\n",
        "\n",
        "shape_check(tf.ones(shape=[64,15,512]),\"b s u\")\n",
        "shape_check(tf.ones(shape=[64,17,512]),\"b t u\")\n",
        "shape_check(tf.ones(shape=[64,2,17,15]),\"b n t s\")\n",
        "shape_check(tf.ones(shape=[64,15,15,512]),\"b s s u\")\n",
        "shape_check.shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcVtwKUCB7w8"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp9wNPyGB7w8"
      },
      "source": [
        "## Embedding and Positional Encoding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAPgRDhBB7w8"
      },
      "source": [
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxzWengvB7w9"
      },
      "source": [
        "- Given a sequence of tokens, both the input tokens (ENGLISH) and target tokens (SPANISH) have to be converted to vectors using a ```keras.layers.Embedding``` layer\n",
        "- The attention layers used throughout the model see their input as a set of vectors, with no order. Since the model doesn't contain any recurrent layers or convolutional layers. It needs some way to idientify word order, otherwise is would see the input sequence as a bag of words for instance,\n",
        "  - ```how are you```\n",
        "  - ```you are how```\n",
        "  - ```are you how```\n",
        "  - ```you how are```\n",
        "\n",
        "- A Transformer adds \"Positional Encoding\" to the embedding vectors. It uses a set of sines and consines at frequencies (across the sequence). By definition nearby elements will have similar position encodings\n",
        "\n",
        "- The original paper uses the following formula for calculating the positional encoding:\n",
        "$$ PE_{(pos,2i)} = sin\\Biggl( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\Biggr)$$\n",
        "\n",
        "- The Original paper uses interleaving sines and cosines but we are going to concatenate it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B0vR3QwB7w9"
      },
      "outputs": [],
      "source": [
        "EMBED_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl7lV-8yB7w9"
      },
      "outputs": [],
      "source": [
        "en_embedding = keras.layers.Embedding(len(en_vocab),EMBED_SIZE,mask_zero=True)\n",
        "es_embedding = keras.layers.Embedding(len(spa_vocab),EMBED_SIZE,mask_zero=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqiTfRbgB7w9"
      },
      "outputs": [],
      "source": [
        "def pos_encoding(length=2048,depth=EMBED_SIZE,casting:Literal[\"interleaving\",\"concatenated\"]=\"interleaving\"):\n",
        "    assert depth%2==0,f\"The given depth: {depth} is not even please give even depth\"\n",
        "    pos = np.arange(depth//2)[np.newaxis,:]/(depth//2)\n",
        "    positions = np.arange(length)[:,np.newaxis]\n",
        "    angles_rads = positions * 1/(10000**pos)\n",
        "    if casting == \"concatenated\":\n",
        "        return tf.cast(np.concatenate([np.sin(angles_rads),np.cos(angles_rads)],axis=-1),tf.float32)\n",
        "    else:\n",
        "        embed = np.zeros(shape=[length,depth])\n",
        "        print(embed.shape)\n",
        "        embed[:,::2] = np.sin(angles_rads)\n",
        "        embed[:,1::2] = np.cos(angles_rads)\n",
        "        return tf.cast(embed,tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_TyAErVB7w-"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(img=pos_encoding(casting=\"interleaving\").numpy().T,color_continuous_scale=px.colors.sequential.RdBu,origin=\"lower\",title=\"Interleaving sines and cosines from original paper\")\n",
        "fig.update_layout(margin=dict(l=20,r=0,t=100,b=0),title=dict(font=dict(size=30)))\n",
        "fig.show(\"png\",width=2000,height=700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvH8_JY0B7w-"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(img=pos_encoding(casting=\"concatenated\").numpy().T,color_continuous_scale=px.colors.sequential.RdBu,origin=\"lower\",title=\"Concatenated sines and cosines for tutorial purposes\")\n",
        "fig.update_layout(margin=dict(l=20,r=0,t=100,b=0),title=dict(font=dict(size=30)))\n",
        "fig.show(\"png\",width=2000,height=700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFHP4jNVB7w-"
      },
      "outputs": [],
      "source": [
        "pos_encode = pos_encoding(casting=\"concatenated\")\n",
        "pos_encode /= tf.norm(pos_encode,axis=1,keepdims=True)\n",
        "d = pos_encode[1000]\n",
        "res = tf.einsum(\"pd,d -> p\",pos_encode,d)\n",
        "print(pos_encode.shape,d.shape,res.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGUQLm_PB7w-"
      },
      "outputs": [],
      "source": [
        "fig = make_subplots(rows=2,subplot_titles=[\"Total plot\",\"Zoomed in\"])\n",
        "fig.add_trace(go.Scatter(y=res,mode=\"lines\",line=dict(color=\"royalblue\")),row=1,col=1)\n",
        "fig.add_shape(type=\"rect\",x0=950,x1=1052,y0=0,y1=1,line=dict(color=\"white\",dash=\"dash\"),row=1,col=1)\n",
        "fig.add_trace(go.Scatter(y=res,mode=\"lines\",line=dict(color=\"royalblue\")),row=2,col=1)\n",
        "fig.for_each_xaxis(lambda axes: axes.update(range=[950,1052]),row=2,col=1)\n",
        "fig.update_layout(height=1000,showlegend=False)\n",
        "fig.show(\"png\",width=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvL-gkD3B7w_"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self,vocab_size:int,embed_size:int=512,casting:Literal[\"concat\",\"interleave\"]=\"concat\",length:int=2048,**kwargs):\n",
        "        \"\"\"\n",
        "        Positional Encoding layer used in the initial layers of the transformers\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding,self).__init__(**kwargs)\n",
        "        assert embed_size%2==0,f\"EMBED_SIZE needs to be even\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size//2\n",
        "        self.embedding = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "        self.casting = casting\n",
        "        self.length = length\n",
        "\n",
        "    def call(self,encoder_inputs):\n",
        "        # shape_checker = Shapecheck()\n",
        "        seq_length = tf.shape(encoder_inputs)[1]\n",
        "        # shape_checker(encoder_inputs,\"batch encoder_sequence\")\n",
        "        embed_out = self.embedding(encoder_inputs)\n",
        "        # shape_checker(embed_out,\"batch encoder_sequence units\")\n",
        "        embed_out *= tf.sqrt(tf.cast(self.embed_size,tf.float32))\n",
        "        embed_out += self._positional_encoding()[tf.newaxis,:seq_length,:]\n",
        "        # shape_checker(embed_out,\"batch encoder_sequence units\")\n",
        "        return embed_out\n",
        "\n",
        "\n",
        "\n",
        "    def compute_mask(self,*args,**kwargs):\n",
        "        return self.embedding.compute_mask(*args,**kwargs)\n",
        "\n",
        "\n",
        "    def _positional_encoding(self):\n",
        "        positions = tf.range(start=0,limit=self.length,dtype=tf.float32)[:,tf.newaxis]\n",
        "        angles = tf.range(start=0,limit=self.embed_size,dtype=tf.float32)[tf.newaxis,:]\n",
        "        angles /= self.embed_size\n",
        "        angles = positions * (1/10000**angles)\n",
        "        if self.casting == \"interleave\":\n",
        "            embed = np.zeros(shape=[self.length,self.embed_size*2])\n",
        "            embed[:,::2] = tf.sin(angles)\n",
        "            embed[:,1::2] = tf.cos(angles)\n",
        "            return tf.convert_to_tensor(embed)\n",
        "        else:\n",
        "            return tf.concat([tf.sin(angles),tf.cos(angles)],axis=-1)\n",
        "\n",
        "\n",
        "en_embedding = PositionalEncoding(vocab_size=len(en_vocab))\n",
        "es_embedding = PositionalEncoding(vocab_size=len(spa_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StuB3U3SB7w_"
      },
      "outputs": [],
      "source": [
        "en_embed_out = en_embedding(en_in)\n",
        "es_embed_out = es_embedding(es_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBPjPmuLB7xA"
      },
      "outputs": [],
      "source": [
        "en_embed_out._keras_mask # Which is nothing but tf.cast(en_in,tf.bool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKH8VKSKB7xG"
      },
      "source": [
        "## Add and Normalize blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chhcBXLTB7xH"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0THHMBlB7xH"
      },
      "source": [
        "- These Add and Norm blocks are scattered throughout the model. Each one joins a residual connection and run the result through a ```LayerNormalization``` layer.\n",
        "- The easiest way to organize the code is around these residual blocks. The following sections will define custom layer classes for each.\n",
        "- The residual Add and norm layer blocks are included to make the training efficient. They ensure the gradients flow without dying and also ensures that the tensors are updated instead of replaced.\n",
        "- The Implementation use add layer instead of + because the masks are propogated with layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgxaLi9xB7xH"
      },
      "source": [
        "## The Base Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxktL7BgB7xH"
      },
      "source": [
        "- Attention layers are used throught the model. Theses are all identical except for how the attention is configured. Each one contains a    ```layers.MultiHeadAttention```<br>\n",
        "```layers.LayerNormalization```<br>\n",
        "```layers.Add```<br>\n",
        "layers.\n",
        "- To implement these attention layers, start with a simple base class that just contains the component layers. Sach us-case will be implemented as a subclass. its a more code to write this way,but it keeps the intention clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BaoWt80B7xI"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "\n",
        "        super(BaseAttention,self).__init__()\n",
        "        self.base_attention = keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.add = keras.layers.Add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltfeBvOvB7xI"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The base attention layer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrE1FOYB7xI"
      },
      "source": [
        "- This is how base attention looks like\n",
        "1. The query sequence: The sequence which needs to be studied i.e., the sequence that needs to be lookedup\n",
        "2. The context sequence: The sequence that is being used for the lookup\n",
        "\n",
        "- The output has the same shape as the query-sequence\n",
        "- The common comparision is that this operation is like a dictionary lookup . A fuzzy,differentiable,vectorized dictionary lookup\n",
        "- Herers a regular python dictionry , with 3 keys and 3 values being passed a single query.\n",
        "```python\n",
        "\n",
        "d = {'color':'blue','age',:22,'type':'pickup'}\n",
        "result = d['color']\n",
        "\n",
        "```\n",
        "-  - The ```query``` $\\text{\\sect}$ is what you're trying to find.\n",
        "-  - The ```key```   $\\text{\\sect}$ is what sort of information the dictionary has.\n",
        "-  - The ```value``` is that information\n",
        "<br>\n",
        "<br>\n",
        "- When you lookup a ```query``` in a regular dictionary, the dictionary finds the matching ```key```,and return its associated ```value```. The ```query``` either has a matching ```key``` or it doesn't. You can imagine a fuzzy dictionary where the keys don't have to match perfectly. If you looked up d[\"sepcies] in the dictionary aboev, maybe you'd want it to return \"pickup\" since that's the best match for the query.\n",
        "- An attentIon layer does a fuzzy lookpu like this but its not just looking for thue best key . It combines the values based on how well the query matches each key.\n",
        "- How does that work? In an attention layer the query , key and value are each vectors.Instead of doing a hash lookup the attenttion layer combines the query and key vectors to determine how well they match, the \"attention score\".The layer return the average across all the values, weighted by the \"attention scors\"\n",
        "- Each location the query-sequence provied a query vector. The context sequence acts as the dictonary. At each location in the context sequence provides a key and value vector .The input vectors are not used directly , the ```layers.MulitHeadAttention``` layer includes layers.Dense layers to project the input vecotort before using them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPIxfmAB7xI"
      },
      "source": [
        "## The Cross Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbhjTgCoB7xJ"
      },
      "source": [
        "- At the literal center of the transformers is the cross attention layer which connects the encoder to decoder. This layer is the most straight-forward use of attention in the model,it performs the same task as the attention block in the NMT Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Bkj0ZUB7xJ"
      },
      "source": [
        "At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1QJMQrJB7xJ"
      },
      "source": [
        "- To implement this you pass the target sequence x as the query and the context sequence as the key/value when calling the mha layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_1ZMJdcB7xJ"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "\n",
        "    def call(self,encoder_outputs,decoder_outputs):\n",
        "        shape_checker = Shapecheck()\n",
        "        # shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
        "        # shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
        "        attetnion_outputs,self.attention_scores = self.base_attention(query=decoder_outputs,key=encoder_outputs,value=encoder_outputs,return_attention_scores=True)\n",
        "        # shape_checker(attetnion_outputs,\"batch decoder_sequence units\")\n",
        "        # shape_checker(self.attention_scores,\"batch num_heads decoder_sequence encoder_sequence\")\n",
        "        return self.layer_norm(self.add([attetnion_outputs,decoder_outputs]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch1HVw33B7xK"
      },
      "outputs": [],
      "source": [
        "cross_attention = CrossAttention(num_heads=2,key_dim=512)\n",
        "print(cross_attention(en_embed_out,es_embed_out).shape)\n",
        "print(cross_attention.attention_scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCF2xEiyB7xK"
      },
      "source": [
        "## Global Self-Attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlhDiFwyB7xN"
      },
      "source": [
        "- This layer is responsible for processing the context sequence, and propogating information along its length:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58pUDfE7B7xN"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMzQ_Mk3B7xO"
      },
      "source": [
        "- Since the context sequence is fixed while the translation is being genreated, information is allowed to flow in both directions. Before Transformers and self attention, models commonly used RNN or CNN to this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yonfJgp4B7xP"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Bidirectional RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DId4SnDfB7xP"
      },
      "source": [
        "The RNN and CNN have their limitations.\n",
        "- The Rnn allows information to flow all the way across the sequenc,but it passes through many processing steps to get there (limiting gradient flow). These RNN steps have to be run sequentially and so the RNN is less able to take advantage of modern parallel devices.\n",
        "- In the CNN each location can be processed in parallel, but it only provised a imited recptive field. The receptive field only grows linealry with the number fo CNN layers, You need to stack a number of Convolution layers to transmit information across the sequence ( Wavenet reduces this problem by using dilated convolutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-996mUsBB7xP"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(BaseAttention):\n",
        "\n",
        "    def call(self,encoder_outputs):\n",
        "        # shape_check = Shapecheck()\n",
        "        # shape_check(encoder_outputs,\"batch encoder_sequence units\")\n",
        "        attention_output,self.attention_scores = self.base_attention(query=encoder_outputs,key=encoder_outputs,value=encoder_outputs,return_attention_scores=True)\n",
        "        # shape_check(attention_output,\"batch encoder_sequence units\")\n",
        "        # shape_check(self.attention_scores,\"batch num_heads encoder_sequence encoder_sequence\")\n",
        "        return self.layer_norm(self.add([attention_output,encoder_outputs]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0JpMpJmB7xQ"
      },
      "outputs": [],
      "source": [
        "self_attention = SelfAttention(num_heads=2,key_dim=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siTLsJndB7xQ"
      },
      "outputs": [],
      "source": [
        "print(self_attention(en_embed_out).shape)\n",
        "print(self_attention(en_embed_out).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdZZnA1EB7xQ"
      },
      "source": [
        "Sticking with the same style as before you could draw it like this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwKwxQfqB7xR"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGcD33jGB7xR"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv9VA9LXB7xR"
      },
      "source": [
        "## The Causal(Masked) Self Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUEjThlB7xR"
      },
      "source": [
        "- This needs to be handled differently from the encoder's global self attention layer.\n",
        "- Like the text generation tutorial and the NMT with attention tutorial, Transformers are an \"Autoreggressive\" model: They generate the text one token at a time and feed that output back to the input. To make this efficient, these models ensure that the output for each seuquence elment only depends on the previous sequence elments; the models are called ```causal```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guBiSjwLB7xS"
      },
      "source": [
        "- A single-direction RNN is causal by definition. To make a causal convolution you just need to pad the input and shift the output so that it aligns correctly (use ```layers.Conv1D(padding=\"causal\")```)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XneJIC4HB7xS"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Causal RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN-causal.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXmvbTAeB7xS"
      },
      "source": [
        "A causal model is efficient in two ways:\n",
        "\n",
        "1. In training, it lets you compute loss for every location in the output sequence whiele executing the model just once.\n",
        "2. During inference, for each new token generated you only need to calculate its outputs, the outputs for the previous sequence elements can be reused.\n",
        "\n",
        "    - For an RNN you just need the RNN-state to account for previous computations (pass return_state=True to the RNN layer's constructor)\n",
        "    - For a CNN you would need to follow the approach of Fast Wavenet\n",
        "\n",
        "To build a causal self attention layer,you need to ues an approprate mask when compution the attention scorse and summing the attention values<br>\n",
        "This is taken care of automatically if you pass use_causal_mask = True to the MultiHeadAtttenion layer when you call it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYfbatPxB7xS"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "\n",
        "    def call(self,decoder_inputs):\n",
        "        # shape_check = Shapecheck()\n",
        "        # shape_check(decoder_inputs,\"batch decoder_sequence units\")\n",
        "        attention_outputs,self.attention_scores = self.base_attention(query=decoder_inputs,key=decoder_inputs,value=decoder_inputs,return_attention_scores=True,use_causal_mask=True)\n",
        "        # shape_check(attention_outputs,\"batch decoder_sequence units\")\n",
        "        # shape_check(self.attention_scores,\"batch num_heads decoder_sequence decoder_sequence\")\n",
        "        return self.layer_norm(self.add([attention_outputs,decoder_inputs]))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_HtfU34B7xT"
      },
      "outputs": [],
      "source": [
        "masked_self_attention = CausalSelfAttention(num_heads=1,key_dim=512)\n",
        "print(masked_self_attention(es_embed_out).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m3UOVOxB7xU"
      },
      "source": [
        "The causal mask ensures that each location has only access to the locations that come before it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_lWImxFB7xU"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "<table>\n",
        "</tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnNNG7xdB7xU"
      },
      "outputs": [],
      "source": [
        "csa = CausalSelfAttention(num_heads=1,key_dim=512)\n",
        "print(csa(self_attention(en_embed_out)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GViiFk0DB7xV"
      },
      "source": [
        "## FeedForward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0OSm6FAB7xV"
      },
      "source": [
        "The transformed also includes this point-wise feed-forward network in both the encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_J2DPeeB7xW"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0MWsnKeB7xX"
      },
      "source": [
        "This network consists of two linear layers ```keras.layers.Dense``` with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalizaiton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l67vZP4AB7xX"
      },
      "outputs": [],
      "source": [
        "class FeedForward(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,length:int,d_model:int,dropout_rate=0.1,**kwargs):\n",
        "\n",
        "        super(FeedForward,self).__init__()\n",
        "        self.ff = keras.Sequential([\n",
        "            keras.layers.Dense(length,\"relu\"),\n",
        "            keras.layers.Dense(d_model),\n",
        "            keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = keras.layers.Add()\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self,attention_outputs):\n",
        "\n",
        "        # shape_check = Shapecheck()\n",
        "        # shape_check(attention_outputs,\"batch encoder_sequence units\")\n",
        "        ff_out = self.ff(attention_outputs)\n",
        "        # shape_check(ff_out,\"batch encoder_sequence d_model\")\n",
        "        return self.layer_norm(self.add([attention_outputs,ff_out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Z-Gk1TB7xX"
      },
      "outputs": [],
      "source": [
        "feed_forward = FeedForward(length=2048,d_model=512)\n",
        "print(feed_forward(self_attention(en_embed_out)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4TY_uMEB7xY"
      },
      "source": [
        "# Encoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMTOk5AsB7xY"
      },
      "source": [
        "The encoder contains a stack of N encoder layers. Where each ```EncoderLayer``` contains ```GlobalSelfAttention``` and ```FeedForward``` laye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylvxb5xdB7xZ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KirMg_PB7xZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,vocab_size:int,length=2048,d_model=512,casting:Literal[\"concat\",\"interleave\"]=\"concat\",self_num_heads=3,embed_dropout_rate=0.1,feed_dropout_rate=0.1,N=4,**kwargs):\n",
        "\n",
        "        super(Encoder,self).__init__(**kwargs)\n",
        "        self.num_layers = N\n",
        "        self.pos_encode = PositionalEncoding(vocab_size,d_model,casting=casting,length=length)\n",
        "        self.drop_out = keras.layers.Dropout(embed_dropout_rate)\n",
        "        self.self_attention = SelfAttention(num_heads=self_num_heads,key_dim=d_model)\n",
        "        self.feed_forward = FeedForward(length,d_model,feed_dropout_rate)\n",
        "\n",
        "\n",
        "    def call(self,encoder_inputs):\n",
        "\n",
        "        # shape_check = Shapecheck()\n",
        "        # shape_check(encoder_inputs,\"batch encoder_sequence\")\n",
        "        z = self.drop_out(self.pos_encode(encoder_inputs))\n",
        "        # shape_check(z,\"batch encoder_sequence units\")\n",
        "        for _ in range(self.num_layers):\n",
        "            z = self.feed_forward(self.self_attention(z))\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6FCcIm_B7xa"
      },
      "outputs": [],
      "source": [
        "en_encoder = Encoder(vocab_size=len(en_vocab))\n",
        "print(en_encoder(en_in).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bHQwQyXB7xa"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,vocab_size:int,length=2048,d_model=512,casting:Literal[\"concat\",\"interleave\"]=\"concat\",N=12,self_num_heads=3,cross_num_heads=3,feed_dropout_rate=0.1,embed_dropout_rate=0.1,**kwargs):\n",
        "\n",
        "        super(Decoder,self).__init__(**kwargs)\n",
        "\n",
        "        self.num_layers = N\n",
        "        self.pos_encode = PositionalEncoding(vocab_size,d_model)\n",
        "        self.dropout = keras.layers.Dropout(embed_dropout_rate)\n",
        "        self.self_attention = SelfAttention(num_heads=self_num_heads,key_dim=d_model)\n",
        "        self.cross_attention = CrossAttention(num_heads=cross_num_heads,key_dim=d_model)\n",
        "        self.feed_forward = FeedForward(length,d_model,feed_dropout_rate)\n",
        "\n",
        "\n",
        "    def call(self,decoder_inputs,encoder_outputs):\n",
        "\n",
        "        shape_check = Shapecheck()\n",
        "        shape_check(decoder_inputs,\"batch decoder_sequence\")\n",
        "        z = self.dropout(self.pos_encode(decoder_inputs))\n",
        "        shape_check(z,\"batch decoder_sequnce units\")\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            z = self.self_attention(z)\n",
        "            z = self.cross_attention(z,encoder_outputs)\n",
        "            z = self.feed_forward(z)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6fG_PtmB7xa"
      },
      "outputs": [],
      "source": [
        "es_decoder = Decoder(vocab_size=len(spa_vocab))\n",
        "print(es_decoder(es_in,en_encoder(en_in)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcWDyIqGB7xb"
      },
      "outputs": [],
      "source": [
        "class Translator(keras.Model):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            encoder_vocab_size:int,\n",
        "            decoder_vocab_size:int,\n",
        "            encoder_units:int,\n",
        "            decoder_units:int,\n",
        "            encoder_length=2048,\n",
        "            decoder_length=2048,\n",
        "            encoder_embed_depth=512,\n",
        "            decoder_embed_depth=512,\n",
        "            encoder_pos_dropout_rate=0.1,\n",
        "            decoder_pos_dropout_rate=0.1,\n",
        "            encoder_self_attention_num_heads=3,\n",
        "            decoder_self_attention_num_heads=3,\n",
        "            decoder_cross_attention_num_heads=3,\n",
        "            encoder_feed_forward_dropout_rate=0.1,\n",
        "            decoder_feed_forward_dropout_rate=0.1,\n",
        "            encoder_feed_forward_units=2048,\n",
        "            decoder_feed_forward_units=2048,\n",
        "            encoder_casting:Literal[\"concat\",\"interleave\"]=\"concat\",\n",
        "            decoder_casting:Literal[\"concat\",\"interleave\"]=\"concat\",\n",
        "            **kwargs\n",
        "    ):\n",
        "\n",
        "        super(Translator,self).__init__(**kwargs)\n",
        "        \"\"\"\n",
        "        Variables Instantiations\n",
        "        \"\"\"\n",
        "        self.encoder_vocab_size = encoder_vocab_size\n",
        "        self.decoder_vocab_size = decoder_vocab_size\n",
        "        self.encoder_length = encoder_length\n",
        "        self.decoder_length = decoder_length\n",
        "        self.encoder_units = encoder_units\n",
        "        self.decoder_units = decoder_units\n",
        "        self.encoder_embed_depth = encoder_embed_depth//2\n",
        "        self.decoder_embed_depth = decoder_embed_depth//2\n",
        "        self.en_casting = encoder_casting\n",
        "        self.de_casting = decoder_casting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Layers Instantiations\n",
        "        \"\"\"\n",
        "        self.encoder_embedder = keras.layers.Embedding(encoder_vocab_size,encoder_embed_depth)\n",
        "        self.decoder_embedder = keras.layers.Embedding(decoder_vocab_size,decoder_embed_depth)\n",
        "\n",
        "    def _pos_encoding_for_encoder(self):\n",
        "        positions = tf.range(start=0,limit=self.encoder_length,dtype=tf.float32)[:,tf.newaxis]\n",
        "        angles = tf.range(start=0,limit=self.encoder_embed_depth,dtype=tf.float32)[tf.newaxis,:]/self.encoder_embed_depth\n",
        "        angles = (1/10000**(angles))\n",
        "        angle_rads = positions * angles\n",
        "        if self.en_casting == \"concat\":\n",
        "            return tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
        "        else:\n",
        "            embed = np.zeros(shape=[self.encoder_length,self.encoder_embed_depth])\n",
        "            embed[:,::2] = tf.sin(angle_rads)\n",
        "            embed[:,1::2] = tf.cos(angle_rads)\n",
        "            return embed\n",
        "\n",
        "    def _pos_encoding_for_decoder(self):\n",
        "        pass\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJt9AJQwB7xc"
      },
      "outputs": [],
      "source": [
        "positions = tf.range(start=0,limit=2048,dtype=tf.float32)[:,tf.newaxis]\n",
        "angles = tf.range(start=0,limit=256,dtype=tf.float32)[tf.newaxis,:]/256\n",
        "angles = (1/10000**(angles))\n",
        "angle_rads = positions * angles\n",
        "embed = np.zeros(shape=[2048,512])\n",
        "embed[:,::2] = tf.sin(angle_rads)\n",
        "embed[:,1::2] = tf.cos(angle_rads)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}