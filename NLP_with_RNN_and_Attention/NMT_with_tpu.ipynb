{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUaRKJW+slpN1gF+NYWU0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/NMT_with_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xHJa_8IeCwTX"
      },
      "outputs": [],
      "source": [
        "import os,warnings\n",
        "from IPython.display import clear_output\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip3 install -q -U \"tensorflow-text==2.13.0\"\n",
        "!pip3 install -q -U einops\n",
        "!pip3 install plotly\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as tf_text\n",
        "np.printoptions(precision=2)\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "import einops\n",
        "from zipfile import ZipFile\n",
        "from typing import Any\n",
        "%xmode Minimal\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)\n",
        "strategy = tf.distribute.OneDeviceStrategy()"
      ],
      "metadata": {
        "id": "DdP5qZJVtU2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapeCheck():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.shapes = {}\n",
        "\n",
        "    def __call__(self,tensor,names,**kwargs):\n",
        "\n",
        "        if not tf.executing_eagerly():\n",
        "            return\n",
        "\n",
        "        for name,dim in einops.parse_shape(tensor,names).items():\n",
        "\n",
        "            if name not in self.shapes:\n",
        "                self.shapes[name] = dim\n",
        "\n",
        "            elif self.shapes[name] == dim:\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Dimension mismatch for tensor {tensor}\\nfound dimention :{self.shapes[name]}\\nnew dimension given :{dim}\")\n"
      ],
      "metadata": {
        "id": "wXQqhK5Q_gAg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
      ],
      "metadata": {
        "id": "y-stDhklf87v"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=origin,extract=True)\n",
        "with ZipFile(file_path,\"r\") as f:\n",
        "    f.extractall(\"spa-eng\")\n",
        "with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "    text = f.read()\n",
        "en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "for en,es in zip(en_text[:10],es_text[:10]):\n",
        "    print(en,es)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id5fVBZMgo8Q",
        "outputId": "2e6c262e-d10a-4d04-9539-be723cbf05e2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go. Ve.\n",
            "Go. Vete.\n",
            "Go. Vaya.\n",
            "Go. Váyase.\n",
            "Hi. Hola.\n",
            "Run! ¡Corre!\n",
            "Run. Corred.\n",
            "Who? ¿Quién?\n",
            "Fire! ¡Fuego!\n",
            "Fire! ¡Incendio!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(sentence:str):\n",
        "    sentence = tf_text.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z.,!?¿]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[.,!?¿]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    sentence = tf.strings.join([\"[START]\",sentence,\"[END]\"],separator=\" \")\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "L-X_f9el6esj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for en,es in zip(text_preprocess(en_text[:10]).numpy(),text_preprocess(es_text[:10]).numpy()):\n",
        "    print(f\"{en}   ---->{es}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAIPy26E9umu",
        "outputId": "c01afa5e-4351-4bbe-b5f8-9e7a570103df"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'[START] go . [END]'   ---->b'[START] ve . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vete . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vaya . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vayase . [END]'\n",
            "b'[START] hi . [END]'   ---->b'[START] hola . [END]'\n",
            "b'[START] run ! [END]'   ---->b'[START] corre ! [END]'\n",
            "b'[START] run . [END]'   ---->b'[START] corred . [END]'\n",
            "b'[START] who ? [END]'   ---->b'[START] \\xc2\\xbf quien ? [END]'\n",
            "b'[START] fire ! [END]'   ---->b'[START] fuego ! [END]'\n",
            "b'[START] fire ! [END]'   ---->b'[START] incendio ! [END]'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "en_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size,standardize=text_preprocess,ragged=True)\n",
        "es_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size,standardize=text_preprocess,ragged=True)\n",
        "en_vec_layer.adapt(en_text)\n",
        "es_vec_layer.adapt(es_text)"
      ],
      "metadata": {
        "id": "bARcJi8D_PHT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(en_inputs,es_inputs):\n",
        "    en_inputs = en_vec_layer(en_inputs).to_tensor()\n",
        "    es_inputs = es_vec_layer(es_inputs).to_tensor()\n",
        "    return (en_inputs,es_inputs[:,:-1]),es_inputs[:,1:]"
      ],
      "metadata": {
        "id": "Av4bEWHDJO0i"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "all_indices = np.random.uniform(size=len(en_text))\n",
        "train_indices = all_indices < 0.8\n",
        "test_indices = all_indices > 0.8\n",
        "en_text = np.array(en_text)\n",
        "es_text = np.array(es_text)\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[train_indices],es_text[train_indices]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(64)\n",
        "    .map(preprocess)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "valid_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[test_indices],es_text[test_indices]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(64)\n",
        "    .map(preprocess)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "metadata": {
        "id": "ZnJ8xsuyLJnB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (en_in,es_in),tar_in in train_ds.take(1):\n",
        "    print(en_in.shape,es_in.shape,tar_in.shape)\n",
        "    print(en_in[:2],es_in[:2],tar_in[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXZKnKSEWCNl",
        "outputId": "aa026591-e5a1-4764-b572-7a1f1fea9a0f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 21) (64, 22) (64, 22)\n",
            "tf.Tensor(\n",
            "[[   2    5  536    1  108   35 4821    4    3    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0]\n",
            " [   2   29 2297   22   11    3    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0]], shape=(2, 21), dtype=int64) tf.Tensor(\n",
            "[[  2  11 817   6 526 177  81   1   4   3   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  2  13   5 862  58  12   3   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]], shape=(2, 22), dtype=int64) tf.Tensor(\n",
            "[[ 11 817   6 526 177  81   1   4   3   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [ 13   5 862  58  12   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]], shape=(2, 22), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units: int=256,vec_layer: keras.layers.TextVectorization=en_vec_layer,**kwargs):\n",
        "\n",
        "        super(Encoder,self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.vec_layer = vec_layer\n",
        "        self.vocab_size = vec_layer.vocabulary_size()\n",
        "\n",
        "        self.embedder = keras.layers.Embedding(self.vocab_size,units,mask_zero=True)\n",
        "        self.encoder_unit = keras.layers.Bidirectional(keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\"),merge_mode=\"sum\")\n",
        "\n",
        "    def call(self,encoder_inputs):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(encoder_inputs,\"batch encoder_sequence\")\n",
        "        encoder_embedded_outputs = self.embedder(encoder_inputs)\n",
        "        shape_checker(encoder_embedded_outputs,\"batch encoder_sequence units\")\n",
        "        encoder_outputs,*encoder_state = self.encoder_unit(encoder_embedded_outputs)\n",
        "        shape_checker(encoder_state[0],\"batch units\")\n",
        "        shape_checker(encoder_state[1],\"batch units\")\n",
        "\n",
        "        return encoder_outputs,encoder_state"
      ],
      "metadata": {
        "id": "ryWbLFERWQ-k"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()"
      ],
      "metadata": {
        "id": "c6MqIEFcrcgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (en_in,es_in),tar_in in train_ds:\n",
        "    encoder(en_in)"
      ],
      "metadata": {
        "id": "56tnW5f_fsct"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (en_in,es_in),tar_in in valid_ds:\n",
        "    encoder(en_in)"
      ],
      "metadata": {
        "id": "5HpDgodzhNG9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units:int=256,vec_layer:keras.layers.TextVectorization=es_vec_layer,**kwargs):\n",
        "\n",
        "        super(Decoder,self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.vec_layer = vec_layer\n",
        "        self.vocab_size = vec_layer.vocabulary_size()\n",
        "\n",
        "        self.embedder = keras.layers.Embedding(self.vocab_size,units,mask_zero=True)\n",
        "        self.decoder_unit = keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\")\n",
        "\n",
        "\n",
        "    def call(self,decoder_inputs,decoder_initial_state=None):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(decoder_inputs,\"batch decoder_sequence\")\n",
        "        decoder_embedded_outputs = self.embedder(decoder_inputs)\n",
        "        shape_checker(decoder_embedded_outputs,\"batch decoder_sequence units\")\n",
        "        decoder_outputs,*decoder_state = self.decoder_unit(decoder_embedded_outputs,initial_state=decoder_initial_state)\n",
        "        shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
        "        shape_checker(decoder_state[0],\"batch units\")\n",
        "        shape_checker(decoder_state[1],\"batch units\")\n",
        "\n",
        "        return decoder_outputs,decoder_state"
      ],
      "metadata": {
        "id": "Iub11qtwhQv0"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder()\n",
        "for (en_in,es_in),tar_in in train_ds:\n",
        "    decoder(es_in)"
      ],
      "metadata": {
        "id": "i-c1HwT6qMn7"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (en_in,es_in),tar_in in valid_ds:\n",
        "    decoder(es_in)"
      ],
      "metadata": {
        "id": "xxyzO-m7sBIL"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units=256,**kwargs):\n",
        "\n",
        "        super(CrossAttention,self).__init__(**kwargs)\n",
        "\n",
        "        self.mha = keras.layers.MultiHeadAttention(num_heads=1,key_dim=units)\n",
        "        self.add = keras.layers.Add()\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self,encoder_outputs,decoder_outputs):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
        "        shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
        "\n",
        "        attention_outputs,attention_scores = self.mha(query=decoder_outputs,value=encoder_outputs,return_attention_scores=True)\n",
        "        shape_checker(attention_outputs,\"batch decoder_sequence units\")\n",
        "        shape_checker(attention_scores,\"batch num_heads decoder_sequence encoder_sequence\")\n",
        "        self.attention_scores = tf.reduce_mean(attention_scores,axis=1)\n",
        "        normalized_attention_outputs = self.layer_norm(self.add([attention_outputs,decoder_outputs]))\n",
        "        return normalized_attention_outputs"
      ],
      "metadata": {
        "id": "HrsGUVD8xkfq"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention()\n",
        "\n",
        "for (en_in,es_in),tar_in in train_ds:\n",
        "    attention_layer(encoder(en_in),decoder(es_in))\n",
        "\n",
        "for (en_in,es_in),tar_in in valid_ds:\n",
        "    attention_layer(encoder(en_in),decoder(es_in))"
      ],
      "metadata": {
        "id": "qKWAe8DW5XOD"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(keras.Model):\n",
        "\n",
        "    def __init__(self,units=256,**kwargs):\n",
        "\n",
        "        super(Translator,self).__init__(**kwargs)\n",
        "\n",
        "        self.encoder_layer = Encoder(units=units)\n",
        "        self.decoder_layer = Decoder(units=units)\n",
        "        self.attention_layer = CrossAttention(units=units)\n",
        "\n",
        "        self.words_to_ids = keras.layers.StringLookup(\n",
        "            vocabulary=self.decoder_layer.vec_layer.get_vocabulary(),\n",
        "            oov_token=\"[UNK]\",\n",
        "            mask_token=\"\"\n",
        "        )\n",
        "        self.ids_to_words = keras.layers.StringLookup(\n",
        "            vocabulary=self.decoder_layer.vec_layer.get_vocabulary(),\n",
        "            oov_token=\"[UNK]\",\n",
        "            mask_token=\"\",\n",
        "            invert=True\n",
        "        )\n",
        "        self.start_token = self.words_to_ids([\"[START]\"])\n",
        "        self.end_token = self.words_to_ids([\"[END]\"])\n",
        "\n",
        "        self.out = keras.layers.Dense(self.decoder_layer.vec_layer.vocabulary_size())\n",
        "\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        encoder_inputs,decoder_inputs = inputs\n",
        "        encoder_outputs,self.encoder_state = self.encoder_layer(encoder_inputs)\n",
        "        decoder_outputs,self.decoder_state = self.decoder_layer(decoder_inputs)\n",
        "        attention_outputs = self.attention_layer(decoder_outputs,encoder_outputs)\n",
        "        total_outputs = self.out(attention_outputs)\n",
        "\n",
        "        try:\n",
        "            del total_outputs._keras_mask\n",
        "\n",
        "        except AttributeError as err:\n",
        "\n",
        "            pass\n",
        "\n",
        "        return total_outputs\n",
        "\n",
        "\n",
        "    def text_to_encoder_outputs(self,texts):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        en_vec_outputs = self.encoder_layer.vec_layer(texts).to_tensor()\n",
        "        return self.encoder(en_vec_outputs)\n",
        "\n",
        "    def get_decoder_initial_state(self,encoder_outputs):\n",
        "        batch_size = tf.shape(encoder_outputs)[0]\n",
        "        start_tokens = tf.fill(dims=[batch_size,1],value=self.start_token)\n",
        "        done = tf.zeros(shape=[batch_size,1],dtype=tf.bool)\n",
        "        embedding = self.decoder.embedder(start_tokens)\n",
        "        return start_tokens,done,self.decoder_layer.decoder_unit.get_initial_state(embedding)\n",
        "\n",
        "    def get_next_token(self,encoder_inputs,next_token,done,state,temperature=0.0):\n",
        "        total_out,state = self(encoder_inputs,next_token)\n",
        "\n",
        "        if temperature:\n",
        "            scaled_total_out = total_out/temperature\n",
        "            next_token = tf.random.categorical(scaled_total_out,num_samples=1)\n",
        "        else:\n",
        "            next_token = tf.argmax(total_out,axis=-1)\n",
        "\n",
        "        done = done | (next_token == self.end_token)\n",
        "        next_token = tf.where(done,tf.constant(0,dtype=tf.int64),next_token)\n",
        "        return next_token,done,state\n",
        "\n",
        "    def tokens_to_text(self,tokens):\n",
        "        texts = self.ids_to_words(tokens)\n",
        "        texts = tf.strings.reduce_join(texts,separator=\" \")\n",
        "        texts = tf.strings.regex_replace(texts,r\"^ *\\[START\\]* \",\"\")\n",
        "        texts = tf.strings.regex_replace(texts,r\" *\\[END]\\ *$\",\"\")\n",
        "        texts = tf.strings.strip(texts)\n",
        "        return texts\n"
      ],
      "metadata": {
        "id": "bmlCyOD6rlIe"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Translator()"
      ],
      "metadata": {
        "id": "UxEGjrOol5ll"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for en_in,es_in in train_ds.map(lambda x,y:x):\n",
        "    model((en_in,es_in))"
      ],
      "metadata": {
        "id": "mcDoU6Xrrbxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for en_in,es_in in valid_ds.map(lambda x,y:x):\n",
        "    model((en_in,es_in))"
      ],
      "metadata": {
        "id": "caHX8uDZs9tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaIKWlbltIEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}