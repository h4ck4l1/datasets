{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/NMT_with_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xHJa_8IeCwTX"
      },
      "outputs": [],
      "source": [
        "import os,warnings,sys\n",
        "from IPython.display import clear_output\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "if \"google.colab\" in sys.modules:\n",
        "    !pip3 install -q -U \"tensorflow-text==2.13.0\"\n",
        "    !pip3 install -q -U einops\n",
        "    !pip3 install plotly\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as tf_text\n",
        "np.printoptions(precision=2)\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "import einops\n",
        "from zipfile import ZipFile\n",
        "from typing import Any\n",
        "%xmode Minimal\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdP5qZJVtU2j"
      },
      "outputs": [],
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)\n",
        "strategy = tf.distribute.OneDeviceStrategy(device=\"/device:GPU:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wXQqhK5Q_gAg"
      },
      "outputs": [],
      "source": [
        "class ShapeCheck():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.shapes = {}\n",
        "\n",
        "    def __call__(self,tensor,names,**kwargs):\n",
        "\n",
        "        if not tf.executing_eagerly():\n",
        "            return\n",
        "\n",
        "        for name,dim in einops.parse_shape(tensor,names).items():\n",
        "\n",
        "            if name not in self.shapes:\n",
        "                self.shapes[name] = dim\n",
        "\n",
        "            elif self.shapes[name] == dim:\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Dimension mismatch for tensor {tensor}\\nfound dimention :{self.shapes[name]}\\nnew dimension given :{dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y-stDhklf87v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go. ---> Ve.\n",
            "Go. ---> Vete.\n",
            "Go. ---> Vaya.\n",
            "Go. ---> Váyase.\n",
            "Hi. ---> Hola.\n",
            "Run! ---> ¡Corre!\n",
            "Run. ---> Corred.\n",
            "Who? ---> ¿Quién?\n",
            "Fire! ---> ¡Fuego!\n",
            "Fire! ---> ¡Incendio!\n"
          ]
        }
      ],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=origin,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "    en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "    for en,es in zip(en_text[:10],es_text[:10]):\n",
        "        print(en,\"--->\",es)\n",
        "else:\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "    en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "    for en,es in zip(en_text[:10],es_text[:10]):\n",
        "        print(en,\"--->\",es)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L-X_f9el6esj"
      },
      "outputs": [],
      "source": [
        "def text_preprocess(sentence:str):\n",
        "    sentence = tf_text.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z.,!?¿]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[.,!?¿]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    sentence = tf.strings.join([\"[START]\",sentence,\"[END]\"],separator=\" \")\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAIPy26E9umu",
        "outputId": "c01afa5e-4351-4bbe-b5f8-9e7a570103df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'[START] go . [END]'   ---->b'[START] ve . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vete . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vaya . [END]'\n",
            "b'[START] go . [END]'   ---->b'[START] vayase . [END]'\n",
            "b'[START] hi . [END]'   ---->b'[START] hola . [END]'\n",
            "b'[START] run ! [END]'   ---->b'[START] corre ! [END]'\n",
            "b'[START] run . [END]'   ---->b'[START] corred . [END]'\n",
            "b'[START] who ? [END]'   ---->b'[START] \\xc2\\xbf quien ? [END]'\n",
            "b'[START] fire ! [END]'   ---->b'[START] fuego ! [END]'\n",
            "b'[START] fire ! [END]'   ---->b'[START] incendio ! [END]'\n"
          ]
        }
      ],
      "source": [
        "for en,es in zip(text_preprocess(en_text[:10]).numpy(),text_preprocess(es_text[:10]).numpy()):\n",
        "    print(f\"{en}   ---->{es}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bARcJi8D_PHT"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5000\n",
        "en_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size,standardize=text_preprocess,ragged=True)\n",
        "es_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size,standardize=text_preprocess,ragged=True)\n",
        "en_vec_layer.adapt(en_text)\n",
        "es_vec_layer.adapt(es_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Av4bEWHDJO0i"
      },
      "outputs": [],
      "source": [
        "def preprocess(en_inputs,es_inputs):\n",
        "    en_inputs = en_vec_layer(en_inputs).to_tensor()\n",
        "    es_inputs = es_vec_layer(es_inputs).to_tensor()\n",
        "    return (en_inputs,es_inputs[:,:-1]),es_inputs[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZnJ8xsuyLJnB"
      },
      "outputs": [],
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "all_indices = np.random.uniform(size=len(en_text))\n",
        "train_indices = all_indices < 0.8\n",
        "test_indices = all_indices > 0.8\n",
        "en_text = np.array(en_text)\n",
        "es_text = np.array(es_text)\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[train_indices],es_text[train_indices]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(64)\n",
        "    .map(preprocess)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "valid_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[test_indices],es_text[test_indices]))\n",
        "    .shuffle(len(en_text))\n",
        "    .batch(64)\n",
        "    .map(preprocess)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXZKnKSEWCNl",
        "outputId": "aa026591-e5a1-4764-b572-7a1f1fea9a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 23) (64, 26) (64, 26)\n",
            "tf.Tensor(\n",
            "[[   2  217  759   28 2116    4    3    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0]\n",
            " [   2    6   27   43  766   15   30  880    4    3    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0]], shape=(2, 23), dtype=int64) tf.Tensor(\n",
            "[[   2   76 2743   64    1    4    3    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   2    9  369    8  887    6   87  868    4    3    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(2, 26), dtype=int64) tf.Tensor(\n",
            "[[  76 2743   64    1    4    3    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   9  369    8  887    6   87  868    4    3    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(2, 26), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for (en_in,es_in),tar_in in train_ds.take(1):\n",
        "    print(en_in.shape,es_in.shape,tar_in.shape)\n",
        "    print(en_in[:2],es_in[:2],tar_in[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ryWbLFERWQ-k"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units: int=256,vec_layer: keras.layers.TextVectorization=en_vec_layer,**kwargs):\n",
        "\n",
        "        super(Encoder,self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.vec_layer = vec_layer\n",
        "        self.vocab_size = vec_layer.vocabulary_size()\n",
        "\n",
        "        self.embedder = keras.layers.Embedding(self.vocab_size,units,mask_zero=True)\n",
        "        self.encoder_unit = keras.layers.Bidirectional(keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\"),merge_mode=\"sum\")\n",
        "\n",
        "    def call(self,encoder_inputs):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(encoder_inputs,\"batch encoder_sequence\")\n",
        "        encoder_embedded_outputs = self.embedder(encoder_inputs)\n",
        "        shape_checker(encoder_embedded_outputs,\"batch encoder_sequence units\")\n",
        "        encoder_outputs,*encoder_state = self.encoder_unit(encoder_embedded_outputs)\n",
        "        shape_checker(encoder_state[0],\"batch units\")\n",
        "        shape_checker(encoder_state[1],\"batch units\")\n",
        "\n",
        "        return encoder_outputs,encoder_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c6MqIEFcrcgO"
      },
      "outputs": [],
      "source": [
        "# '''Testing Encoder on the whole set'''\n",
        "# encoder = Encoder()\n",
        "# for en_in in train_ds.map(lambda x,y:x[0]):\n",
        "#     encoder(en_in)\n",
        "# for en_in in valid_ds.map(lambda x,y:x[0]):\n",
        "#     encoder(en_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Iub11qtwhQv0"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units:int=256,vec_layer:keras.layers.TextVectorization=es_vec_layer,**kwargs):\n",
        "\n",
        "        super(Decoder,self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.vec_layer = vec_layer\n",
        "        self.vocab_size = vec_layer.vocabulary_size()\n",
        "\n",
        "        self.embedder = keras.layers.Embedding(self.vocab_size,units,mask_zero=True)\n",
        "        self.decoder_unit = keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\")\n",
        "\n",
        "\n",
        "    def call(self,decoder_inputs,decoder_initial_state=None):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(decoder_inputs,\"batch decoder_sequence\")\n",
        "        decoder_embedded_outputs = self.embedder(decoder_inputs)\n",
        "        shape_checker(decoder_embedded_outputs,\"batch decoder_sequence units\")\n",
        "        decoder_outputs,*decoder_state = self.decoder_unit(decoder_embedded_outputs,initial_state=decoder_initial_state)\n",
        "        shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
        "        shape_checker(decoder_state[0],\"batch units\")\n",
        "        shape_checker(decoder_state[1],\"batch units\")\n",
        "\n",
        "        return decoder_outputs,decoder_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i-c1HwT6qMn7"
      },
      "outputs": [],
      "source": [
        "# '''Testing Decoder on whole set'''\n",
        "# decoder = Decoder()\n",
        "# for es_in in train_ds.map(lambda x,y:x[1]):\n",
        "#     decoder(es_in)\n",
        "# for es_in in valid_ds.map(lambda x,y:x[1]):\n",
        "#     decoder(es_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HrsGUVD8xkfq"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,units=256,**kwargs):\n",
        "\n",
        "        super(CrossAttention,self).__init__(**kwargs)\n",
        "\n",
        "        self.mha = keras.layers.MultiHeadAttention(num_heads=1,key_dim=units)\n",
        "        self.add = keras.layers.Add()\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self,encoder_outputs,decoder_outputs):\n",
        "\n",
        "        shape_checker = ShapeCheck()\n",
        "        shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
        "        shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
        "\n",
        "        attention_outputs,attention_scores = self.mha(query=decoder_outputs,value=encoder_outputs,return_attention_scores=True)\n",
        "        shape_checker(attention_outputs,\"batch decoder_sequence units\")\n",
        "        shape_checker(attention_scores,\"batch num_heads decoder_sequence encoder_sequence\")\n",
        "        self.attention_scores = tf.reduce_mean(attention_scores,axis=1)\n",
        "        normalized_attention_outputs = self.layer_norm(self.add([attention_outputs,decoder_outputs]))\n",
        "        return normalized_attention_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qKWAe8DW5XOD"
      },
      "outputs": [],
      "source": [
        "# '''Testing Attention on all the outputs'''\n",
        "# attention_layer = CrossAttention()\n",
        "\n",
        "# for en_in,es_in in train_ds.map(lambda x,y:x):\n",
        "#     attention_layer(encoder(en_in)[0],decoder(es_in)[0])\n",
        "\n",
        "# for en_in,es_in in valid_ds.map(lambda x,y:x):\n",
        "#     attention_layer(encoder(en_in)[0],decoder(es_in)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bmlCyOD6rlIe"
      },
      "outputs": [],
      "source": [
        "class Translator(keras.Model):\n",
        "\n",
        "    def __init__(self,units=256,**kwargs):\n",
        "\n",
        "        super(Translator,self).__init__(**kwargs)\n",
        "\n",
        "        self.encoder_layer = Encoder(units=units)\n",
        "        self.decoder_layer = Decoder(units=units)\n",
        "        self.attention_layer = CrossAttention(units=units)\n",
        "\n",
        "        self.words_to_ids = keras.layers.StringLookup(\n",
        "            vocabulary=self.decoder_layer.vec_layer.get_vocabulary(),\n",
        "            oov_token=\"[UNK]\",\n",
        "            mask_token=\"\"\n",
        "        )\n",
        "        self.ids_to_words = keras.layers.StringLookup(\n",
        "            vocabulary=self.decoder_layer.vec_layer.get_vocabulary(),\n",
        "            oov_token=\"[UNK]\",\n",
        "            mask_token=\"\",\n",
        "            invert=True\n",
        "        )\n",
        "        self.start_token = self.words_to_ids([\"[START]\"])\n",
        "        self.end_token = self.words_to_ids([\"[END]\"])\n",
        "\n",
        "        self.out = keras.layers.Dense(self.decoder_layer.vec_layer.vocabulary_size())\n",
        "\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        encoder_inputs,decoder_inputs = inputs\n",
        "        encoder_outputs,self.encoder_state = self.encoder_layer(encoder_inputs)\n",
        "        decoder_outputs,self.decoder_state = self.decoder_layer(decoder_inputs)\n",
        "        attention_outputs = self.attention_layer(decoder_outputs,encoder_outputs)\n",
        "        total_outputs = self.out(attention_outputs)\n",
        "\n",
        "        try:\n",
        "            del total_outputs._keras_mask\n",
        "\n",
        "        except AttributeError as err:\n",
        "\n",
        "            pass\n",
        "\n",
        "        return total_outputs\n",
        "\n",
        "\n",
        "    def text_to_encoder_outputs(self,texts):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        en_vec_outputs = self.encoder_layer.vec_layer(texts).to_tensor()\n",
        "        return self.encoder_layer(en_vec_outputs)\n",
        "\n",
        "    def get_decoder_initial_state(self,encoder_outputs):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        start_tokens = tf.fill(dims=[batch_size,1],value=self.start_token)\n",
        "        done = tf.zeros(shape=[batch_size,1],dtype=tf.bool)\n",
        "        embedding = self.decoder_layer.embedder(start_tokens)\n",
        "        return start_tokens,done,self.decoder_layer.decoder_unit.get_initial_state(embedding)\n",
        "\n",
        "    def get_next_token(self,encoder_inputs,next_token,done,state,temperature=0.0):\n",
        "        total_out,state = self(encoder_inputs,next_token)\n",
        "\n",
        "        if temperature:\n",
        "            scaled_total_out = total_out[:,-1,:]/temperature\n",
        "            next_token = tf.random.categorical(scaled_total_out,num_samples=1)\n",
        "        else:\n",
        "            next_token = tf.argmax(total_out,axis=-1)\n",
        "\n",
        "        done = done | (next_token == self.end_token)\n",
        "        next_token = tf.where(done,tf.constant(0,dtype=tf.int64),next_token)\n",
        "        return next_token,done,state\n",
        "\n",
        "    def tokens_to_text(self,tokens):\n",
        "        texts = self.ids_to_words(tokens)\n",
        "        texts = tf.strings.reduce_join(texts,separator=\" \")\n",
        "        texts = tf.strings.regex_replace(texts,r\"^ *\\[START\\]* \",\"\")\n",
        "        texts = tf.strings.regex_replace(texts,r\" *\\[END]\\ *$\",\"\")\n",
        "        texts = tf.strings.strip(texts)\n",
        "        return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UxEGjrOol5ll"
      },
      "outputs": [],
      "source": [
        "model = Translator()\n",
        "for en_in,es_in in train_ds.map(lambda x,y:x):\n",
        "    model((en_in,es_in))\n",
        "for en_in,es_in in valid_ds.map(lambda x,y:x):\n",
        "    model((en_in,es_in))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaIKWlbltIEA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMUaRKJW+slpN1gF+NYWU0k",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
