{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPveunNIUnlaNCmFGLttumL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/Bahdanau_Attention_with_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PrGW3cUv9LE8"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import os,warnings\n",
        "os.environ[\"TF_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from zipfile import ZipFile\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
      ],
      "metadata": {
        "id": "NPZQxC8N9lqy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "KY3B4jgn9pJI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/job:localhost\"):\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "    text = text.replace(\"¡\",\"\").replace(\"¿\",\"\")\n",
        "    text = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "    en_text,es_text = zip(*text)\n",
        "    total_size = len(en_text)"
      ],
      "metadata": {
        "id": "IiPgrk9G98ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de3c7a5-3911-4469-e7ce-8d4ffa00f87c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layers(vocab_size=1000,seq_length=50):\n",
        "    en_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    es_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    en_vec_layer.adapt(en_text)\n",
        "    es_vec_layer.adapt([f\"soseq {s} eoseq\" for s in es_text])\n",
        "    return en_vec_layer,es_vec_layer"
      ],
      "metadata": {
        "id": "O1b-AKTAEEnq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(es_vec_layer,train_size=100_000):\n",
        "\n",
        "    # if full:\n",
        "    train_size = slice(None,100_000)\n",
        "    valid_size = slice(100_000,None)\n",
        "    # else:\n",
        "    #     train_size = slice(None,1000)\n",
        "    #     valid_size = slice(1000,1500)\n",
        "\n",
        "    tsize = train_size.stop\n",
        "    vsize = (valid_size.stop - valid_size.start) if valid_size.stop else (len(en_text) - tsize)\n",
        "    X_train = en_vec_layer(tf.constant(en_text[train_size]))\n",
        "    X_valid = en_vec_layer(tf.constant(en_text[valid_size]))\n",
        "    X_dec_train = es_vec_layer(tf.constant([f\"soseq {s}\" for s in es_text[train_size]]))\n",
        "    X_dec_valid = es_vec_layer(tf.constant([f\"soseq {s}\" for s in es_text[valid_size]]))\n",
        "    y_train = es_vec_layer(tf.constant([f\"{s} eoseq\" for s in es_text[train_size]]))\n",
        "    y_valid = es_vec_layer(tf.constant([f\"{s} eoseq\" for s in es_text[valid_size]]))\n",
        "\n",
        "    return (X_train,X_dec_train),y_train,(X_valid,X_dec_valid),y_valid,tsize,vsize"
      ],
      "metadata": {
        "id": "qZZuPYs2EqYo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# def get_model(vocab_size=1000,embed_size=128):\n",
        "\n",
        "#     encoder_inputs = keras.layers.Input(shape=(),dtype=tf.string)\n",
        "#     decoder_inputs = keras.layers.Input(shape=(),dtype=tf.string)\n",
        "#     en_vec_out = en_vec_layer(encoder_inputs)\n",
        "#     es_vec_out = es_vec_layer(decoder_inputs)\n",
        "#     en_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "#     es_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "#     en_embed_out = en_embed(en_vec_out)\n",
        "#     es_embed_out = es_embed(es_vec_out)\n",
        "#     encoder = keras.layers.Bidirectional(keras.layers.LSTM(256,return_state=True,return_sequences=True))\n",
        "#     decoder = keras.layers.LSTM(512,return_sequences=True)\n",
        "#     encoder_out,*encoder_state = encoder(en_embed_out)\n",
        "#     initial_state = [tf.concat(encoder_state[::2],axis=-1),tf.concat(encoder_state[1::2],axis=-1)]\n",
        "#     decoder_out = decoder(es_embed_out,initial_state=initial_state)\n",
        "#     attention = keras.layers.Attention()\n",
        "#     attention_out = attention([decoder_out,encoder_out])\n",
        "#     out_layer = keras.layers.Dense(vocab_size,\"softmax\")\n",
        "#     out = out_layer(attention_out)\n",
        "#     return keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=[out])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8IflJZ3XEsPq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModel(keras.Model):\n",
        "\n",
        "    def __init__(self,vocab_size=1000,embed_size=128,**kwargs):\n",
        "\n",
        "        super(AttentionModel,self).__init__(**kwargs)\n",
        "        self.en_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "        self.es_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "        self.encoder = keras.layers.Bidirectional(keras.layers.LSTM(256,return_state=True,return_sequences=True))\n",
        "        self.decoder = keras.layers.LSTM(512,return_sequences=True)\n",
        "        self.attention = keras.layers.Attention()\n",
        "        self.out = keras.layers.Dense(vocab_size,\"softmax\")\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        encoder_inputs = inputs[0]\n",
        "        decoder_inputs = inputs[1]\n",
        "        en_embed_out = self.en_embed(encoder_inputs)\n",
        "        es_embed_out = self.es_embed(decoder_inputs)\n",
        "        encoder_out,*encoder_state = self.encoder(en_embed_out)\n",
        "        encoder_state = [tf.concat(encoder_state[::2],axis=-1),tf.concat(encoder_state[1::2],axis=-1)]\n",
        "        decoder_out = self.decoder(es_embed_out,initial_state=encoder_state)\n",
        "        attention_out = self.attention([decoder_out,encoder_out])\n",
        "        return self.out(attention_out)"
      ],
      "metadata": {
        "id": "l55fqgSgFG_a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "\n",
        "    en_vec_layer,es_vec_layer = get_layers()\n",
        "    epochs = 20\n",
        "    X_train,y_train,X_valid,y_valid,train_size,valid_size = get_data(es_vec_layer)\n",
        "    model = AttentionModel()\n",
        "    BATCH_SIZE = 50*8\n",
        "    steps_per_epoch = train_size//BATCH_SIZE\n",
        "    validation_steps = valid_size//BATCH_SIZE\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"nadam\",\n",
        "        metrics=[\"accuracy\"],\n",
        "        steps_per_execution=25\n",
        "    )"
      ],
      "metadata": {
        "id": "STZuV9NlEwxh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_valid,y_valid),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ],
      "metadata": {
        "id": "u21J16vzEy79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec194be-e365-4c04-94ad-1375ab4bc576"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "250/250 [==============================] - 41s 163ms/step - loss: 4.4972 - accuracy: 0.2249 - val_loss: 4.2670 - val_accuracy: 0.2757\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 3.3085 - accuracy: 0.3959 - val_loss: 3.3398 - val_accuracy: 0.3772\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 2.4976 - accuracy: 0.4986 - val_loss: 2.7221 - val_accuracy: 0.4492\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.9880 - accuracy: 0.5647 - val_loss: 2.3995 - val_accuracy: 0.4907\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.7108 - accuracy: 0.6090 - val_loss: 2.2318 - val_accuracy: 0.5134\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.5416 - accuracy: 0.6427 - val_loss: 2.1148 - val_accuracy: 0.5334\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.4262 - accuracy: 0.6663 - val_loss: 2.0285 - val_accuracy: 0.5495\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.3390 - accuracy: 0.6838 - val_loss: 2.0237 - val_accuracy: 0.5518\n",
            "Epoch 9/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.2685 - accuracy: 0.6987 - val_loss: 1.9830 - val_accuracy: 0.5598\n",
            "Epoch 10/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.2083 - accuracy: 0.7113 - val_loss: 1.9716 - val_accuracy: 0.5654\n",
            "Epoch 11/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.1556 - accuracy: 0.7224 - val_loss: 1.9783 - val_accuracy: 0.5649\n",
            "Epoch 12/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.1076 - accuracy: 0.7321 - val_loss: 1.9633 - val_accuracy: 0.5720\n",
            "Epoch 13/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.0632 - accuracy: 0.7411 - val_loss: 1.9807 - val_accuracy: 0.5727\n",
            "Epoch 14/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.0248 - accuracy: 0.7490 - val_loss: 1.9892 - val_accuracy: 0.5731\n",
            "Epoch 15/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.9867 - accuracy: 0.7566 - val_loss: 2.0151 - val_accuracy: 0.5730\n",
            "Epoch 16/20\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.9527 - accuracy: 0.7640 - val_loss: 2.0376 - val_accuracy: 0.5748\n",
            "Epoch 17/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.9204 - accuracy: 0.7709 - val_loss: 2.0788 - val_accuracy: 0.5706\n",
            "Epoch 18/20\n",
            "250/250 [==============================] - 6s 22ms/step - loss: 0.8902 - accuracy: 0.7770 - val_loss: 2.1025 - val_accuracy: 0.5701\n",
            "Epoch 19/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.8616 - accuracy: 0.7832 - val_loss: 2.1240 - val_accuracy: 0.5704\n",
            "Epoch 20/20\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.8337 - accuracy: 0.7895 - val_loss: 2.1613 - val_accuracy: 0.5697\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ad4f0230b50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence:str,beam_width=3):\n",
        "\n",
        "    translation_list = []\n",
        "    X_inp = en_vec_layer(tf.constant([sentence]))\n",
        "    X_dec_inp = es_vec_layer(tf.constant([\"soseq\"]))\n",
        "    outputs = model.predict((X_inp,X_dec_inp),verbose=0)[0,0]\n",
        "    top_k_values,top_k_indices = tf.math.top_k(outputs,beam_width)\n",
        "    for i in range(beam_width):\n",
        "        translation = es_vec_layer.get_vocabulary()[top_k_indices[i]]\n",
        "        pred_proba = tf.math.log(top_k_values[i])\n",
        "        for word_id in range(1,50):\n",
        "            X_dec_inp = es_vec_layer(tf.constant([\"soseq \"+translation]))\n",
        "            outputs = model.predict((X_inp,X_dec_inp),verbose=0)[0,word_id]\n",
        "            top_1_val,top_1_ind = tf.math.top_k(outputs,1)\n",
        "            pred_word = es_vec_layer.get_vocabulary()[top_1_ind[0]]\n",
        "            if pred_word == \"eoseq\":\n",
        "                break\n",
        "            pred_proba += tf.math.log(top_1_val[0])\n",
        "            translation += \" \" + pred_word\n",
        "        translation_list.append((pred_proba,translation))\n",
        "\n",
        "    return translation_list"
      ],
      "metadata": {
        "id": "8ozkWOWTgPr2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I love Soccer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBe58o9pwv_I",
        "outputId": "0195ca5f-0fa6-4b3b-b46e-f60af3fb84c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(), dtype=float32, numpy=-1.926172>,\n",
              "  'me encanta el fútbol'),\n",
              " (<tf.Tensor: shape=(), dtype=float32, numpy=-3.100997>, 'yo amo el fútbol'),\n",
              " (<tf.Tensor: shape=(), dtype=float32, numpy=-7.6268578>,\n",
              "  'le encanta me el fútbol')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}