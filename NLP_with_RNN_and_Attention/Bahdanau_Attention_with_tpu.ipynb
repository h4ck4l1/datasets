{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM2izanFMW/dsDsW/f1Qtev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/Bahdanau_Attention_with_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PrGW3cUv9LE8"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import os,warnings\n",
        "os.environ[\"TF_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from zipfile import ZipFile\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
      ],
      "metadata": {
        "id": "NPZQxC8N9lqy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "KY3B4jgn9pJI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/job:localhost\"):\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "    text = text.replace(\"¡\",\"\").replace(\"¿\",\"\")\n",
        "    text = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "    en_text,es_text = zip(*text)\n",
        "    total_size = len(en_text)"
      ],
      "metadata": {
        "id": "IiPgrk9G98ih"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layers(vocab_size=1000,seq_length=50):\n",
        "    en_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    es_vec_layer = keras.layers.TextVectorization(vocab_size,output_sequence_length=50)\n",
        "    en_vec_layer.adapt(en_text)\n",
        "    es_vec_layer.adapt([f\"soseq {s} eoseq\" for s in es_text])\n",
        "    return en_vec_layer,es_vec_layer"
      ],
      "metadata": {
        "id": "O1b-AKTAEEnq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(es_vec_layer,train_size=100_000,full=True):\n",
        "\n",
        "    if full:\n",
        "        train_size = slice(None,100_000)\n",
        "        valid_size = slice(100_000,None)\n",
        "    else:\n",
        "        train_size = slice(None,1000)\n",
        "        valid_size = slice(1000,1500)\n",
        "\n",
        "    tsize = train_size.stop\n",
        "    vsize = (valid_size.stop - valid_size.start) if valid_size.stop else (len(en_text) - tsize)\n",
        "    X_train = en_vec_layer(tf.constant(en_text[train_size]))\n",
        "    X_valid = en_vec_layer(tf.constant(en_text[valid_size]))\n",
        "    X_dec_train = es_vec_layer(tf.constant([f\"soseq {s}\" for s in es_text[train_size]]))\n",
        "    X_dec_valid = es_vec_layer(tf.constant([f\"soseq {s}\" for s in es_text[valid_size]]))\n",
        "    y_train = es_vec_layer(tf.constant([f\"{s} eoseq\" for s in es_text[train_size]]))\n",
        "    y_valid = es_vec_layer(tf.constant([f\"{s} eoseq\" for s in es_text[valid_size]]))\n",
        "\n",
        "    return (X_train,X_dec_train),y_train,(X_valid,X_dec_valid),y_valid,tsize,vsize"
      ],
      "metadata": {
        "id": "qZZuPYs2EqYo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# def get_model(vocab_size=1000,embed_size=128):\n",
        "\n",
        "#     encoder_inputs = keras.layers.Input(shape=(),dtype=tf.string)\n",
        "#     decoder_inputs = keras.layers.Input(shape=(),dtype=tf.string)\n",
        "#     en_vec_out = en_vec_layer(encoder_inputs)\n",
        "#     es_vec_out = es_vec_layer(decoder_inputs)\n",
        "#     en_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "#     es_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "#     en_embed_out = en_embed(en_vec_out)\n",
        "#     es_embed_out = es_embed(es_vec_out)\n",
        "#     encoder = keras.layers.Bidirectional(keras.layers.LSTM(256,return_state=True,return_sequences=True))\n",
        "#     decoder = keras.layers.LSTM(512,return_sequences=True)\n",
        "#     encoder_out,*encoder_state = encoder(en_embed_out)\n",
        "#     initial_state = [tf.concat(encoder_state[::2],axis=-1),tf.concat(encoder_state[1::2],axis=-1)]\n",
        "#     decoder_out = decoder(es_embed_out,initial_state=initial_state)\n",
        "#     attention = keras.layers.Attention()\n",
        "#     attention_out = attention([decoder_out,encoder_out])\n",
        "#     out_layer = keras.layers.Dense(vocab_size,\"softmax\")\n",
        "#     out = out_layer(attention_out)\n",
        "#     return keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=[out])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8IflJZ3XEsPq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModel(keras.Model):\n",
        "\n",
        "    def __init__(self,vocab_size=1000,embed_size=128,**kwargs):\n",
        "\n",
        "        super(AttentionModel,self).__init__(**kwargs)\n",
        "        self.en_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "        self.es_embed = keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "        self.encoder = keras.layers.Bidirectional(keras.layers.LSTM(256,return_state=True,return_sequences=True))\n",
        "        self.decoder = keras.layers.LSTM(512,return_sequences=True)\n",
        "        self.attention = keras.layers.Attention()\n",
        "        self.out = keras.layers.Dense(vocab_size,\"softmax\")\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        encoder_inputs = inputs[0]\n",
        "        decoder_inputs = inputs[1]\n",
        "        en_embed_out = self.en_embed(encoder_inputs)\n",
        "        es_embed_out = self.es_embed(decoder_inputs)\n",
        "        encoder_out,*encoder_state = self.encoder(en_embed_out)\n",
        "        encoder_state = [tf.concat(encoder_state[::2],axis=-1),tf.concat(encoder_state[1::2],axis=-1)]\n",
        "        decoder_out = self.decoder(es_embed_out,initial_state=encoder_state)\n",
        "        attention_out = self.attention([decoder_out,encoder_out])\n",
        "        return self.out(attention_out)"
      ],
      "metadata": {
        "id": "l55fqgSgFG_a"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "\n",
        "    en_vec_layer,es_vec_layer = get_layers()\n",
        "    full = True\n",
        "    epochs = 10\n",
        "    X_train,y_train,X_valid,y_valid,train_size,valid_size = get_data(es_vec_layer,full=full)\n",
        "    model = AttentionModel()\n",
        "    BATCH_SIZE = 50*8\n",
        "    steps_per_epoch = train_size//BATCH_SIZE\n",
        "    validation_steps = valid_size//BATCH_SIZE\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"nadam\",\n",
        "        metrics=[\"accuracy\"],\n",
        "        steps_per_execution=25\n",
        "    )"
      ],
      "metadata": {
        "id": "STZuV9NlEwxh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_valid,y_valid),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ],
      "metadata": {
        "id": "u21J16vzEy79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1be6668-db0b-4470-c711-106be594cead"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 39s 157ms/step - loss: 4.6461 - accuracy: 0.1907 - val_loss: 4.4325 - val_accuracy: 0.2094\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 3.2534 - accuracy: 0.3894 - val_loss: 3.1936 - val_accuracy: 0.3779\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 2.1869 - accuracy: 0.5430 - val_loss: 2.5397 - val_accuracy: 0.4691\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.7471 - accuracy: 0.6156 - val_loss: 2.2743 - val_accuracy: 0.5092\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.5293 - accuracy: 0.6518 - val_loss: 2.1154 - val_accuracy: 0.5329\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.3960 - accuracy: 0.6757 - val_loss: 2.0471 - val_accuracy: 0.5453\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.3004 - accuracy: 0.6937 - val_loss: 1.9923 - val_accuracy: 0.5559\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.2256 - accuracy: 0.7079 - val_loss: 1.9581 - val_accuracy: 0.5608\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.1631 - accuracy: 0.7198 - val_loss: 1.9726 - val_accuracy: 0.5629\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 1.1083 - accuracy: 0.7308 - val_loss: 1.9682 - val_accuracy: 0.5659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781d102ff2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ozkWOWTgPr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}