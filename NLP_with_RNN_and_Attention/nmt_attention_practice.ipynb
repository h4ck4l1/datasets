{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     !pip3 install -q -U \"tensorflow-text==2.13.0\"\n",
    "#     !pip3 install -q -U einops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import einops\n",
    "import tensorflow_text as tftext\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ShapeCheck():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self,tensor,names,**kwargs) -> Any:\n",
    "        parsed_tensor = einops.parse_shape(tensor,names)\n",
    "        for name,new_dim in parsed_tensor.items():\n",
    "            \n",
    "            if name not in self.shapes:\n",
    "                self.shapes[name] = new_dim\n",
    "            else:\n",
    "                if new_dim == self.shapes[name]:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise AttributeError(f\"\\n\\033[1mDIMENSION MISMATCH FOR {tensor.shape}\\033[0m\\n\\033[1mFOUND DIMENSION\\033[0m ----->{new_dim}\\n\\033[1mINCOMPATIBLE WITH OLD DIMENSION\\033[0m ---->{self.shapes[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch': 64, 't': 17, 'units': 256, 's': 16}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_checker = ShapeCheck()\n",
    "shape_checker(tf.random.normal(shape=[64,17,256]),\"batch t units\")\n",
    "shape_checker(tf.random.normal(shape=[64,16,256]),\"batch s units\")\n",
    "shape_checker(tf.random.normal(shape=[64,17,16]),\"batch t s\")\n",
    "# shape_checker(tf.random.normal(shape=[64,17,256]),\"batch t s\")\n",
    "shape_checker.shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go. ----> Ve.\n",
      "Go. ----> Vete.\n",
      "Go. ----> Vaya.\n",
      "Go. ----> Váyase.\n",
      "Hi. ----> Hola.\n",
      "Run! ----> ¡Corre!\n",
      "Run. ----> Corred.\n",
      "Who? ----> ¿Quién?\n",
      "Fire! ----> ¡Fuego!\n",
      "Fire! ----> ¡Incendio!\n"
     ]
    }
   ],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=origin,extract=True)\n",
    "    with ZipFile(file_path,\"r\") as f:\n",
    "        f.extractall(\"spa-eng\")\n",
    "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
    "for en_in,es_in in zip(en_text[:10],es_text[:10]):\n",
    "    print(en_in,\"---->\",es_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = np.array(en_text)\n",
    "es_text = np.array(es_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.uniform(0,1,len(en_text))\n",
    "train_indices = indices < 0.8\n",
    "test_indices = indices > 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((en_text[train_indices],es_text[train_indices]))\n",
    "    .shuffle(len(en_text))\n",
    "    .batch(64)\n",
    ")\n",
    "valid_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((en_text[test_indices],es_text[test_indices]))\n",
    "    .shuffle(len(en_text))\n",
    "    .batch(64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(sentence):\n",
    "    sentence = tftext.normalize_utf8(sentence,\"NFKD\")\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    sentence = tf.strings.regex_replace(sentence,\"[^ a-z¿?!.,]\",\"\")\n",
    "    sentence = tf.strings.regex_replace(sentence,\"[.?!,¿]\",r\" \\0 \")\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.strings.join([\"[startofsequence]\",sentence,\"[endofsequence]\"],separator=\" \")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "en_vec_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    standardize=text_preprocessing,\n",
    "    ragged=True\n",
    ")\n",
    "es_vec_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    standardize=text_preprocessing,\n",
    "    ragged=True\n",
    ")\n",
    "\n",
    "en_vec_layer.adapt(en_text)\n",
    "es_vec_layer.adapt(es_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(en,es):\n",
    "    en = en_vec_layer(en)\n",
    "    es = es_vec_layer(es)\n",
    "    return (en.to_tensor(),es[:,:-1].to_tensor()),es[:,1:].to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2   27  165   22    1 1071  189  953  540    4    3    0    0    0\n",
      "     0    0]\n",
      " [   2    6   70  204   44 1077    4    3    0    0    0    0    0    0\n",
      "     0    0]], shape=(2, 16), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  2   9 830   5  40 105   6   1   1  14 510   1   4   0   0   0   0   0\n",
      "    0]\n",
      " [  2   9  22  55 342  21 503 328   4   0   0   0   0   0   0   0   0   0\n",
      "    0]], shape=(2, 19), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  9 830   5  40 105   6   1   1  14 510   1   4   3   0   0   0   0   0\n",
      "    0]\n",
      " [  9  22  55 342  21 503 328   4   3   0   0   0   0   0   0   0   0   0\n",
      "    0]], shape=(2, 19), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for i,y in train_raw.map(preprocess).take(1):\n",
    "    print(i[0][:2])\n",
    "    print(i[1][:2])\n",
    "    print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_raw.map(preprocess)\n",
    "valid_ds = valid_raw.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 14)\n",
      "(64, 14)\n"
     ]
    }
   ],
   "source": [
    "for (en_inputs,es_inputs),es_targets in train_ds.take(1):\n",
    "    print(en_inputs.shape)\n",
    "    print(es_inputs.shape)\n",
    "    print(es_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,units=256,vec_layer=en_vec_layer,**kwargs):\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "        self.vec_layer = vec_layer\n",
    "        self.vocab_size = vec_layer.vocabulary_size()\n",
    "        self.Embedding = keras.layers.Embedding(self.vocab_size,output_dim=units,mask_zero=True)\n",
    "        self.encoder_unit = keras.layers.Bidirectional(keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\"),merge_mode=\"sum\")\n",
    "\n",
    "    def call(self,inputs,return_state=False):\n",
    "        shape_checker = ShapeCheck()\n",
    "        shape_checker(inputs,\"batch encoder_sequence\")\n",
    "\n",
    "        embedder_outputs = self.Embedding(inputs)\n",
    "        shape_checker(embedder_outputs,\"batch encoder_sequence units\")\n",
    "\n",
    "        encoder_outputs,*encoder_state = self.encoder_unit(embedder_outputs)\n",
    "        shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
    "\n",
    "        if return_state:\n",
    "            return encoder_outputs,encoder_state\n",
    "        else:\n",
    "            return encoder_outputs\n",
    "        \n",
    "\n",
    "    def text_to_encoder_outputs(self,texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = texts[tf.newaxis]\n",
    "        texts = self.vec_layer(texts).to_tensor()\n",
    "        return self(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing encoder is working on the whole set'''\n",
    "\n",
    "encoder = Encoder()\n",
    "\n",
    "for (en_inputs,es_inputs),es_targets in train_ds:\n",
    "    encoder(en_inputs)\n",
    "\n",
    "for (en_inputs,es_inputs),es_targets in valid_ds:\n",
    "    encoder(es_inputs)\n",
    "\n",
    "_ = encoder.text_to_encoder_outputs(en_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,units=256,**kwargs):\n",
    "\n",
    "        super(CrossAttention,self).__init__(**kwargs)\n",
    "        self.attention = keras.layers.MultiHeadAttention(num_heads=1,key_dim=units)\n",
    "        self.add = keras.layers.Add()\n",
    "        self.layer_norm = keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "    def call(self,encoder_outputs,decoder_outputs):\n",
    "\n",
    "        shape_checker = ShapeCheck()\n",
    "        shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
    "        shape_checker(decoder_outputs,\"batch decoder_sequence units\")\n",
    "        attention_outputs,attention_scores = self.attention(query=decoder_outputs,value=encoder_outputs,return_attention_scores=True)\n",
    "        shape_checker(attention_outputs,\"batch decoder_sequence units\")\n",
    "        self.attention_scores = tf.reduce_mean(attention_scores,axis=1)\n",
    "        shape_checker(self.attention_scores,\"batch decoder_sequence encoder_sequence\")\n",
    "        return self.layer_norm(self.add([decoder_outputs,attention_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 15, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Testing Attention layer'''\n",
    "attention_layer = CrossAttention()\n",
    "\n",
    "out = attention_layer(tf.random.normal(shape=[64,17,256]),tf.random.normal(shape=[64,15,256]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = keras.layers.StringLookup(vocabulary=es_vec_layer.get_vocabulary(),mask_token=\"\",oov_token=\"[UNK]\")\n",
    "start_token = lookup([\"[startofsequence]\"])\n",
    "start_tokens = tf.fill([64,1],start_token)\n",
    "start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = keras.layers.Embedding(es_vec_layer.vocabulary_size(),256,mask_zero=True)\n",
    "initial_state = embedding(start_tokens)\n",
    "initial_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,units=256,vec_layer=es_vec_layer,**kwargs):\n",
    "\n",
    "        super(Decoder,self).__init__(**kwargs)\n",
    "        '''preprocessing layers'''\n",
    "        self.vec_layer = vec_layer\n",
    "        self.words_to_ids = keras.layers.StringLookup(\n",
    "            vocabulary=vec_layer.get_vocabulary(),\n",
    "            oov_token=\"[UNK]\",\n",
    "            mask_token=\"\"\n",
    "        )\n",
    "        self.ids_to_words = keras.layers.StringLookup(\n",
    "            vocabulary=vec_layer.get_vocabulary(),\n",
    "            oov_token=\"[UNK]\",\n",
    "            mask_token=\"\",\n",
    "            invert=True\n",
    "        )\n",
    "        self.start_token = self.words_to_ids([\"[startofsequence]\"])\n",
    "        self.end_token = self.words_to_ids([\"[endofsequence]\"])\n",
    "\n",
    "        '''decoder layers'''\n",
    "        self.embedding = keras.layers.Embedding(vec_layer.vocabulary_size(),units,mask_zero=True)\n",
    "        self.decoder_unit = keras.layers.LSTM(units,return_state=True,return_sequences=True,recurrent_initializer=\"glorot_uniform\")\n",
    "        self.attention = CrossAttention()\n",
    "        self.out = keras.layers.Dense(vec_layer.vocabulary_size())\n",
    "\n",
    "    def call(self,encoder_outputs,decoder_inputs,decoder_state=None,return_state=False):\n",
    "\n",
    "        shape_checker = ShapeCheck()\n",
    "        shape_checker(encoder_outputs,\"batch encoder_sequence units\")\n",
    "        shape_checker(decoder_inputs,\"batch decoder_sequence\")\n",
    "\n",
    "        embedding_outputs = self.embedding(decoder_inputs)\n",
    "        shape_checker(embedding_outputs,\"batch decoder_sequence units\")\n",
    "\n",
    "        decoder_ouputs,*decoder_state = self.decoder_unit(embedding_outputs,initial_state=decoder_state)\n",
    "        shape_checker(decoder_ouputs,\"batch decoder_sequence units\")\n",
    "        shape_checker(decoder_state[0],\"batch units\")\n",
    "        shape_checker(decoder_state[1],\"batch units\")\n",
    "\n",
    "        attention_output = self.attention(encoder_outputs,decoder_ouputs)\n",
    "        total_out = self.out(attention_output)        \n",
    "\n",
    "        if return_state:\n",
    "            return total_out,decoder_state #decoder_state in shape (64,seq_len,256)\n",
    "        else:\n",
    "            return total_out #total_out in shape (64,seq_len,5000)\n",
    "        \n",
    "\n",
    "    def get_initial_state(self,encoder_inputs):\n",
    "        batch_size = tf.shape(encoder_inputs)[0] # 64\n",
    "        done = tf.zeros(shape=[batch_size,1],dtype=tf.bool) #done in shape (64,1)\n",
    "        start_tokens = tf.fill(shape=[batch_size,1],value=self.start_token) #start_tokens in shape (64,1)\n",
    "        return start_tokens,done,self.decoder_unit.get_initial_state(self.embedding(start_tokens)) #state in shape (64,1,256)\n",
    "    \n",
    "    def get_next_token(self,encoder_inputs,prev_token,done,decoder_state,temperature=0):\n",
    "\n",
    "        total_out,decoder_state = self(encoder_inputs,prev_token,decoder_state,return_state=True)\n",
    "        #total_out in shape (64,1,5000)\n",
    "\n",
    "        if temperature:\n",
    "            next_token = np.argmax(total_out,axis=-1)\n",
    "        else:\n",
    "            scaled_out = total_out[:,-1,:]/temperature\n",
    "            next_token = tf.random.categorical(scaled_out,num_samples=1,dtype=tf.int64)\n",
    "        #scaled_out in shape (64,1)\n",
    "        \n",
    "        done = done | (next_token == self.end_token)\n",
    "        next_token = tf.where(done,tf.zeros(shape=tf.shape(next_token),dtype=tf.int64),next_token)\n",
    "\n",
    "        return next_token,decoder_state\n",
    "    \n",
    "    def tokens_to_text(self,tokens):\n",
    "        texts = self.ids_to_words(tokens)\n",
    "        texts = tf.strings.reduce_join(texts,separator=\" \",axis=-1)\n",
    "        texts = tf.strings.regex_replace(texts,r\"^ *\\[startofsequence\\]* \",\"\")\n",
    "        texts = tf.strings.regex_replace(texts,r\" *\\[endofsequence\\] *$\",\"\")\n",
    "        texts = tf.strings.strip(texts)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (en_in,es_in),tar_in in train_ds:\n",
    "    decoder(encoder(en_in),es_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (en_in,es_in),tar_in in valid_ds:\n",
    "    decoder(encoder(en_in),es_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(keras.Model):\n",
    "\n",
    "    def __init__(self,units=256,**kwargs):\n",
    "\n",
    "        super(Translator,self).__init__(**kwargs)\n",
    "        self.encoder = Encoder(units=units)\n",
    "        self.decoder = Decoder(units=units)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "\n",
    "        encoder_inputs,decoder_inputs = inputs\n",
    "        encoder_outputs = self.encoder(encoder_inputs)\n",
    "        total_out = self.decoder(encoder_outputs,decoder_inputs)\n",
    "\n",
    "        try:\n",
    "            del total_out._keras_mask\n",
    "        except AttributeError as error:\n",
    "            pass\n",
    "\n",
    "        return total_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (en_in,es_in),tar_in in train_ds:\n",
    "    model((en_in,es_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (en_in,es_in),tar_in in valid_ds:\n",
    "    model((en_in,es_in))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
