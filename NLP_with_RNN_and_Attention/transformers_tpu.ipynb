{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "import os,sys,warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install -q \"tensorflow-text==2.13.0\"\n",
    "    %pip install -q kaleido\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tftext\n",
    "import tensorflow_datasets as tfds\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=9216)]\n",
    "    )\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "# strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:localhost\"):\n",
    "    dataset_name = \"ted_hrlr_translate/pt_to_en\"\n",
    "    total_dataset = tfds.load(name=dataset_name,batch_size=-1,shuffle_files=True)\n",
    "    ds_info = tfds.builder(dataset_name).info\n",
    "    pt_tokenizer = tftext.BertTokenizer(\"pt_en_vocab.txt\",lower_case=True)\n",
    "    en_tokenizer = tftext.BertTokenizer(\"en_pt_vocab.txt\",lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n't test for curiosity .\",\n",
       "       b'and this conscious defiance is why i , as an agnostic , can still have faith .',\n",
       "       b'you can use everything on the table on me .'], dtype=object)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset['train']['en'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       "array([b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .',\n",
       "       b'mas e se estes fatores fossem ativos ?',\n",
       "       b'mas eles n\\xc3\\xa3o tinham a curiosidade de me testar .',\n",
       "       b'e esta rebeldia consciente \\xc3\\xa9 a raz\\xc3\\xa3o pela qual eu , como agn\\xc3\\xb3stica , posso ainda ter f\\xc3\\xa9 .',\n",
       "       b\"`` `` '' podem usar tudo sobre a mesa no meu corpo . ''\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset['train']['pt'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "MAX_TOKENS = 128\n",
    "def get_dataset(total_dataset,split,ds_info,batch_size,preprocess_fn):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(total_dataset[split])\n",
    "    ds_size = ds_info.splits[split].num_examples\n",
    "    if \"train\" in split:\n",
    "        ds = ds.shuffle(ds_size//2)\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size,drop_remainder=True,num_parallel_calls=AUTO)\n",
    "    ds = ds.map(preprocess_fn,AUTO)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds,ds_size\n",
    "\n",
    "def preprocess_fn(pt_text,en_text):\n",
    "    pt_tokens = pt_tokenizer.tokenize(pt_text).merge_dims(-2,-1)\n",
    "    en_tokens = en_tokenizer.tokenize(en_text).merge_dims(-2,-1)\n",
    "    pt_tokens = pt_tokens[:,:MAX_TOKENS+1]\n",
    "    en_tokens = en_tokens[:,:MAX_TOKENS]\n",
    "    return (en_tokens.to_tensor(),pt_tokens[:,:-1].to_tensor()),pt_tokens[:,1:].to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,casting:Literal[\"concat\",\"interleave\"],length:int=2048,d_model:int=512,**kwargs):\n",
    "\n",
    "\n",
    "        super(PositionEmbedding,self).__init__(**kwargs)\n",
    "        assert d_model%2==0,f\"The depth_of_model {d_model} should be even number\"\n",
    "        d_model = d_model//2\n",
    "        positions = np.arange(length)[:,np.newaxis]\n",
    "        angles = np.arange(d_model)[np.newaxis,:]/d_model\n",
    "        angles = 1/(10000**angles)\n",
    "        angle_rads = positions * angles\n",
    "        if casting == \"concat\":\n",
    "            self.embed = tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
    "        else:\n",
    "            self.embed = np.zeros(shape=[length,d_model])\n",
    "            self.embed[:,::2] = tf.sin(angle_rads)\n",
    "            self.embed[:,1::2] = tf.cos(angle_rads)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,inputs):\n",
    "\n",
    "        seq_l = tf.shape(inputs)[1]\n",
    "        return tf.cast(inputs,self.embed.dtype) + self.embed[tf.newaxis,:seq_l,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    context_vocab_size:int,\n",
    "    target_vocab_size:int,\n",
    "    d_model:int=128,\n",
    "    length:int=2048,\n",
    "    enc_heads:int=8,\n",
    "    dec_heads:int=8,\n",
    "    num_encoder_layers:int=4,\n",
    "    num_decoder_layers:int=4,\n",
    "    encoder_feed_forward_units:int=512,\n",
    "    decoder_feed_forward_units:int=512,\n",
    "    encoder_dropout_rate:float=0.1,\n",
    "    decoder_dropout_rate:float=0.1\n",
    "    ):\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[],dtype=tf.int64)\n",
    "    decoder_inputs = keras.layers.Input(shape=[],dtype=tf.int64)\n",
    "    encoder_embedding = keras.layers.Embedding(context_vocab_size,d_model,mask_zero=True)(encoder_inputs)\n",
    "    decoder_embedding = keras.layers.Embedding(target_vocab_size,d_model,mask_zero=True)(decoder_inputs)\n",
    "    z_enc = PositionEmbedding(\"concat\",length,d_model)(encoder_embedding)\n",
    "    z_dec = PositionEmbedding(\"concat\",length,d_model)(decoder_embedding)\n",
    "\n",
    "    for _ in range(num_encoder_layers):\n",
    "\n",
    "        '''Self Attention Part'''\n",
    "        z_copy = z_enc\n",
    "        self_attention = keras.layers.MultiHeadAttention(enc_heads,d_model)\n",
    "        z_enc = self_attention(query=z_enc,key=z_enc,value=z_enc)\n",
    "        z_enc = keras.layers.Add()([z_copy,z_enc])\n",
    "        z_enc = keras.layers.LayerNormalization()(z_enc)\n",
    "\n",
    "        '''Feed Forward Part'''\n",
    "        z_copy = z_enc\n",
    "        z_enc = keras.layers.Dense(encoder_feed_forward_units,\"relu\")(z_enc)\n",
    "        z_enc = keras.layers.Dense(d_model)(z_enc)\n",
    "        z_enc = keras.layers.Dropout(encoder_dropout_rate)(z_enc)\n",
    "        z_enc = keras.layers.Add()([z_enc,z_copy])\n",
    "        z_enc = keras.layers.LayerNormalization()(z_enc)\n",
    "\n",
    "\n",
    "    for _ in range(num_decoder_layers):\n",
    "\n",
    "        '''Masked Self Attention Part'''\n",
    "        z_copy = z_dec\n",
    "        masked_self_attention = keras.layers.MultiHeadAttention(dec_heads,d_model)\n",
    "        z_dec = masked_self_attention(query=z_dec,key=z_dec,value=z_dec,use_causal_mask=True)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "        '''Cross Attention Part'''\n",
    "        z_copy = z_dec\n",
    "        cross_attention = keras.layers.MultiHeadAttention(dec_heads,d_model)\n",
    "        z_dec = cross_attention(query=z_dec,key=z_enc,value=z_enc)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "        '''Feed Forward Part'''\n",
    "        z_copy = z_dec\n",
    "        z_dec = keras.layers.Dense(decoder_feed_forward_units,\"relu\")(z_dec)\n",
    "        z_dec = keras.layers.Dense(d_model)(z_dec)\n",
    "        z_dec = keras.layers.Dropout(decoder_dropout_rate)(z_dec)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "    out = keras.layers.Dense(target_vocab_size)(z_dec)\n",
    "\n",
    "    return keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_pred):\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=keras.losses.Reduction.NONE)\n",
    "    loss = loss_fn(y_true,y_pred)\n",
    "    mask = tf.cast(y_true != 0,loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def custom_accuracy(y_true,y_pred):\n",
    "\n",
    "    y_pred = tf.cast(tf.argmax(y_pred,axis=-1),y_true.dtype)\n",
    "    mask = tf.cast(y_true != 0,tf.int32)\n",
    "    acc = tf.cast(y_true == y_pred,tf.int32)\n",
    "    acc = acc & mask\n",
    "    return tf.reduce_sum(acc)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "class CustomLR(keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self,d_model:int=512,warmup:int=4000,**kwargs):\n",
    "\n",
    "        self.factor = tf.math.rsqrt(tf.cast(d_model,tf.float32))\n",
    "        self.warmup_factor = tf.math.pow(tf.cast(warmup,tf.float32),tf.cast(-1.5,tf.float32))\n",
    "\n",
    "    def __call__(self,step):\n",
    "        step = tf.cast(step,tf.float32)\n",
    "        return self.factor * tf.math.minimum(tf.math.rsqrt(step),step*self.warmup_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: tf__preprocess_fn() missing 1 required positional argument: 'en_text'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m EPOCH \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     train_ds,train_size \u001b[39m=\u001b[39m get_dataset(total_dataset,\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,ds_info,BATCH_SIZE,preprocess_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     valid_ds,valid_size \u001b[39m=\u001b[39m get_dataset(total_dataset,\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m,ds_info,BATCH_SIZE,preprocess_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     train_steps \u001b[39m=\u001b[39m train_size\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mBATCH_SIZE\n",
      "\u001b[1;32m/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mrepeat()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mbatch(batch_size,drop_remainder\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,num_parallel_calls\u001b[39m=\u001b[39mAUTO)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mmap(preprocess_fn,AUTO)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mcache()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_tpu.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mprefetch(AUTO)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2278\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2279\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2280\u001b[0m     map_func,\n\u001b[1;32m   2281\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2282\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2283\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[1;32m     43\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m     44\u001b[0m       deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/map_op.py:148\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m--> 148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m    149\u001b[0m     map_func,\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m    151\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m    152\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    273\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1188\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1190\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1169\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1169\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1173\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 694\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    696\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    308\u001b[0m         args,\n\u001b[1;32m    309\u001b[0m         kwargs,\n\u001b[1;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:238\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    239\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    240\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:168\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    167\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 168\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    169\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    694\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__preprocess_fn() missing 1 required positional argument: 'en_text'\n"
     ]
    }
   ],
   "source": [
    "cont_vocab = 7010\n",
    "targ_vocab = 7765\n",
    "BATCH_SIZE = 16*strategy.num_replicas_in_sync\n",
    "EPOCH = 20\n",
    "\n",
    "with strategy.scope():\n",
    "    train_ds,train_size = get_dataset(total_dataset,\"train\",ds_info,BATCH_SIZE,preprocess_fn)\n",
    "    valid_ds,valid_size = get_dataset(total_dataset,\"validation\",ds_info,BATCH_SIZE,preprocess_fn)\n",
    "    train_steps = train_size//BATCH_SIZE\n",
    "    valid_steps = valid_size//BATCH_SIZE\n",
    "    total_steps = train_steps*BATCH_SIZE\n",
    "    model = get_model(cont_vocab,targ_vocab)\n",
    "    cust_lr = CustomLR(d_model=128,warmup=total_steps//10)\n",
    "    model.compile(\n",
    "        loss=custom_loss,\n",
    "        metrics=[custom_accuracy,custom_loss],\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=cust_lr,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.98,\n",
    "            epsilon=1e-9\n",
    "        ),\n",
    "        steps_per_execution=24,\n",
    "        jit_compile=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
