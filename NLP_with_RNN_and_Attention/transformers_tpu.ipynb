{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "import os,sys,warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install -q \"tensorflow-text==2.13.0\"\n",
    "    %pip install -q kaleido\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tftext\n",
    "import tensorflow_datasets as tfds\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=9216)]\n",
    "    )\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "# strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:localhost\"):\n",
    "    dataset_name = \"ted_hrlr_translate/pt_to_en\"\n",
    "    total_dataset = tfds.load(name=dataset_name,batch_size=-1,shuffle_files=True)\n",
    "    ds_info = tfds.builder(dataset_name).info\n",
    "    pt_tokenizer = tftext.BertTokenizer(\"pt_en_vocab.txt\",lower_case=True)\n",
    "    en_tokenizer = tftext.BertTokenizer(\"en_pt_vocab.txt\",lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "MAX_TOKENS = 128\n",
    "def get_dataset(total_dataset,split,ds_info,batch_size,preprocess_fn):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((total_dataset[split]['pt'],total_dataset[split]['en']))\n",
    "    ds_size = ds_info.splits[split].num_examples\n",
    "    if \"train\" in split:\n",
    "        ds = ds.shuffle(ds_size//2)\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size,drop_remainder=True,num_parallel_calls=AUTO)\n",
    "    ds = ds.map(preprocess_fn,AUTO)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds,ds_size\n",
    "\n",
    "def preprocess_fn(pt_text,en_text):\n",
    "    pt_tokens = pt_tokenizer.tokenize(pt_text).merge_dims(-2,-1)\n",
    "    en_tokens = en_tokenizer.tokenize(en_text).merge_dims(-2,-1)\n",
    "    pt_tokens = pt_tokens[:,:MAX_TOKENS+1]\n",
    "    en_tokens = en_tokens[:,:MAX_TOKENS]\n",
    "    return (en_tokens.to_tensor(),pt_tokens[:,:-1].to_tensor()),pt_tokens[:,1:].to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,casting:Literal[\"concat\",\"interleave\"],length:int=2048,d_model:int=512,**kwargs):\n",
    "\n",
    "\n",
    "        super(PositionEmbedding,self).__init__(**kwargs)\n",
    "        assert d_model%2==0,f\"The depth_of_model {d_model} should be even number\"\n",
    "        d_model = d_model//2\n",
    "        positions = np.arange(length)[:,np.newaxis]\n",
    "        angles = np.arange(d_model)[np.newaxis,:]/d_model\n",
    "        angles = 1/(10000**angles)\n",
    "        angle_rads = positions * angles\n",
    "        if casting == \"concat\":\n",
    "            self.embed = tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
    "        else:\n",
    "            self.embed = np.zeros(shape=[length,d_model])\n",
    "            self.embed[:,::2] = tf.sin(angle_rads)\n",
    "            self.embed[:,1::2] = tf.cos(angle_rads)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,inputs):\n",
    "\n",
    "        seq_l = tf.shape(inputs)[1]\n",
    "        return tf.cast(inputs,self.embed.dtype) + self.embed[tf.newaxis,:seq_l,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    context_vocab_size:int,\n",
    "    target_vocab_size:int,\n",
    "    d_model:int=128,\n",
    "    length:int=2048,\n",
    "    enc_heads:int=3,\n",
    "    dec_heads:int=3,\n",
    "    num_encoder_layers:int=2,\n",
    "    num_decoder_layers:int=2,\n",
    "    encoder_feed_forward_units:int=512,\n",
    "    decoder_feed_forward_units:int=512,\n",
    "    encoder_dropout_rate:float=0.1,\n",
    "    decoder_dropout_rate:float=0.1\n",
    "    ):\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[],dtype=tf.int64)\n",
    "    decoder_inputs = keras.layers.Input(shape=[],dtype=tf.int64)\n",
    "    encoder_embedding = keras.layers.Embedding(context_vocab_size,d_model,mask_zero=True)(encoder_inputs)\n",
    "    decoder_embedding = keras.layers.Embedding(target_vocab_size,d_model,mask_zero=True)(decoder_inputs)\n",
    "    z_enc = PositionEmbedding(\"concat\",length,d_model)(encoder_embedding)\n",
    "    z_dec = PositionEmbedding(\"concat\",length,d_model)(decoder_embedding)\n",
    "\n",
    "    for _ in range(num_encoder_layers):\n",
    "\n",
    "        '''Self Attention Part'''\n",
    "        z_copy = z_enc\n",
    "        self_attention = keras.layers.MultiHeadAttention(enc_heads,d_model)\n",
    "        z_enc = self_attention(query=z_enc,key=z_enc,value=z_enc)\n",
    "        z_enc = keras.layers.Add()([z_copy,z_enc])\n",
    "        z_enc = keras.layers.LayerNormalization()(z_enc)\n",
    "\n",
    "        '''Feed Forward Part'''\n",
    "        z_copy = z_enc\n",
    "        z_enc = keras.layers.Dense(encoder_feed_forward_units,\"relu\")(z_enc)\n",
    "        z_enc = keras.layers.Dense(d_model)(z_enc)\n",
    "        z_enc = keras.layers.Dropout(encoder_dropout_rate)(z_enc)\n",
    "        z_enc = keras.layers.Add()([z_enc,z_copy])\n",
    "        z_enc = keras.layers.LayerNormalization()(z_enc)\n",
    "\n",
    "\n",
    "    for _ in range(num_decoder_layers):\n",
    "\n",
    "        '''Masked Self Attention Part'''\n",
    "        z_copy = z_dec\n",
    "        masked_self_attention = keras.layers.MultiHeadAttention(dec_heads,d_model)\n",
    "        z_dec = masked_self_attention(query=z_dec,key=z_dec,value=z_dec,use_causal_mask=True)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "        '''Cross Attention Part'''\n",
    "        z_copy = z_dec\n",
    "        cross_attention = keras.layers.MultiHeadAttention(dec_heads,d_model)\n",
    "        z_dec = cross_attention(query=z_dec,key=z_enc,value=z_enc)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "        '''Feed Forward Part'''\n",
    "        z_copy = z_dec\n",
    "        z_dec = keras.layers.Dense(decoder_feed_forward_units,\"relu\")(z_dec)\n",
    "        z_dec = keras.layers.Dense(d_model)(z_dec)\n",
    "        z_dec = keras.layers.Dropout(decoder_dropout_rate)(z_dec)\n",
    "        z_dec = keras.layers.Add()([z_copy,z_dec])\n",
    "        z_dec = keras.layers.LayerNormalization()(z_dec)\n",
    "\n",
    "    out = keras.layers.Dense(target_vocab_size)(z_dec)\n",
    "\n",
    "    return keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_loss(y_true,y_pred):\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=keras.losses.Reduction.NONE)\n",
    "    loss = loss_fn(y_true,y_pred)\n",
    "    mask = tf.cast(y_true != 0,loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def custom_accuracy(y_true,y_pred):\n",
    "\n",
    "    y_pred = tf.cast(tf.argmax(y_pred,axis=-1),y_true.dtype)\n",
    "    mask = tf.cast(y_true != 0,tf.int32)\n",
    "    acc = tf.cast(y_true == y_pred,tf.int32)\n",
    "    acc = acc & mask\n",
    "    return tf.reduce_sum(acc)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "class CustomLR(keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self,d_model:int=512,warmup:int=4000,**kwargs):\n",
    "\n",
    "        self.factor = tf.math.rsqrt(tf.cast(d_model,tf.float32))\n",
    "        self.warmup_factor = tf.math.pow(tf.cast(warmup,tf.float32),tf.cast(-1.5,tf.float32))\n",
    "\n",
    "    def __call__(self,step):\n",
    "        step = tf.cast(step,tf.float32)\n",
    "        return self.factor * tf.math.minimum(tf.math.rsqrt(step),step*self.warmup_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vocab = 7010\n",
    "targ_vocab = 7765\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "with strategy.scope():\n",
    "    train_ds,train_size = get_dataset(total_dataset,\"train\",ds_info,BATCH_SIZE,preprocess_fn)\n",
    "    valid_ds,valid_size = get_dataset(total_dataset,\"validation\",ds_info,BATCH_SIZE,preprocess_fn)\n",
    "    train_steps = train_size//BATCH_SIZE\n",
    "    valid_steps = valid_size//BATCH_SIZE\n",
    "    total_steps = train_steps*BATCH_SIZE\n",
    "    model = get_model(cont_vocab,targ_vocab)\n",
    "    cust_lr = CustomLR(d_model=128,warmup=total_steps//10)\n",
    "    model.compile(\n",
    "        loss=custom_loss,\n",
    "        metrics=[custom_accuracy,custom_loss],\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=cust_lr,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.98,\n",
    "            epsilon=1e-9\n",
    "        ),\n",
    "        steps_per_execution=24,\n",
    "        jit_compile=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 128)                  897280    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " position_embedding (Positi  (1, 128, 128)                0         ['embedding[0][0]']           \n",
      " onEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (1, 128, 128)                197888    ['position_embedding[0][0]',  \n",
      " iHeadAttention)                                                     'position_embedding[0][0]',  \n",
      "                                                                     'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                   (1, 128, 128)                0         ['position_embedding[0][0]',  \n",
      "                                                                     'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (1, 128, 128)                256       ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense (Dense)               (1, 128, 512)                66048     ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (1, 128, 128)                65664     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (1, 128, 128)                0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (1, 128, 128)                0         ['dropout[0][0]',             \n",
      "                                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (1, 128, 128)                256       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (1, 128, 128)                197888    ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (1, 128, 128)                0         ['layer_normalization_1[0][0]'\n",
      "                                                                    , 'multi_head_attention_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (1, 128, 128)                256       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 128)                  993920    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (1, 128, 512)                66048     ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " position_embedding_1 (Posi  (1, 128, 128)                0         ['embedding_1[0][0]']         \n",
      " tionEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (1, 128, 128)                65664     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (1, 128, 128)                197888    ['position_embedding_1[0][0]',\n",
      " ltiHeadAttention)                                                   'position_embedding_1[0][0]',\n",
      "                                                                     'position_embedding_1[0][0]']\n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (1, 128, 128)                0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (1, 128, 128)                0         ['position_embedding_1[0][0]',\n",
      "                                                                     'multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (1, 128, 128)                0         ['dropout_1[0][0]',           \n",
      "                                                                     'layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (1, 128, 128)                256       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (1, 128, 128)                256       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (1, 128, 128)                197888    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (1, 128, 128)                0         ['layer_normalization_4[0][0]'\n",
      "                                                                    , 'multi_head_attention_3[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (1, 128, 128)                256       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (1, 128, 512)                66048     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (1, 128, 128)                65664     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (1, 128, 128)                0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (1, 128, 128)                0         ['layer_normalization_5[0][0]'\n",
      "                                                                    , 'dropout_2[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (1, 128, 128)                256       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (1, 128, 128)                197888    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (1, 128, 128)                0         ['layer_normalization_6[0][0]'\n",
      "                                                                    , 'multi_head_attention_4[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (1, 128, 128)                256       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (1, 128, 128)                197888    ['layer_normalization_7[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (1, 128, 128)                0         ['layer_normalization_7[0][0]'\n",
      "                                                                    , 'multi_head_attention_5[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (1, 128, 128)                256       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (1, 128, 512)                66048     ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (1, 128, 128)                65664     ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (1, 128, 128)                0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (1, 128, 128)                0         ['layer_normalization_8[0][0]'\n",
      "                                                                    , 'dropout_3[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (1, 128, 128)                256       ['add_9[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (1, 128, 7765)               1001685   ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4609621 (17.58 MB)\n",
      "Trainable params: 4609621 (17.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 48/809 [>.............................] - ETA: 2:37:56 - loss: 8.9366 - custom_accuracy: 3.3460e-04 - custom_loss: 8.9366"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
