{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h4ck4l1/datasets/blob/main/NLP_with_RNN_and_Attention/transformers_XLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3xXei0QBNCXT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "import sys,os,warnings\n",
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install \"tensorflow-text==2.13.0\"\n",
        "    %pip install kaleido\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Literal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as tftext\n",
        "import tensorflow_text.tools.wordpiece_vocab.bert_vocab_from_dataset as bert_vocab\n",
        "from zipfile import ZipFile\n",
        "from IPython.display import clear_output\n",
        "from shutil import copytree,copy2\n",
        "import requests\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=9000)]\n",
        "        )\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "%xmode Context\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
        "strategy = tf.distribute.experimental.CentralStorageStrategy()"
      ],
      "metadata": {
        "id": "dne5Wg3MGtGq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDc80HBNCXg",
        "outputId": "30ea89c9-584e-4a35-dfd2-65b152664d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 1s 0us/step\n",
            "Go. ----> Ve.\n",
            "Go. ----> Vete.\n",
            "Go. ----> Vaya.\n",
            "Go. ----> Váyase.\n",
            "Hi. ----> Hola.\n",
            "Run! ----> ¡Corre!\n",
            "Run. ----> Corred.\n",
            "Who? ----> ¿Quién?\n",
            "Fire! ----> ¡Fuego!\n",
            "Fire! ----> ¡Incendio!\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/job:localhost\"):\n",
        "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "    for en,es in zip(en_text[:10],es_text[:10]):\n",
        "        print(f\"{en} ----> {es}\")\n",
        "    en_url = \"https://github.com/h4ck4l1/datasets/raw/main/NLP_with_RNN_and_Attention/en_vocab.txt\"\n",
        "    es_url = \"https://github.com/h4ck4l1/datasets/raw/main/NLP_with_RNN_and_Attention/spa_vocab.txt\"\n",
        "\n",
        "    en_content = requests.get(en_url).content\n",
        "    es_content = requests.get(es_url).content\n",
        "\n",
        "    with open(\"en_vocab.txt\",\"wb\") as f:\n",
        "        f.write(en_content)\n",
        "\n",
        "    with open(\"spa_vocab.txt\",\"wb\") as f:\n",
        "        f.write(es_content)\n",
        "    en_tokenizer = tftext.BertTokenizer(\n",
        "        \"en_vocab.txt\",\n",
        "        normalization_form=\"NFKD\"\n",
        "    )\n",
        "    es_tokenizer = tftext.BertTokenizer(\n",
        "        \"spa_vocab.txt\",\n",
        "        normalization_form=\"NFKD\"\n",
        "    )\n",
        "    with open(\"en_vocab.txt\",\"r\") as f:\n",
        "        en_vocab = f.read()\n",
        "\n",
        "    with open(\"spa_vocab.txt\",\"r\") as f:\n",
        "        es_vocab = f.read()\n",
        "\n",
        "    en_vocab = np.array(en_vocab.splitlines())\n",
        "    es_vocab = np.array(es_vocab.splitlines())\n",
        "    en_text = np.array(en_text)\n",
        "    es_text = np.array(es_text)\n",
        "    start_token = tf.argmax(en_vocab == \"[START]\",output_type=tf.int64)\n",
        "    end_token = tf.argmax(es_vocab== \"[END]\",output_type=tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yvsBzjvpNCY3"
      },
      "outputs": [],
      "source": [
        "def upstream(sentence:str,lang:Literal[\"en\",\"es\"]):\n",
        "    assert lang in [\"en\",\"es\"],f\"The provided argument for lang is not in ['en','es']\"\n",
        "    bsize = tf.shape(sentence)[0]\n",
        "    sentence = tf.convert_to_tensor(sentence)\n",
        "    sentence = tftext.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z,.?!¿]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[,.?!¿]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    if lang == \"en\":\n",
        "        tokens = en_tokenizer.tokenize(sentence).merge_dims(-2,-1).to_tensor()\n",
        "    else:\n",
        "        tokens = es_tokenizer.tokenize(sentence).merge_dims(-2,-1).to_tensor()\n",
        "    return tf.concat([tf.fill(dims=[bsize,1],value=start_token),tokens,tf.fill(dims=[bsize,1],value=end_token)],axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KrEz8I4sNCY5"
      },
      "outputs": [],
      "source": [
        "def downstream(tokens:tf.Tensor,lang:Literal[\"en\",\"es\"]):\n",
        "    assert lang in [\"en\",\"es\"],f\"The provided argument for lang is not in ['en','es']\"\n",
        "    if lang == \"en\":\n",
        "        words = en_tokenizer.detokenize(tokens)\n",
        "    else:\n",
        "        words = es_tokenizer.detokenize(tokens)\n",
        "    bad_tokens = \"|\".join([re.escape(_) for _ in [\"[START]\",\"[END]\",\"[PAD]\"]])\n",
        "    mask = tf.strings.regex_full_match(words,bad_tokens)\n",
        "    re_words = tf.ragged.boolean_mask(words,~mask)\n",
        "    return tf.strings.reduce_join(re_words,separator=\" \",axis=1)\n",
        "\n",
        "\n",
        "# escape strings ^\\[START\\](?:.*?\\[PAD\\])*?(.*?)\\[END\\]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "egMW7Tq4NCY6"
      },
      "outputs": [],
      "source": [
        "def preprocess(context,target):\n",
        "    context = upstream(context,\"en\")\n",
        "    target = upstream(target,\"es\")\n",
        "    return (context,target[:,:-1]),target[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V9UZD1omNCY7"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size:int):\n",
        "    all_indices = np.random.uniform(size=len(en_text))\n",
        "    train_indices = all_indices <= 0.8\n",
        "    valid_indices = all_indices > 0.8\n",
        "    train_size = len(train_indices)\n",
        "    valid_size = len(valid_indices)\n",
        "    train_ds = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((en_text[train_indices],es_text[train_indices]))\n",
        "        .batch(batch_size)\n",
        "        .shuffle(len(en_text))\n",
        "        .map(preprocess)\n",
        "        # .repeat()\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    valid_ds = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((en_text[valid_indices],es_text[valid_indices]))\n",
        "        .batch(batch_size)\n",
        "        .shuffle(len(en_text))\n",
        "        .map(preprocess)\n",
        "        # .repeat()\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return train_ds,valid_ds,train_size,valid_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MCiXnteBNCY9"
      },
      "outputs": [],
      "source": [
        "class PositionEncoding(keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size:int,\n",
        "        length:int,\n",
        "        d_model:int,\n",
        "        casting:Literal[\"concat\",\"interleave\"],\n",
        "        **kwargs\n",
        "        ):\n",
        "\n",
        "        super(PositionEncoding,self).__init__(**kwargs)\n",
        "        assert d_model%2==0,f\"The provided d_model is not even it should be even\"\n",
        "        assert casting in [\"concat\",\"interleave\"],f\"The provided casting is not in the given values\"\n",
        "        self.depth = d_model//2\n",
        "        angle_rads = np.arange(length)[:,np.newaxis] * 1/(10000**(np.arange(self.depth)[np.newaxis,:]/self.depth))\n",
        "        if casting == \"concat\":\n",
        "            embed = tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
        "        else:\n",
        "            embed = np.zeros(shape=angle_rads.shape)\n",
        "            embed[:,::2] = tf.sin(angle_rads)\n",
        "            embed[:,1::2] = tf.cos(angle_rads)\n",
        "\n",
        "        self.embed = tf.cast(embed,tf.float32)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size,d_model,mask_zero=True)\n",
        "\n",
        "    def compute_mask(self,*args,**kwargs):\n",
        "        return self.embedding.compute_mask(*args,**kwargs)\n",
        "\n",
        "    def call(self,inputs):\n",
        "        seq_l = tf.shape(inputs)[1]\n",
        "        embed_out = self.embedding(inputs)\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.depth*2,tf.float32))\n",
        "        return embed_out + self.embed[tf.newaxis,:seq_l,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers:int,\n",
        "        num_heads:int,\n",
        "        d_model:int,\n",
        "        feed_forward_dense_units:int,\n",
        "        dropout_rate:float,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        super(Encoder,self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attention = keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=d_model)\n",
        "        self.ff_dense = keras.layers.Dense(feed_forward_dense_units,\"relu\")\n",
        "        self.scale_dense = keras.layers.Dense(d_model)\n",
        "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "\n",
        "    def call(self,z_enc):\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "\n",
        "            z_enc_copy = z_enc\n",
        "            z_enc = self.self_attention(query=z_enc,key=z_enc,value=z_enc)\n",
        "            z_enc = self.layer_norm(self.add([z_enc_copy,z_enc]))\n",
        "            z_enc_copy = z_enc\n",
        "            z_enc = self.ff_dense(z_enc)\n",
        "            z_enc = self.scale_dense(z_enc)\n",
        "            z_enc = self.dropout(z_enc)\n",
        "            z_enc = self.layer_norm(self.add([z_enc_copy,z_enc]))\n",
        "\n",
        "        return z_enc\n"
      ],
      "metadata": {
        "id": "kokuRBHiWuPk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers:int,\n",
        "        num_self_heads:int,\n",
        "        num_cross_heads:int,\n",
        "        d_model:int,\n",
        "        feed_forward_dense_units:int,\n",
        "        dropout_rate:float,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        super(Decoder,self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attention = keras.layers.MultiHeadAttention(num_heads=num_self_heads,key_dim=d_model)\n",
        "        self.cross_attention = keras.layers.MultiHeadAttention(num_heads=num_cross_heads,key_dim=d_model)\n",
        "        self.ff_dense = keras.layers.Dense(feed_forward_dense_units,\"relu\")\n",
        "        self.scale_dense = keras.layers.Dense(d_model)\n",
        "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "    def call(self,z_enc,z):\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "\n",
        "            z_copy = z\n",
        "            z = self.self_attention(query=z,key=z,value=z,use_causal_mask=True)\n",
        "            z = self.layer_norm(self.add([z_copy,z]))\n",
        "            z_copy = z\n",
        "            z = self.cross_attention(query=z,key=z_enc,value=z_enc)\n",
        "            z = self.layer_norm(self.add([z_copy,z]))\n",
        "            z_copy = z\n",
        "            z = self.ff_dense(z)\n",
        "            z = self.scale_dense(z)\n",
        "            z = self.dropout(z)\n",
        "            z = self.layer_norm(self.add([z_copy,z]))\n",
        "\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "JiMXcd_ihfoi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(keras.Model):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_vocab_size:int,\n",
        "        decoder_vocab_size:int,\n",
        "        encoder_length:int=2048,\n",
        "        decoder_length:int=2048,\n",
        "        d_model:int=512,\n",
        "        encoder_casting:Literal[\"concat\",\"interleave\"]=\"concat\",\n",
        "        decoder_casting:Literal[\"concat\",\"interleave\"]=\"concat\",\n",
        "        encoder_num_layers:int=8,\n",
        "        encoder_num_heads:int=3,\n",
        "        encoder_feed_forward_dense_units:int=2048,\n",
        "        encoder_dropout_rate:float=.1,\n",
        "        decoder_num_layers:int=12,\n",
        "        decoder_num_self_heads:int=3,\n",
        "        decoder_num_cross_heads:int=3,\n",
        "        decoder_feed_forward_dense_units:int=2048,\n",
        "        decoder_dropout_rate:float=.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "\n",
        "        super(Transformer,self).__init__(**kwargs)\n",
        "        self.encoder_embed = PositionEncoding(\n",
        "            encoder_vocab_size,\n",
        "            encoder_length,\n",
        "            d_model,\n",
        "            encoder_casting\n",
        "            )\n",
        "        self.decoder_embed = PositionEncoding(\n",
        "            decoder_vocab_size,\n",
        "            decoder_length,\n",
        "            d_model,\n",
        "            decoder_casting\n",
        "            )\n",
        "        self.encoder_layer = Encoder(\n",
        "            encoder_num_layers,\n",
        "            encoder_num_heads,\n",
        "            d_model,\n",
        "            encoder_feed_forward_dense_units,\n",
        "            encoder_dropout_rate\n",
        "            )\n",
        "        self.decoder_layer = Decoder(\n",
        "            decoder_num_layers,\n",
        "            decoder_num_self_heads,\n",
        "            decoder_num_cross_heads,\n",
        "            d_model,\n",
        "            decoder_feed_forward_dense_units,\n",
        "            decoder_dropout_rate\n",
        "            )\n",
        "        self.total_out = keras.layers.Dense(decoder_vocab_size)\n",
        "\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        z_enc,z_dec = inputs\n",
        "\n",
        "        z_enc = self.encoder_embed(z_enc)\n",
        "        z_enc = self.encoder_layer(z_enc)\n",
        "        z_dec = self.decoder_embed(z_dec)\n",
        "        z = self.decoder_layer(z_enc,z_dec)\n",
        "        z = self.total_out(z)\n",
        "\n",
        "        try:\n",
        "            del z._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return z\n",
        ""
      ],
      "metadata": {
        "id": "mwJtD4s4kRgW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true,y_pred):\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=\"none\")\n",
        "    loss = loss_fn(y_true,y_pred)\n",
        "    mask = tf.cast(y_true != 0,loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "def custom_metric(y_true,y_pred):\n",
        "    y_pred = tf.cast(tf.argmax(y_pred,axis=-1),y_true.dtype)\n",
        "    mask = tf.cast(y_true != 0,tf.int64)\n",
        "    acc = tf.cast(y_true == y_pred,tf.int64)\n",
        "    acc = acc & mask\n",
        "    return tf.reduce_sum(acc)/tf.reduce_sum(mask)\n",
        ""
      ],
      "metadata": {
        "id": "QesX4pKEHOom"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLR(keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self,d_model:int=512,warmup:int=4000,**kwargs):\n",
        "\n",
        "        self.factor = tf.math.rsqrt(tf.cast(d_model,tf.float32)) # d_model^-0.5\n",
        "        self.warmup_factor = tf.math.pow(tf.cast(warmup,tf.float32),tf.cast(-1.5,tf.float32))\n",
        "\n",
        "    def __call__(self,step):\n",
        "        step = tf.cast(step,tf.float32)\n",
        "        return self.factor * tf.math.minimum(tf.math.rsqrt(step),step*self.warmup_factor)\n",
        ""
      ],
      "metadata": {
        "id": "Opi67QgoIGCr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    BATCH_SIZE = 64\n",
        "    train_ds,valid_ds,train_size,valid_size = get_data(BATCH_SIZE)\n",
        "    train_steps = train_size//BATCH_SIZE\n",
        "    valid_steps = valid_size//BATCH_SIZE\n",
        "    model = Transformer(\n",
        "        encoder_vocab_size=len(en_vocab),\n",
        "        decoder_vocab_size=len(es_vocab)\n",
        "        )\n",
        "    cust_lr = CustomLR(d_model=512,warmpu=4000)\n",
        "    model.compile(\n",
        "        loss=custom_loss,\n",
        "        metrics=[custom_metric,custom_loss],\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            learning_rate=cust_lr,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.98\n",
        "        ),\n",
        "        steps_per_execution=25\n",
        "    )"
      ],
      "metadata": {
        "id": "M8EeNXvHpOHQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds,validation_data=valid_ds,epochs=10,steps_per_epoch=train_steps,validation_steps=valid_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUDtZTpOo1Q",
        "outputId": "58c20e63-2621-4159-a9f5-e94ef4008601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 250/1858 [===>..........................] - ETA: 17:58 - loss: 6.6554 - custom_metric: 0.1933 - custom_loss: 6.6554"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}