{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3xXei0QBNCXT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "import sys,os,warnings\n",
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install \"tensorflow-text==2.13.0\"\n",
        "    %pip install kaleido\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Literal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as tftext\n",
        "import tensorflow_text.tools.wordpiece_vocab.bert_vocab_from_dataset as bert_vocab\n",
        "from zipfile import ZipFile\n",
        "from IPython.display import clear_output\n",
        "from shutil import copytree,copy2\n",
        "import requests\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=9000)]\n",
        "        )\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "%xmode Context\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HNDc80HBNCXg",
        "outputId": "8569f21e-88d5-4777-c625-9d0fa3ced0ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go. ----> Ve.\n",
            "Go. ----> Vete.\n",
            "Go. ----> Vaya.\n",
            "Go. ----> Váyase.\n",
            "Hi. ----> Hola.\n",
            "Run! ----> ¡Corre!\n",
            "Run. ----> Corred.\n",
            "Who? ----> ¿Quién?\n",
            "Fire! ----> ¡Fuego!\n",
            "Fire! ----> ¡Incendio!\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/job:localhost\"):\n",
        "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
        "    with ZipFile(file_path,\"r\") as f:\n",
        "        f.extractall(\"spa-eng\")\n",
        "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
        "        text = f.read()\n",
        "clear_output()\n",
        "en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
        "for en,es in zip(en_text[:10],es_text[:10]):\n",
        "    print(f\"{en} ----> {es}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_url = \"https://github.com/h4ck4l1/datasets/raw/main/NLP_with_RNN_and_Attention/en_vocab.txt\"\n",
        "es_url = \"https://github.com/h4ck4l1/datasets/raw/main/NLP_with_RNN_and_Attention/spa_vocab.txt\"\n",
        "\n",
        "en_content = requests.get(en_url).content\n",
        "es_content = requests.get(es_url).content\n",
        "\n",
        "with open(\"en_vocab.txt\",\"wb\") as f:\n",
        "    f.write(en_content)\n",
        "\n",
        "with open(\"spa_vocab.txt\",\"wb\") as f:\n",
        "    f.write(es_content)"
      ],
      "metadata": {
        "id": "0DbyrdTSYMyC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oj6cK0AnNCYz"
      },
      "outputs": [],
      "source": [
        "en_tokenizer = tftext.BertTokenizer(\n",
        "    \"en_vocab.txt\",\n",
        "    normalization_form=\"NFKD\"\n",
        ")\n",
        "es_tokenizer = tftext.BertTokenizer(\n",
        "    \"spa_vocab.txt\",\n",
        "    normalization_form=\"NFKD\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PwgbXk1HNCY1"
      },
      "outputs": [],
      "source": [
        "with open(\"en_vocab.txt\",\"r\") as f:\n",
        "    en_vocab = f.read()\n",
        "\n",
        "with open(\"spa_vocab.txt\",\"r\") as f:\n",
        "    es_vocab = f.read()\n",
        "\n",
        "en_vocab = np.array(en_vocab.splitlines())\n",
        "es_vocab = np.array(es_vocab.splitlines())\n",
        "en_text = np.array(en_text)\n",
        "es_text = np.array(es_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VST9WjiGNCY2"
      },
      "outputs": [],
      "source": [
        "start_token = tf.argmax(en_vocab == \"[START]\",output_type=tf.int64)\n",
        "end_token = tf.argmax(es_vocab== \"[END]\",output_type=tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yvsBzjvpNCY3"
      },
      "outputs": [],
      "source": [
        "def upstream(sentence:str,lang:Literal[\"en\",\"es\"]):\n",
        "    assert lang in [\"en\",\"es\"],f\"The provided argument for lang is not in ['en','es']\"\n",
        "    bsize = tf.shape(sentence)[0]\n",
        "    sentence = tf.convert_to_tensor(sentence)\n",
        "    sentence = tftext.normalize_utf8(sentence,\"NFKD\")\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[^ a-z,.?!¿]\",\"\")\n",
        "    sentence = tf.strings.regex_replace(sentence,r\"[,.?!¿]\",r\" \\0 \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    if lang == \"en\":\n",
        "        tokens = en_tokenizer.tokenize(sentence).merge_dims(-2,-1).to_tensor()\n",
        "    else:\n",
        "        tokens = es_tokenizer.tokenize(sentence).merge_dims(-2,-1).to_tensor()\n",
        "    return tf.concat([tf.fill(dims=[bsize,1],value=start_token),tokens,tf.fill(dims=[bsize,1],value=end_token)],axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KrEz8I4sNCY5"
      },
      "outputs": [],
      "source": [
        "def downstream(tokens:tf.Tensor,lang:Literal[\"en\",\"es\"]):\n",
        "    assert lang in [\"en\",\"es\"],f\"The provided argument for lang is not in ['en','es']\"\n",
        "    if lang == \"en\":\n",
        "        words = en_tokenizer.detokenize(tokens)\n",
        "    else:\n",
        "        words = es_tokenizer.detokenize(tokens)\n",
        "    bad_tokens = \"|\".join([re.escape(_) for _ in [\"[START]\",\"[END]\",\"[PAD]\"]])\n",
        "    mask = tf.strings.regex_full_match(words,bad_tokens)\n",
        "    re_words = tf.ragged.boolean_mask(words,~mask)\n",
        "    return tf.strings.reduce_join(re_words,separator=\" \",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "egMW7Tq4NCY6"
      },
      "outputs": [],
      "source": [
        "def preprocess(context,target):\n",
        "    context = upstream(context,\"en\")\n",
        "    target = upstream(target,\"es\")\n",
        "    return (context,target[:,:-1]),target[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V9UZD1omNCY7",
        "outputId": "0cf2173c-2d96-4363-ef9a-14c6f36f8aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 18) (64, 19) (64, 19)\n",
            "tf.Tensor([   2   40   21  110 1295 3517   97  239  309  135], shape=(10,), dtype=int64)\n",
            "tf.Tensor([  40   21  110 1295 3517   97  239  309  135   51], shape=(10,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "all_indices = np.random.uniform(size=len(en_text))\n",
        "train_indices = all_indices <= 0.8\n",
        "valid_indices = all_indices > 0.8\n",
        "train_size = len(train_indices)\n",
        "valid_size = len(valid_indices)\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[train_indices],es_text[train_indices]))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(len(en_text))\n",
        "    .map(preprocess)\n",
        "    .repeat()\n",
        "    .prefetch(-1)\n",
        ")\n",
        "valid_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((en_text[valid_indices],es_text[valid_indices]))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(len(en_text))\n",
        "    .map(preprocess)\n",
        "    .repeat()\n",
        "    .prefetch(-1)\n",
        ")\n",
        "for (en_in,es_in),targ_in in train_ds.take(1):\n",
        "    print(en_in.shape,es_in.shape,targ_in.shape)\n",
        "    print(es_in[0,:10])\n",
        "    print(targ_in[0,:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MCiXnteBNCY9"
      },
      "outputs": [],
      "source": [
        "class PositionEncoding(keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size:int,\n",
        "        length:int,\n",
        "        d_model:int,\n",
        "        casting:Literal[\"concat\",\"interleave\"],\n",
        "        **kwargs\n",
        "        ):\n",
        "\n",
        "        super(PositionEncoding,self).__init__(**kwargs)\n",
        "        assert d_model%2==0,f\"The provided d_model is not even it should be even\"\n",
        "        assert casting in [\"concat\",\"interleave\"],f\"The provided casting is not in the given values\"\n",
        "\n",
        "        depth = d_model//2\n",
        "        angle_rads = np.arange(length)[:,np.newaxis] * 1/(10000**(np.arange(depth)[np.newaxis,:]/depth))\n",
        "        if casting == \"concat\":\n",
        "            self.embed = tf.concat([tf.sin(angle_rads),tf.cos(angle_rads)],axis=-1)\n",
        "        else:\n",
        "            self.embed = np.zeros(shape=angle_rads.sahpe)\n",
        "            self.embed[:,::2] = tf.sin(angle_rads)\n",
        "            self.embed[:,1::2] = tf.cos(angle_rads)\n",
        "\n",
        "    def call(self,inputs):\n",
        "        seq_l = tf.shape(inputs)[1]\n",
        "        return inputs + self.embed[tf.newaxis,:seq_l,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kokuRBHiWuPk"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}