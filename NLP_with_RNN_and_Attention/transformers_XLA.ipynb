{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "import sys,os,warnings\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install \"tensorflow-text==2.13.0\"\n",
    "    %pip install kaleido\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tftext\n",
    "import tensorflow_text.tools.wordpiece_vocab.bert_vocab_from_dataset as bert_vocab\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "from shutil import copytree,copy2\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=9000)]\n",
    "        )\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "%xmode Context\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go. ----> Ve.\n",
      "Go. ----> Vete.\n",
      "Go. ----> Vaya.\n",
      "Go. ----> Váyase.\n",
      "Hi. ----> Hola.\n",
      "Run! ----> ¡Corre!\n",
      "Run. ----> Corred.\n",
      "Who? ----> ¿Quién?\n",
      "Fire! ----> ¡Fuego!\n",
      "Fire! ----> ¡Incendio!\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/job:localhost\"):\n",
    "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "    file_path = keras.utils.get_file(fname=\"spa-eng.zip\",origin=url,extract=True)\n",
    "    with ZipFile(file_path,\"r\") as f:\n",
    "        f.extractall(\"spa-eng\")\n",
    "    with open(\"spa-eng/spa-eng/spa.txt\",\"r\") as f:\n",
    "        text = f.read()\n",
    "clear_output()\n",
    "en_text,es_text = zip(*[line.split(\"\\t\") for line in text.splitlines()])\n",
    "for en,es in zip(en_text[:10],es_text[:10]):\n",
    "    print(f\"{en} ----> {es}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 05:21:22.684684: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at lookup_table_init_op.cc:152 : NOT_FOUND: es_vocab.txt; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__InitializeTableFromTextFileV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} es_vocab.txt; No such file or directory [Op:InitializeTableFromTextFileV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m en_tokenizer \u001b[39m=\u001b[39m tftext\u001b[39m.\u001b[39mBertTokenizer(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39men_vocab.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     normalization_form\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNFKD\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m es_tokenizer \u001b[39m=\u001b[39m tftext\u001b[39m.\u001b[39;49mBertTokenizer(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mes_vocab.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     normalization_form\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mNFKD\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f70726f6a6563742d74656e736f72666c6f772d31222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/tf-sohail/datasets/NLP_with_RNN_and_Attention/transformers_XLA.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_text/python/ops/bert_tokenizer.py:229\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters, lower_case, keep_whitespace, normalization_form, preserve_unused_token, basic_tokenizer_class)\u001b[0m\n\u001b[1;32m    224\u001b[0m _tf_text_bert_tokenizer_op_create_counter\u001b[39m.\u001b[39mget_cell()\u001b[39m.\u001b[39mincrease_by(\u001b[39m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_basic_tokenizer \u001b[39m=\u001b[39m basic_tokenizer_class(lower_case, keep_whitespace,\n\u001b[1;32m    227\u001b[0m                                               normalization_form,\n\u001b[1;32m    228\u001b[0m                                               preserve_unused_token)\n\u001b[0;32m--> 229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wordpiece_tokenizer \u001b[39m=\u001b[39m WordpieceTokenizer(\n\u001b[1;32m    230\u001b[0m     vocab_lookup_table, suffix_indicator, max_bytes_per_word,\n\u001b[1;32m    231\u001b[0m     max_chars_per_token, token_out_type, unknown_token,\n\u001b[1;32m    232\u001b[0m     split_unknown_characters)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_text/python/ops/wordpiece_tokenizer.py:146\u001b[0m, in \u001b[0;36mWordpieceTokenizer.__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(vocab_lookup_table, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    143\u001b[0m     \u001b[39misinstance\u001b[39m(vocab_lookup_table, ops\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     vocab_lookup_table\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mstring):\n\u001b[1;32m    145\u001b[0m   init \u001b[39m=\u001b[39m lookup_ops\u001b[39m.\u001b[39mTextFileIdTableInitializer(vocab_lookup_table)\n\u001b[0;32m--> 146\u001b[0m   vocab_lookup_table \u001b[39m=\u001b[39m lookup_ops\u001b[39m.\u001b[39;49mStaticVocabularyTableV1(\n\u001b[1;32m    147\u001b[0m       init, num_oov_buckets\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, lookup_key_dtype\u001b[39m=\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mstring)\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(vocab_lookup_table, lookup_ops\u001b[39m.\u001b[39mLookupInterface):\n\u001b[1;32m    150\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mUnable to build a lookup table from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(vocab_lookup_table))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:102\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n\u001b[1;32m    100\u001b[0m   previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:97\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.<lambda>\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     94\u001b[0m   obj\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m     95\u001b[0m   \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m---> 97\u001b[0m previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: default_resource_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     98\u001b[0m resource_creator_stack \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_resource_creator_stack\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:94\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.default_resource_creator\u001b[0;34m(next_creator, *a, **kw)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39massert\u001b[39;00m next_creator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m---> 94\u001b[0m obj\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/lookup_ops.py:1303\u001b[0m, in \u001b[0;36mStaticVocabularyTable.__init__\u001b[0;34m(self, initializer, num_oov_buckets, lookup_key_dtype, name, experimental_is_anonymous)\u001b[0m\n\u001b[1;32m   1301\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(initializer, trackable_base\u001b[39m.\u001b[39mTrackable):\n\u001b[1;32m   1302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initializer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_track_trackable(initializer, \u001b[39m\"\u001b[39m\u001b[39m_initializer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table \u001b[39m=\u001b[39m HashTable(\n\u001b[1;32m   1304\u001b[0m       initializer,\n\u001b[1;32m   1305\u001b[0m       default_value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1306\u001b[0m       experimental_is_anonymous\u001b[39m=\u001b[39;49mexperimental_is_anonymous)\n\u001b[1;32m   1307\u001b[0m   name \u001b[39m=\u001b[39m name \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table\u001b[39m.\u001b[39mname\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:102\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n\u001b[1;32m    100\u001b[0m   previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:97\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.<lambda>\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     94\u001b[0m   obj\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m     95\u001b[0m   \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m---> 97\u001b[0m previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: default_resource_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     98\u001b[0m resource_creator_stack \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_resource_creator_stack\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/resource.py:94\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.default_resource_creator\u001b[0;34m(next_creator, *a, **kw)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39massert\u001b[39;00m next_creator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m---> 94\u001b[0m obj\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/lookup_ops.py:347\u001b[0m, in \u001b[0;36mStaticHashTable.__init__\u001b[0;34m(self, initializer, default_value, name, experimental_is_anonymous)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhash_table\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[39msuper\u001b[39;49m(StaticHashTable, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(default_value, initializer)\n\u001b[1;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_value\u001b[39m.\u001b[39mget_shape()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/lookup_ops.py:204\u001b[0m, in \u001b[0;36mInitializableLookupTableBase.__init__\u001b[0;34m(self, default_value, initializer)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize()\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/lookup_ops.py:207\u001b[0m, in \u001b[0;36mInitializableLookupTableBase._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 207\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initializer\u001b[39m.\u001b[39;49minitialize(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/lookup_ops.py:771\u001b[0m, in \u001b[0;36mTextFileInitializer.initialize\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, \u001b[39m\"\u001b[39m\u001b[39mtext_file_init\u001b[39m\u001b[39m\"\u001b[39m, (table\u001b[39m.\u001b[39mresource_handle,)):\n\u001b[1;32m    769\u001b[0m   filename \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    770\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename, dtypes\u001b[39m.\u001b[39mstring, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39masset_filepath\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 771\u001b[0m   init_op \u001b[39m=\u001b[39m gen_lookup_ops\u001b[39m.\u001b[39;49minitialize_table_from_text_file_v2(\n\u001b[1;32m    772\u001b[0m       table\u001b[39m.\u001b[39;49mresource_handle, filename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key_index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value_index,\n\u001b[1;32m    773\u001b[0m       \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_size \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_delimiter,\n\u001b[1;32m    774\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_offset)\n\u001b[1;32m    775\u001b[0m ops\u001b[39m.\u001b[39madd_to_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mTABLE_INITIALIZERS, init_op)\n\u001b[1;32m    776\u001b[0m \u001b[39m# If the filename tensor is anything other than a string constant (e.g.,\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# if it is a placeholder) then it does not make sense to track it as an\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39m# asset.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_lookup_ops.py:667\u001b[0m, in \u001b[0;36minitialize_table_from_text_file_v2\u001b[0;34m(table_handle, filename, key_index, value_index, vocab_size, delimiter, offset, name)\u001b[0m\n\u001b[1;32m    665\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m    666\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 667\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m    668\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m    669\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__InitializeTableFromTextFileV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} es_vocab.txt; No such file or directory [Op:InitializeTableFromTextFileV2] name: "
     ]
    }
   ],
   "source": [
    "en_tokenizer = tftext.BertTokenizer(\n",
    "    \"en_vocab.txt\",\n",
    "    normalization_form=\"NFKD\"\n",
    ")\n",
    "es_tokenizer = tftext.BertTokenizer(\n",
    "    \"es_vocab.txt\",\n",
    "    normalization_form=\"NFKD\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
