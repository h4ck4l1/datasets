{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,sys,subprocess,warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake = keras.utils.get_file(\"shakes.txt\",'https://homl.info/shakespeare')\n",
    "with open(shake) as f:\n",
    "    shakes = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shakes[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encode every character to integer\n",
    "  - one option to create custom preprocessing layer\n",
    "  - second is to use keras tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q i u s ; u p g . o y m l e ' g z t ; n f ? m k ? k u p ' b m m g $ v n w w\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakes)\n",
    "tokenizer.texts_to_sequences([\"Hello this is mario\"])\n",
    "print(\"\".join(tokenizer.sequences_to_texts(np.random.randint(1,50,size=[1,50]))))\n",
    "max_id = len(tokenizer.word_index)\n",
    "total_size = tokenizer.document_count\n",
    "encoded = np.array(tokenizer.texts_to_sequences(shakes))-1\n",
    "encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_layer = keras.layers.TextVectorization(split=\"character\",standardize=\"lower\")\n",
    "token_layer.adapt([shakes])\n",
    "encoded = token_layer([shakes])[0]\n",
    "encoded -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = token_layer.vocabulary_size()-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 1003854 and valid size = 111540\n"
     ]
    }
   ],
   "source": [
    "train_size = total_size * 90//100\n",
    "valid_size = total_size - train_size\n",
    "print(f\"train size = {train_size} and valid size = {valid_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "batch_size = 32\n",
    "shuffle_size = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_logical_devices()\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds(slices,seq_len=100,batch_size=32,shuffle_size=10000,shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.window(seq_len+1,shift=1,drop_remainder=True).flat_map(lambda x: x.batch(seq_len+1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(lambda x: (x[:,:-1],x[:,1:]))\n",
    "    return ds.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_ds(encoded[:train_size],shuffle=True)\n",
    "valid_ds = create_ds(encoded[train_size:train_size+80_000])\n",
    "test_ds = create_ds(encoded[train_size+80_000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This will take about an hour so skipping and downloading the model'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with strategy.scope():\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(keras.layers.Embedding(input_dim=max_id,output_dim=16))\n",
    "#     model.add(keras.layers.GRU(128,return_sequences=True))\n",
    "#     model.add(keras.layers.Dense(max_id,\"softmax\"))\n",
    "#     model.compile(\n",
    "#         loss=\"sparse_categorical_crossentropy\",\n",
    "#         optimizer=\"nadam\",\n",
    "#         metrics=[\"accuracy\"]\n",
    "#     )\n",
    "# checkp = keras.callbacks.ModelCheckpoint(\"shakes_model\",monitor=\"val_accuracy\",save_best_only=True)\n",
    "# model.fit(train_ds,validation_data=valid_ds,epochs=10,callbacks=[checkp])\n",
    "'''This will take about an hour so skipping and downloading the model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
    "path = keras.utils.get_file(\"shakespeare_model.tgz\",origin=url,extract=True)\n",
    "model_path = Path(path).with_name(\"shakespeare_model\")\n",
    "shakes_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 771ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=6>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"To be or not to b\"\n",
    "tf.argmax(shakes_model.predict([phrase])[0,-1])+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bgucawcenfuldretseai\n",
      "e'dst to l to tcmen e'vy  rsd \n"
     ]
    }
   ],
   "source": [
    "def generate_poem(phrase,using_model=shakes_model,chars=50,temperature=1):\n",
    "    for i in range(chars):\n",
    "        logits = tf.math.log(using_model.predict([phrase],verbose=0)[0,-1:])\n",
    "        rescaled_logits = logits/temperature\n",
    "        char_ids = tf.reshape(tf.random.categorical(rescaled_logits,num_samples=1),shape=[])\n",
    "        phrase += token_layer.get_vocabulary()[char_ids+2]\n",
    "\n",
    "    print(phrase)\n",
    "tf.random.set_seed(42)\n",
    "generate_poem(\"To be or not to b\",temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be lstim,uktn.o\n",
      "eecder tcceseaeddnay: naeasaodrs.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def next_char(text,using_model,temperature=1):\n",
    "    y_proba = using_model.predict([text],verbose=0)[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return token_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text, n_chars=50,using_model=shakes_model, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text,using_model=shakes_model, temperature=temperature)\n",
    "    return text\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stateful_ds(slices,seq_len=100):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.window(seq_len+1,shift=seq_len,drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(seq_len+1))\n",
    "    ds = ds.batch(1)\n",
    "    ds = ds.map(lambda x: (x[:,:-1],x[:,1:])).prefetch(1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1_000_000\n",
    "train_ds = create_stateful_ds(encoded[:train_size])\n",
    "valid_ds = create_stateful_ds(encoded[train_size:train_size+70_000])\n",
    "test_ds = create_stateful_ds(encoded[train_size+70_000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_shakes = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_tokens,output_dim=16,batch_input_shape=[1,None]),\n",
    "    keras.layers.GRU(256,return_sequences=True,stateful=True),\n",
    "    keras.layers.Dense(n_tokens,activation=\"softmax\")\n",
    "])\n",
    "stateful_shakes.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "class ResetStates(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = stateful_shakes.fit(train_ds,validation_data=valid_ds,epochs=10,callbacks=[ResetStates()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shakes_stateful_model = keras.models.load_model(\"shakes_stateful_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakes_stateless_model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_tokens,output_dim=16),\n",
    "    keras.layers.GRU(256,return_sequences=True),\n",
    "    keras.layers.Dense(n_tokens,\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakes_stateless_model.build(input_shape=tf.TensorShape([None,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakes_stateless_model.set_weights(stateful_shakes.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = keras.Sequential([\n",
    "    token_layer,\n",
    "    keras.layers.Lambda(lambda x: x-2),\n",
    "    shakes_stateless_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to beion brnumi\n",
      "  lucei eay tceyceneeayrenai?o\n",
      "oh tcey\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to b\",using_model=stateful_shakes,temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds,test_ds = tfds.load(\"imdb_reviews\",split=[\"train[:90%]\",\"train[90%:]\",\"test\"],as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(5000).batch(32).prefetch(1)\n",
    "valid_ds = valid_ds.batch(32).prefetch(1)\n",
    "test_ds = test_ds.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radio was not a 24 hour 7days a week happening when I grew up in the 1930s England, so Children's Hour was a treat for me when we had batteries and an accumulator to spare for the power. The few programmes I heard therefore made a great impression on my young mind, and the 3 that I recall still are \"Toytown\", one about all the animals at the Zoo, and --- Grey Owl, talking about the animals he knew, which he called his \"brothers\". It was only in recently that I learnt that Grey Owl wasn't a genuine \"Indian\", but the tribute paid by the Sioux Chief makes great sense to me \"A man becomes what he dreams\". Would that we could all dream as world changing and beneficial as Archie Grey Owl Belaney. Would that a new Grey Owl could influence world leaders to clean up the environment.\n",
      "1\n",
      "Ettore Scola, one of the most refined and grand directors we worldly citizens have, is not yet available on DVD... (it's summer 2001 right now....) Mysteries to goggle the mind. <br /><br />This grand classic returned to the theaters in my home-town thanks to a Sophia Loren - summer-retrospective, and to see it again on the big screen after all these years of viewing it on a video-tape ... it is a true gift. <br /><br />To avoid a critique but nonetheless try to prove a point: i took my reluctant younger brother with me to see this film. He never saw the film before and \"doesn't like those Italian Oldies...\" Like all the others in the theater he was intrigued by this wonder. Even during the end-titles the theater remained completely silent. <br /><br />This SPECIAL DAY is truly special. A wonder of refinement. And a big loss if you haven't seen it (yet)...\n",
      "1\n",
      "De Sica is becoming one of my favorite directors, but this one was a hit-and-miss for me. A grinning idiot youth becomes the leader of a community of illegal settlers in a deserted area outside Milano. It is a detailed and sparkling story of the innocent poor masses, complete with evil capitalists and trigger-happy police forces, but slowly it evolves into a magic fantasy tale, as the boy wonder Toto develops unlimited superhero powers. I had it up to here with the ever-smiling Toto after 10 minutes, and when the magic took over, I was left in the dust. There were so many wonderfully orchestrated shots, so many good characters among the settlers, that I kept thinking it was a waste the movie wasn't more serious with its material. The coupling of neorealism and fantasy comes out more as an experiment of the \"look-what-we-can-also-do-mum\" sort than as a fully developed piece of work/art.\n",
      "1\n",
      "is seismic activity with little or negligible results on the surface. So in that respect, IMDb's average voting score is spot on.<br /><br />A Spanish film made in the USA with third or fourth rate actors giving a kind of \"Falcon Crest\" dimension to the whole affair is a wonderful way to waste your time, as well as wasting the money of those who backed the project financially.<br /><br />The slugs involved are originals from Asturias, northern Spain, but as they were not allowed into the United States, plastic ones had to be made. However, chopping them up in the lettuce being used for making the evening dinner-time salad contrasts rather weirdly with Parisienne music as well as a rather tatty array of other US forgotten hits (or misses if you have no idea who was responsible for composing it). The actors involved were also a rather tatty array, just suitable for a low-budget film which might be categorised as horrific, horrifying, horrible or just simple awful.<br /><br />As a result, the outcome is negligible on the surface, undetected underground, and about as attractive as Chapter 17,000 of Coronation Street or the latest news from Baghdad.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for reviews,labels in train_ds.take(1):\n",
    "    count = 0\n",
    "    for review,label in zip(reviews,labels):\n",
    "        print(review.numpy().decode(\"utf-8\"))\n",
    "        print(label.numpy())\n",
    "        count += 1\n",
    "        if count == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    text_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "    text_vec_layer.adapt(train_ds.map(lambda reviews,labels: reviews))\n",
    "    # sentiment_model = keras.Sequential([\n",
    "    #     text_vec_layer,\n",
    "    #     keras.layers.Embedding(input_dim=vocab_size,output_dim=128),\n",
    "    #     keras.layers.GRU(256),\n",
    "    #     keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    # ])\n",
    "    # sentiment_model.compile(\n",
    "    #     loss=\"binary_crossentropy\",\n",
    "    #     optimizer=\"nadam\",\n",
    "    #     metrics=[\"accuracy\"]\n",
    "    # )\n",
    "    sentiment_model = keras.models.load_model(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "704/704 [==============================] - 53s 72ms/step - loss: 0.6934 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4984\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - 32s 46ms/step - loss: 0.6930 - accuracy: 0.5056 - val_loss: 0.6945 - val_accuracy: 0.5012\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.6924 - accuracy: 0.5074 - val_loss: 0.6959 - val_accuracy: 0.5012\n",
      "Epoch 4/10\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.6966 - accuracy: 0.5050 - val_loss: 0.7164 - val_accuracy: 0.4976\n",
      "Epoch 5/10\n",
      "704/704 [==============================] - 29s 40ms/step - loss: 0.5997 - accuracy: 0.6486 - val_loss: 0.4480 - val_accuracy: 0.7928\n",
      "Epoch 6/10\n",
      "704/704 [==============================] - 29s 40ms/step - loss: 0.3766 - accuracy: 0.8318 - val_loss: 0.3502 - val_accuracy: 0.8476\n",
      "Epoch 7/10\n",
      "704/704 [==============================] - 28s 40ms/step - loss: 0.3130 - accuracy: 0.8655 - val_loss: 0.3307 - val_accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "704/704 [==============================] - 28s 40ms/step - loss: 0.2857 - accuracy: 0.8804 - val_loss: 0.3303 - val_accuracy: 0.8556\n",
      "Epoch 9/10\n",
      "704/704 [==============================] - 28s 39ms/step - loss: 0.2657 - accuracy: 0.8908 - val_loss: 0.3232 - val_accuracy: 0.8648\n",
      "Epoch 10/10\n",
      "704/704 [==============================] - 29s 40ms/step - loss: 0.2505 - accuracy: 0.8982 - val_loss: 0.3198 - val_accuracy: 0.8588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f599f27e4f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(train_ds,validation_data=valid_ds,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9977658]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.predict([\"This is the most perfectly good movie of all time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00370183]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.predict([\"This is the most worst and bad movie of all time I had ever wathced, all the actors were bad and movie is super bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    text_vec_layer = keras.layers.TextVectorization(vocab_size)\n",
    "    text_vec_layer.adapt(train_ds.map(lambda reviews,labels: reviews))\n",
    "    # masked_model = keras.Sequential([\n",
    "    #     text_vec_layer,\n",
    "    #     keras.layers.Embedding(input_dim=vocab_size,output_dim=128,mask_zero=True),\n",
    "    #     keras.layers.GRU(256),\n",
    "    #     keras.layers.Dense(1,\"sigmoid\")\n",
    "    # ])\n",
    "    # masked_model.compile(\n",
    "    #     loss=\"binary_crossentropy\",\n",
    "    #     optimizer=\"nadam\",\n",
    "    #     metrics=[\"accuracy\"]\n",
    "    # )\n",
    "    masked_model = keras.models.load_model(\"masked_model\")\n",
    "masked_model.fit(train_ds,validation_data=valid_ds,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999198]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_model.predict([\"This is the most perfectly good movie of all time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00030891]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_model.predict([\"This is the most worst and bad movie of all time I had ever wathced, all the actors were bad and movie is super bad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
